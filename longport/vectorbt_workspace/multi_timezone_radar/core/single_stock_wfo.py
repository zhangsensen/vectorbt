#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ­£ç¡®çš„WFO (Walk Forward Optimization) å•è‚¡ç¥¨å› å­éªŒè¯ç³»ç»Ÿ
æ¯åªè‚¡ç¥¨ç‹¬ç«‹è¿›è¡ŒWFOï¼Œé¿å…è·¨è‚¡ç¥¨æ•°æ®æ³„éœ²
"""

import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import json
import gc
from tqdm import tqdm
import psutil
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed

class SingleStockWFO:
    """å•è‚¡ç¥¨WFOåˆ†æå™¨"""
    
    def __init__(self, 
                 symbol: str,
                 data: pd.DataFrame,
                 start_date: pd.Timestamp,
                 end_date: pd.Timestamp,
                 train_window: timedelta = timedelta(days=60),
                 test_window: timedelta = timedelta(days=30),
                 step_size: timedelta = timedelta(days=15),
                 min_samples: int = 100):
        """
        åˆå§‹åŒ–å•è‚¡ç¥¨WFO
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data: è‚¡ç¥¨æ•°æ®
            start_date: åˆ†æå¼€å§‹æ—¥æœŸ
            end_date: åˆ†æç»“æŸæ—¥æœŸ
            train_window: è®­ç»ƒçª—å£é•¿åº¦
            test_window: æµ‹è¯•çª—å£é•¿åº¦  
            step_size: æ»šåŠ¨æ­¥é•¿
            min_samples: æœ€å°æ ·æœ¬æ•°
        """
        self.symbol = symbol
        self.data = data
        self.start_date = start_date
        self.end_date = end_date
        self.train_window = train_window
        self.test_window = test_window
        self.step_size = step_size
        self.min_samples = min_samples
        self.logger = logging.getLogger(f"{__name__}.{symbol}")
        
        # éªŒè¯timestamp_nsåˆ—æ˜¯å¦å­˜åœ¨
        assert 'timestamp_ns' in data.columns, f"[{symbol}] timestamp_ns åˆ—ä¸¢å¤±ï¼Œæ— æ³•é‡å»ºç´¢å¼•ã€‚å®é™…åˆ—: {data.columns.tolist()}"
        
    def run_wfo(self, factors: Dict[str, pd.Series]) -> Dict[str, Any]:
        """
        è¿è¡Œå•è‚¡ç¥¨WFO
        
        Args:
            factors: å› å­å­—å…¸ {factor_name: factor_series}
            
        Returns:
            WFOç»“æœå­—å…¸
        """
        self.logger.info(f"ğŸ”„ å¼€å§‹ {self.symbol} çš„WFOåˆ†æ...")
        
        # ç¡®ä¿ç´¢å¼•æ˜¯ DatetimeIndex - æ•°æ®å·²ç»ä»load_timeframe_dataæ­£ç¡®å¤„ç†è¿‡æ—¶åŒº
        # åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­ï¼Œæ•°æ®åºåˆ—åŒ–å¯èƒ½ä¼šä¸¢å¤±æ—¶åŒºä¿¡æ¯ï¼Œéœ€è¦é‡æ–°å¤„ç†
        if not isinstance(self.data.index, pd.DatetimeIndex):
            # å¦‚æœç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œè¯´æ˜æ•°æ®åœ¨åºåˆ—åŒ–è¿‡ç¨‹ä¸­å‡ºäº†é—®é¢˜
            # å°è¯•ä»åŸå§‹æ•°æ®é‡æ–°æ„å»ºæ­£ç¡®çš„ç´¢å¼•
            self.logger.warning(f"æ•°æ®ç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œå°è¯•é‡æ–°æ„å»º...")
            
            # ä¼˜å…ˆä½¿ç”¨timestamp_nsåˆ—ï¼ˆçº³ç§’ç²¾åº¦ï¼Œæœ€å¯é ï¼‰
            if 'timestamp_ns' in self.data.columns:
                self.data.index = pd.to_datetime(self.data['timestamp_ns'], unit='ns')
                self.logger.info(f"ä½¿ç”¨timestamp_nsåˆ—é‡å»ºç´¢å¼•")
            # å…¶æ¬¡ä½¿ç”¨timestampåˆ—
            elif 'timestamp' in self.data.columns:
                if pd.api.types.is_datetime64_any_dtype(self.data['timestamp']):
                    self.data = self.data.set_index('timestamp')
                elif self.data['timestamp'].dtype in ['int64', 'float64']:
                    # æ™ºèƒ½åˆ¤æ–­æ—¶é—´æˆ³å•ä½
                    if self.data['timestamp'].max() > 1e16:          # å¾®ç§’
                        self.data['timestamp'] = pd.to_datetime(self.data['timestamp'], unit='us')
                    elif self.data['timestamp'].max() > 1e13:        # æ¯«ç§’
                        self.data['timestamp'] = pd.to_datetime(self.data['timestamp'], unit='ms')
                    elif self.data['timestamp'].max() > 1e10:        # ç§’
                        self.data['timestamp'] = pd.to_datetime(self.data['timestamp'], unit='s')
                    else:
                        self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])
                    self.data = self.data.set_index('timestamp')
                else:
                    self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])
                    self.data = self.data.set_index('timestamp')
                self.logger.info(f"ä½¿ç”¨timestampåˆ—é‡å»ºç´¢å¼•")
            else:
                # å¦‚æœæ²¡æœ‰timestampåˆ—ï¼Œä½†ç´¢å¼•æ˜¯æ•´æ•°ï¼Œå¯èƒ½éœ€è¦è½¬æ¢
                if self.data.index.dtype in ['int64', 'float64']:
                    # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯Unixæ—¶é—´æˆ³
                    if self.data.index.max() > 1e10:  # å¾ˆå¯èƒ½æ˜¯æ—¶é—´æˆ³
                        self.data.index = pd.to_datetime(self.data.index, unit='s')
                    else:
                        # å¦‚æœä¸æ˜¯æ—¶é—´æˆ³ï¼Œä¿ç•™åŸæ ·ä½†è®°å½•è­¦å‘Š
                        self.logger.warning(f"ç´¢å¼•æ˜¯æ•´æ•°ä½†å€¼è¾ƒå°ï¼Œå¯èƒ½ä¸æ˜¯æ—¶é—´æˆ³: {self.data.index.min()} - {self.data.index.max()}")
        
        # ç¡®ä¿æ—¶åŒºä¿¡æ¯
        if hasattr(self.data.index, 'tz') and self.data.index.tz is None:
            self.data.index = self.data.index.tz_localize('Asia/Hong_Kong')
        elif not hasattr(self.data.index, 'tz'):
            # å¦‚æœç´¢å¼•æ²¡æœ‰tzå±æ€§ï¼Œè¯´æ˜å®ƒä¸æ˜¯DatetimeIndexï¼Œè·³è¿‡æ—¶åŒºå¤„ç†
            self.logger.warning(f"ç´¢å¼•æ²¡æœ‰tzå±æ€§ï¼Œè·³è¿‡æ—¶åŒºå¤„ç†")
        
        # 1. å¼ºåˆ¶å•è°ƒé€’å¢ & å»ç©º
        self.data = self.data.sort_index().dropna()
        
        # 2. ç»Ÿä¸€æ—¶åŒºå¤„ç†ï¼ˆé¿å…æ—¶åŒºæ¯”è¾ƒå¤±è´¥ï¼‰
        if hasattr(self.data.index, 'tz') and self.data.index.tz is None:
            self.data.index = self.data.index.tz_localize('Asia/Hong_Kong')
        elif not hasattr(self.data.index, 'tz'):
            # å¦‚æœç´¢å¼•æ²¡æœ‰tzå±æ€§ï¼Œè¯´æ˜å®ƒä¸æ˜¯DatetimeIndexï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡
            self.logger.error(f"ç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œæ— æ³•è¿›è¡Œæ—¶åŒºå¤„ç†")
            return {'error': 'æ•°æ®ç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œæ— æ³•è¿›è¡Œæ—¶åŒºå¤„ç†'}
        
        # 3. ç¡®ä¿åˆ†ææ—¥æœŸæœ‰æ—¶åŒºä¿¡æ¯
        if self.start_date.tz is None:
            self.start_date = self.start_date.tz_localize('Asia/Hong_Kong')
        if self.end_date.tz is None:
            self.end_date = self.end_date.tz_localize('Asia/Hong_Kong')
        
        # 4. æŠŠè®­ç»ƒ/æµ‹è¯•çª—å£å‚æ•°ä¸´æ—¶è°ƒå°ï¼ˆå¿«é€ŸéªŒè¯ï¼‰
        original_train_window = self.train_window
        original_test_window = self.test_window
        original_step_size = self.step_size
        original_min_samples = self.min_samples
        
        self.train_window = pd.Timedelta(days=10)      # 10 æ ¹å³å¯
        self.test_window = pd.Timedelta(days=5)
        self.step_size = pd.Timedelta(days=2)
        self.min_samples = 10      # æœ€å°æ ·æœ¬
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ•°æ®èŒƒå›´
        self.logger.info(f"æ•°æ®èŒƒå›´: {self.data.index.min()} åˆ° {self.data.index.max()}")
        self.logger.info(f"åˆ†æèŒƒå›´: {self.start_date} åˆ° {self.end_date}")
        self.logger.info(f"æ•°æ®ç‚¹æ•°: {len(self.data)}")
        
        wfo_results = []
        current_date = self.start_date
        
        while current_date + self.train_window + self.test_window <= self.end_date:
            # å®šä¹‰å½“å‰çª—å£
            train_start = current_date
            train_end = current_date + self.train_window
            test_start = train_end
            test_end = test_start + self.test_window
            
            # è·å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
            train_data = self.data[(self.data.index >= train_start) & (self.data.index < train_end)]
            test_data = self.data[(self.data.index >= test_start) & (self.data.index < test_end)]
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°çª—å£ä¿¡æ¯
            self.logger.info(f"çª—å£ {len(wfo_results)+1}: è®­ç»ƒæœŸ {train_start} åˆ° {train_end} (æ•°æ®ç‚¹: {len(train_data)}), æµ‹è¯•æœŸ {test_start} åˆ° {test_end} (æ•°æ®ç‚¹: {len(test_data)})")
            
            # æ£€æŸ¥æ•°æ®å……è¶³æ€§
            if len(train_data) < self.min_samples or len(test_data) < self.min_samples // 2:
                self.logger.info(f"  -> è·³è¿‡çª—å£ï¼šè®­ç»ƒæ•°æ® {len(train_data)} < {self.min_samples} æˆ– æµ‹è¯•æ•°æ® {len(test_data)} < {self.min_samples//2}")
                current_date += self.step_size
                continue
                
            # è®¡ç®—è®­ç»ƒæœŸå› å­è¡¨ç°
            train_factors = {}
            for factor_name, factor_series in factors.items():
                # å¯¹é½å› å­æ•°æ®åˆ°è®­ç»ƒæœŸæ—¶é—´èŒƒå›´
                aligned_factor = factor_series.loc[train_data.index.intersection(factor_series.index)]
                if len(aligned_factor) > 0:
                    train_factors[factor_name] = aligned_factor
            
            # è®¡ç®—æµ‹è¯•æœŸå› å­è¡¨ç°  
            test_factors = {}
            for factor_name, factor_series in factors.items():
                # å¯¹é½å› å­æ•°æ®åˆ°æµ‹è¯•æœŸæ—¶é—´èŒƒå›´
                aligned_factor = factor_series.loc[test_data.index.intersection(factor_series.index)]
                if len(aligned_factor) > 0:
                    test_factors[factor_name] = aligned_factor
            
            # è®¡ç®—æŒ‡æ ‡
            window_result = {
                'train_start': train_start,
                'train_end': train_end,
                'test_start': test_start,
                'test_end': test_end,
                'train_samples': len(train_data),
                'test_samples': len(test_data),
                'train_metrics': self._calculate_window_metrics(train_factors, train_data),
                'test_metrics': self._calculate_window_metrics(test_factors, test_data)
            }
            
            wfo_results.append(window_result)
            current_date += self.step_size
            
        self.logger.info(f"âœ… {self.symbol} WFOå®Œæˆï¼Œå…± {len(wfo_results)} ä¸ªçª—å£")
        
        # æ±‡æ€»WFOç»“æœ
        return self._aggregate_wfo_results(wfo_results)
    
    def _calculate_window_metrics(self, factors: Dict[str, pd.Series], data: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—çª—å£å†…çš„å› å­æŒ‡æ ‡"""
        metrics = {}
        
        if 'close' not in data.columns or len(data) < 2:
            self.logger.warning(f"æ•°æ®ä¸è¶³: closeåˆ—å­˜åœ¨={ 'close' in data.columns}, æ•°æ®é•¿åº¦={len(data)}")
            return metrics
            
        returns = data['close'].pct_change().dropna()
        
        if len(returns) < 5:
            self.logger.warning(f"æ”¶ç›Šæ•°æ®ä¸è¶³: {len(returns)}")
            return metrics
        
        self.logger.info(f"å¼€å§‹è®¡ç®—æŒ‡æ ‡ï¼Œå› å­æ•°é‡: {len(factors)}, æ”¶ç›Šæ•°æ®é•¿åº¦: {len(returns)}")
        
        for factor_name, factor_series in factors.items():
            try:
                # æ£€æŸ¥å› å­æ•°æ®
                if factor_series.isna().all():
                    self.logger.warning(f"å› å­ {factor_name} å…¨ä¸ºNaN")
                    continue
                
                # å¯¹é½æ•°æ®
                aligned_returns, aligned_factor = returns.align(factor_series, join='inner')
                
                if len(aligned_returns) < 5:
                    self.logger.warning(f"å› å­ {factor_name} å¯¹é½åæ•°æ®ä¸è¶³: {len(aligned_returns)}")
                    continue
                
                # ç§»é™¤NaNå€¼
                mask = ~(aligned_returns.isna() | aligned_factor.isna())
                clean_returns = aligned_returns[mask]
                clean_factor = aligned_factor[mask]
                
                if len(clean_returns) < 5:
                    self.logger.warning(f"å› å­ {factor_name} æ¸…æ´—åæ•°æ®ä¸è¶³: {len(clean_returns)}")
                    continue
                
                # è®¡ç®—IC
                ic = clean_returns.corr(clean_factor)
                
                # è®¡ç®—RankIC
                rank_ic = clean_returns.corr(clean_factor.rank())
                
                # è®¡ç®—åˆ†ä½æ•°æ”¶ç›Š
                try:
                    quantiles = pd.qcut(clean_factor, q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'], duplicates='drop')
                    quantile_returns = clean_returns.groupby(quantiles).mean()
                except Exception as e:
                    self.logger.warning(f"å› å­ {factor_name} åˆ†ä½æ•°è®¡ç®—å¤±è´¥: {e}")
                    quantile_returns = pd.Series()
                
                metrics[factor_name] = {
                    'ic': ic if not np.isnan(ic) else 0,
                    'rank_ic': rank_ic if not np.isnan(rank_ic) else 0,
                    'samples': len(clean_returns),
                    'quantile_returns': quantile_returns.to_dict(),
                    'factor_mean': clean_factor.mean(),
                    'factor_std': clean_factor.std()
                }
                
                self.logger.info(f"å› å­ {factor_name} è®¡ç®—å®Œæˆ: IC={ic:.4f}, æ ·æœ¬æ•°={len(clean_returns)}")
                
            except Exception as e:
                self.logger.error(f"å› å­ {factor_name} è®¡ç®—å¤±è´¥: {e}")
                continue
            
        return metrics
    
    def _aggregate_wfo_results(self, wfo_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ±‡æ€»WFOç»“æœ"""
        if not wfo_results:
            return {}
            
        aggregated = {}
        
        # æ”¶é›†æ‰€æœ‰å› å­çš„æµ‹è¯•æœŸIC
        factor_ics = {}
        factor_rank_ics = {}
        
        for window in wfo_results:
            test_metrics = window.get('test_metrics', {})
            for factor_name, metrics in test_metrics.items():
                if factor_name not in factor_ics:
                    factor_ics[factor_name] = []
                    factor_rank_ics[factor_name] = []
                    
                if 'ic' in metrics and not np.isnan(metrics['ic']):
                    factor_ics[factor_name].append(metrics['ic'])
                    
                if 'rank_ic' in metrics and not np.isnan(metrics['rank_ic']):
                    factor_rank_ics[factor_name].append(metrics['rank_ic'])
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        for factor_name in factor_ics.keys():
            ics = factor_ics[factor_name]
            rank_ics = factor_rank_ics[factor_name]
            
            if ics:
                aggregated[factor_name] = {
                    'windows': len(ics),
                    'mean_ic': np.mean(ics),
                    'std_ic': np.std(ics),
                    'median_ic': np.median(ics),
                    'ic_positive_ratio': sum(1 for ic in ics if ic > 0) / len(ics),
                    'mean_rank_ic': np.mean(rank_ics) if rank_ics else np.nan,
                    'std_rank_ic': np.std(rank_ics) if rank_ics else np.nan,
                    'stability_score': 1 - (np.std(ics) / abs(np.mean(ics))) if np.mean(ics) != 0 else 0,
                    'all_ics': ics,
                    'all_rank_ics': rank_ics
                }
        
        return {
            'symbol': self.symbol,
            'total_windows': len(wfo_results),
            'wfo_results': wfo_results,
            'aggregated_metrics': aggregated
        }


class MultiStockWFOAnalyzer:
    """å¤šè‚¡ç¥¨WFOåˆ†æå™¨"""
    
    def __init__(self, 
                 data_dir: str,
                 start_date: str = "2025-03-01",
                 end_date: str = "2025-09-01",
                 train_window_days: int = 60,
                 test_window_days: int = 30,
                 step_days: int = 15,
                 n_workers: int = None):
        """
        åˆå§‹åŒ–å¤šè‚¡ç¥¨WFOåˆ†æå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            train_window_days: è®­ç»ƒçª—å£å¤©æ•°
            test_window_days: æµ‹è¯•çª—å£å¤©æ•°
            step_days: æ»šåŠ¨æ­¥é•¿å¤©æ•°
            n_workers: å·¥ä½œè¿›ç¨‹æ•°
        """
        self.data_dir = Path(data_dir)
        self.start_date = pd.to_datetime(start_date)
        self.end_date = pd.to_datetime(end_date)
        self.train_window = timedelta(days=train_window_days)
        self.test_window = timedelta(days=test_window_days)
        self.step_size = timedelta(days=step_days)
        self.n_workers = n_workers or max(1, min(8, mp.cpu_count() - 2))
        self.logger = logging.getLogger(__name__)
        
    def load_stock_data(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½è‚¡ç¥¨æ•°æ®"""
        self.logger.info("ğŸ“‚ åŠ è½½è‚¡ç¥¨æ•°æ®...")
        stock_data = {}
        
        # å‡è®¾æ•°æ®æ–‡ä»¶ä¸ºCSVæ ¼å¼
        for file_path in self.data_dir.glob("*.csv"):
            symbol = file_path.stem
            try:
                df = pd.read_csv(file_path, index_col=0, parse_dates=True)
                df = df[(df.index >= self.start_date) & (df.index <= self.end_date)]
                
                if len(df) > 100:  # æœ€å°æ•°æ®é‡è¦æ±‚
                    stock_data[symbol] = df
                    self.logger.info(f"   åŠ è½½ {symbol}: {len(df)} æ¡è®°å½•")
                    
            except Exception as e:
                self.logger.warning(f"   åŠ è½½ {symbol} å¤±è´¥: {e}")
                
        self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(stock_data)} åªè‚¡ç¥¨æ•°æ®")
        return stock_data
    
    def calculate_factors(self, data: pd.DataFrame) -> Dict[str, pd.Series]:
        """è®¡ç®—æŠ€æœ¯å› å­"""
        factors = {}
        
        if len(data) < 20:
            return factors
            
        try:
            # å°è¯•ä½¿ç”¨TALibï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨pandaså®ç°
            try:
                import talib
                use_talib = True
            except ImportError:
                use_talib = False
                self.logger.info("TALibä¸å¯ç”¨ï¼Œä½¿ç”¨pandaså®ç°")
            
            close = data['close'].astype(np.float64).values
            high = data['high'].astype(np.float64).values if 'high' in data.columns else close
            low = data['low'].astype(np.float64).values if 'low' in data.columns else close
            volume = data['volume'].astype(np.float64).values if 'volume' in data.columns else np.ones(len(close), dtype=np.float64)
            
            # RSI
            if len(close) > 14:
                if use_talib:
                    factors['RSI'] = pd.Series(talib.RSI(close, timeperiod=14), index=data.index)
                else:
                    delta = np.diff(close)
                    gain = np.where(delta > 0, delta, 0)
                    loss = np.where(delta < 0, -delta, 0)
                    avg_gain = pd.Series(gain).rolling(14).mean()
                    avg_loss = pd.Series(loss).rolling(14).mean()
                    rs = avg_gain / avg_loss
                    factors['RSI'] = 100 - (100 / (1 + rs))
                    factors['RSI'].index = data.index
                
            # MACD
            if len(close) > 26:
                if use_talib:
                    macd, signal, hist = talib.MACD(close)
                    factors['MACD'] = pd.Series(macd, index=data.index)
                else:
                    ema12 = pd.Series(close).ewm(span=12).mean()
                    ema26 = pd.Series(close).ewm(span=26).mean()
                    factors['MACD'] = ema12 - ema26
                
            # ç®€å•ç§»åŠ¨å¹³å‡å› å­
            if len(close) > 20:
                ma5 = pd.Series(close).rolling(5).mean()
                ma20 = pd.Series(close).rolling(20).mean()
                factors['MA_Ratio'] = ma5 / ma20
                
            # åŠ¨é‡æŒ‡æ ‡
            if len(close) > 10:
                factors['Momentum_ROC'] = data['close'].pct_change(10)
                
            # æ³¢åŠ¨ç‡
            if len(close) > 10:
                factors['Volatility'] = data['close'].pct_change().rolling(10).std()
                
            # ä»·æ ¼ä½ç½®
            if len(high) > 20 and len(low) > 20:
                high_20 = data['high'].rolling(20).max()
                low_20 = data['low'].rolling(20).min()
                factors['Price_Position'] = (data['close'] - low_20) / (high_20 - low_20)
                
            # æˆäº¤é‡æ¯”ç‡
            if len(volume) > 20:
                volume_ma = pd.Series(volume).rolling(20).mean()
                factors['Volume_Ratio'] = pd.Series(volume) / volume_ma
                
        except Exception as e:
            self.logger.warning(f"è®¡ç®—å› å­å¤±è´¥: {e}")
            
        # è¿‡æ»¤æ‰NaNè¿‡å¤šçš„å› å­
        valid_factors = {}
        for name, series in factors.items():
            if series.notna().sum() >= len(series) * 0.5:  # è‡³å°‘50%éNaN
                valid_factors[name] = series
            else:
                self.logger.warning(f"å› å­ {name} NaNè¿‡å¤šï¼Œå·²ä¸¢å¼ƒ")
        
        return valid_factors
    
    def run_single_stock_wfo(self, args: Tuple[str, pd.DataFrame]) -> Dict[str, Any]:
        """è¿è¡Œå•åªè‚¡ç¥¨çš„WFO"""
        symbol, data = args
        
        try:
            # éªŒè¯å¹¶ä¿®å¤æ•°æ®ç´¢å¼•
            if not isinstance(data.index, pd.DatetimeIndex):
                self.logger.warning(f"{symbol}: æ•°æ®ç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œå°è¯•ä¿®å¤...")
                
                # ä¼˜å…ˆä½¿ç”¨timestamp_nsåˆ—ï¼ˆçº³ç§’ç²¾åº¦ï¼Œæœ€å¯é ï¼‰
                if 'timestamp_ns' in data.columns:
                    data.index = pd.to_datetime(data['timestamp_ns'], unit='ns')
                    self.logger.info(f"{symbol}: ä½¿ç”¨timestamp_nsåˆ—é‡å»ºç´¢å¼•")
                # å…¶æ¬¡ä½¿ç”¨timestampåˆ—
                elif 'timestamp' in data.columns:
                    if pd.api.types.is_datetime64_any_dtype(data['timestamp']):
                        data = data.set_index('timestamp')
                    elif data['timestamp'].dtype in ['int64', 'float64']:
                        # æ™ºèƒ½åˆ¤æ–­æ—¶é—´æˆ³å•ä½
                        if data['timestamp'].max() > 1e16:          # å¾®ç§’
                            data['timestamp'] = pd.to_datetime(data['timestamp'], unit='us')
                        elif data['timestamp'].max() > 1e13:        # æ¯«ç§’
                            data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')
                        elif data['timestamp'].max() > 1e10:        # ç§’
                            data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')
                        else:
                            data['timestamp'] = pd.to_datetime(data['timestamp'])
                        data = data.set_index('timestamp')
                    else:
                        data['timestamp'] = pd.to_datetime(data['timestamp'])
                        data = data.set_index('timestamp')
                    self.logger.info(f"{symbol}: ä½¿ç”¨timestampåˆ—é‡å»ºç´¢å¼•")
                else:
                    # å¦‚æœæ²¡æœ‰timestampåˆ—ï¼Œæ— æ³•ä¿®å¤
                    self.logger.error(f"{symbol}: æ•°æ®ç´¢å¼•ä¸æ˜¯DatetimeIndexä¸”æ— timestampåˆ—")
                    return {'symbol': symbol, 'error': 'æ•°æ®ç´¢å¼•ä¸æ˜¯DatetimeIndexä¸”æ— timestampåˆ—'}
            
            # ç¡®ä¿æ—¶åŒºä¿¡æ¯
            if hasattr(data.index, 'tz') and data.index.tz is None:
                data.index = data.index.tz_localize('Asia/Hong_Kong')
            
            # è®¡ç®—å› å­
            factors = self.calculate_factors(data)
            
            if not factors:
                self.logger.warning(f"{symbol}: æ— æ³•è®¡ç®—å› å­")
                return {'symbol': symbol, 'error': 'æ— æ³•è®¡ç®—å› å­'}
            
            # è¿è¡ŒWFO
            print(f"[DEBUG] {symbol}: ä¼ é€’å‰çš„åˆ—: {data.columns.tolist()}")
            print(f"[DEBUG] {symbol}: æ•°æ®ç´¢å¼•ç±»å‹: {type(data.index)}")
            print(f"[DEBUG] {symbol}: æ•°æ®å½¢çŠ¶: {data.shape}")
            
            wfo_analyzer = SingleStockWFO(
                symbol=symbol,
                data=data,
                start_date=self.start_date,
                end_date=self.end_date,
                train_window=self.train_window,
                test_window=self.test_window,
                step_size=self.step_size,
                min_samples=30  # é™ä½æœ€å°æ ·æœ¬è¦æ±‚
            )
            
            result = wfo_analyzer.run_wfo(factors)
            
            # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
            if not result or 'aggregated_metrics' not in result or not result['aggregated_metrics']:
                self.logger.warning(f"{symbol}: WFOç»“æœä¸ºç©º")
                return {'symbol': symbol, 'error': 'WFOç»“æœä¸ºç©º'}
            
            return result
            
        except Exception as e:
            self.logger.error(f"å¤„ç† {symbol} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {'symbol': symbol, 'error': str(e)}
    
    def run_all_stocks_wfo(self, stock_data: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰è‚¡ç¥¨çš„WFO"""
        self.logger.info(f"ğŸš€ å¼€å§‹å¤šè‚¡ç¥¨WFOåˆ†æï¼ˆ{self.n_workers}ä¸ªå·¥ä½œè¿›ç¨‹ï¼‰...")
        
        all_results = {}
        
        # å‡†å¤‡å‚æ•°
        tasks = [(symbol, data) for symbol, data in stock_data.items()]
        
        # å¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_symbol = {executor.submit(self.run_single_stock_wfo, task): task[0] 
                               for task in tasks}
            
            # ä½¿ç”¨è¿›åº¦æ¡
            with tqdm(total=len(tasks), desc="WFOè¿›åº¦") as pbar:
                for future in as_completed(future_to_symbol):
                    symbol = future_to_symbol[future]
                    try:
                        result = future.result()
                        all_results[symbol] = result
                    except Exception as e:
                        self.logger.error(f"{symbol} å¤„ç†å¤±è´¥: {e}")
                        all_results[symbol] = {'symbol': symbol, 'error': str(e)}
                    finally:
                        pbar.update(1)
        
        # æ±‡æ€»ç»“æœ
        summary = self._summarize_all_results(all_results)
        
        self.logger.info("âœ… å¤šè‚¡ç¥¨WFOåˆ†æå®Œæˆ")
        return {
            'individual_results': all_results,
            'summary': summary,
            'config': {
                'start_date': self.start_date.strftime('%Y-%m-%d'),
                'end_date': self.end_date.strftime('%Y-%m-%d'),
                'train_window_days': self.train_window.days,
                'test_window_days': self.test_window.days,
                'step_days': self.step_size.days,
                'total_stocks': len(stock_data)
            }
        }
    
    def _summarize_all_results(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ±‡æ€»æ‰€æœ‰è‚¡ç¥¨çš„WFOç»“æœ"""
        summary = {
            'total_stocks': len(all_results),
            'successful_stocks': 0,
            'factor_performance': {},
            'stable_factors': {}
        }
        
        # æ”¶é›†æ‰€æœ‰å› å­çš„è¡¨ç°
        factor_windows = {}
        factor_ics = {}
        
        for symbol, result in all_results.items():
            if 'error' in result or 'aggregated_metrics' not in result:
                continue
                
            summary['successful_stocks'] += 1
            
            aggregated_metrics = result['aggregated_metrics']
            
            for factor_name, metrics in aggregated_metrics.items():
                if factor_name not in factor_windows:
                    factor_windows[factor_name] = []
                    factor_ics[factor_name] = []
                    
                factor_windows[factor_name].append(metrics.get('windows', 0))
                factor_ics[factor_name].extend(metrics.get('all_ics', []))
        
        # è®¡ç®—å› å­æ±‡æ€»ç»Ÿè®¡
        for factor_name in factor_windows.keys():
            windows = factor_windows[factor_name]
            ics = factor_ics[factor_name]
            
            if windows and ics:
                summary['factor_performance'][factor_name] = {
                    'avg_windows_per_stock': np.mean(windows),
                    'total_windows': sum(windows),
                    'mean_ic': np.mean(ics),
                    'std_ic': np.std(ics),
                    'median_ic': np.median(ics),
                    'ic_positive_ratio': sum(1 for ic in ics if ic > 0) / len(ics),
                    'stability': 1 - (np.std(ics) / abs(np.mean(ics))) if np.mean(ics) != 0 else 0
                }
                
                # è¯†åˆ«ç¨³å®šå› å­ï¼ˆICç¨³å®šæ€§ > 0.7 ä¸” å¹³å‡ICç»å¯¹å€¼ > 0.02ï¼‰
                performance = summary['factor_performance'][factor_name]
                if (performance['stability'] > 0.7 and 
                    abs(performance['mean_ic']) > 0.02 and
                    performance['ic_positive_ratio'] > 0.6 or performance['ic_positive_ratio'] < 0.4):
                    summary['stable_factors'][factor_name] = performance
        
        return summary
    
    def save_results(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜ç»“æœ"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
        self.logger.info(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = MultiStockWFOAnalyzer(
        data_dir="/Users/zhangshenshen/longport/vectorbt_workspace/data",
        start_date="2025-03-01",
        end_date="2025-09-01",
        train_window_days=60,
        test_window_days=30,
        step_days=15,
        n_workers=4
    )
    
    # åŠ è½½æ•°æ®
    stock_data = analyzer.load_stock_data()
    
    # è¿è¡ŒWFO
    results = analyzer.run_all_stocks_wfo(stock_data)
    
    # ä¿å­˜ç»“æœ
    analyzer.save_results(results, "wfo_results.json")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*60)
    print("WFOåˆ†ææ‘˜è¦")
    print("="*60)
    print(f"æ€»è‚¡ç¥¨æ•°: {results['summary']['total_stocks']}")
    print(f"æˆåŠŸåˆ†æ: {results['summary']['successful_stocks']}")
    print(f"ç¨³å®šå› å­: {list(results['summary']['stable_factors'].keys())}")
    
    for factor_name, performance in results['summary']['stable_factors'].items():
        print(f"\n{factor_name}:")
        print(f"  å¹³å‡IC: {performance['mean_ic']:.4f}")
        print(f"  ç¨³å®šæ€§: {performance['stability']:.4f}")
        print(f"  æ­£å‘æ¯”ç‡: {performance['ic_positive_ratio']:.4f}")


if __name__ == "__main__":
    main()