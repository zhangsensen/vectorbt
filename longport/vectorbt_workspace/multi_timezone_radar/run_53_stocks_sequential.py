#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
53åªæ¸¯è‚¡é€ä¸ªå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æå›æµ‹è„šæœ¬
å‚ç…§0700.HKçš„åˆ†ææ–¹å¼ï¼Œå¯¹æ¯åªè‚¡ç¥¨è¿›è¡Œç‹¬ç«‹åˆ†æ
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging
import json
import gc
import traceback
import time
import argparse
from logging.handlers import RotatingFileHandler

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from core.vectorbt_wfo_analyzer import VectorbtWFOAnalyzer

def setup_timestamp_logging_for_stock(stock):
    """ä¸ºæŒ‡å®šè‚¡ç¥¨è®¾ç½®åŸºäºæ—¶é—´æˆ³çš„æ—¥å¿—è®°å½•"""
    # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = Path(f"logs/individual_analysis/{stock}_{timestamp}")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file = log_dir / f"{stock}_full_analysis.log"
    
    # æ¸…é™¤ç°æœ‰çš„æ—¥å¿—é…ç½®
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # é…ç½®æ—¥å¿— - ä½¿ç”¨æ»šåŠ¨æ—¥å¿—é˜²æ­¢æ–‡ä»¶è¿‡å¤§
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            RotatingFileHandler(log_file, maxBytes=50*1024*1024, backupCount=3, encoding='utf-8', mode='w'),
            logging.StreamHandler()
        ],
        force=True
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç›®å½•: {log_dir}")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info(f"åˆ†æè‚¡ç¥¨: {stock}")
    logger.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    return logger, log_dir, timestamp

def save_analysis_parameters(log_dir, timestamp, stock):
    """ä¿å­˜åˆ†æå‚æ•°é…ç½®"""
    parameters = {
        "analysis_type": f"{stock}å…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ",
        "timestamp": timestamp,
        "stock": stock,
        "timeframes": ['1m', '5m', '15m', '30m', '1h', '4h', '1d'],
        "factors": ["RSI", "Price_Position", "Momentum_ROC", "MACD", "Volume_Ratio"],
        "start_date": "2024-01-01",
        "end_date": "2025-09-01",
        "memory_limit_gb": 16.0,
        "n_workers": 6,
        "analysis_mode": "single_stock_comprehensive"
    }
    
    params_file = log_dir / "analysis_parameters.json"
    with open(params_file, 'w', encoding='utf-8') as f:
        json.dump(parameters, f, ensure_ascii=False, indent=2)
    
    return parameters

def save_checkpoint(overall_results, start_index, end_index, timestamp):
    """ä¿å­˜åˆ†ææ£€æŸ¥ç‚¹"""
    checkpoint_dir = Path(f"logs/sequential_analysis_summary_{timestamp}")
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_file = checkpoint_dir / "checkpoint.json"
    checkpoint_data = {
        "timestamp": timestamp,
        "start_index": start_index,
        "end_index": end_index,
        "overall_results": overall_results,
        "saved_at": datetime.now().isoformat()
    }
    
    with open(checkpoint_file, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_file}")
    return checkpoint_file

def load_checkpoint(checkpoint_path):
    """åŠ è½½åˆ†ææ£€æŸ¥ç‚¹"""
    try:
        with open(checkpoint_path, 'r', encoding='utf-8') as f:
            checkpoint_data = json.load(f)
        
        print(f"ğŸ“‚ æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")
        print(f"   åˆ†æèŒƒå›´: ç¬¬{checkpoint_data['start_index']+1}åˆ°ç¬¬{checkpoint_data['end_index']}åªè‚¡ç¥¨")
        print(f"   å·²å®Œæˆè‚¡ç¥¨æ•°: {len(checkpoint_data['overall_results'])}")
        
        return checkpoint_data
    except Exception as e:
        print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return None

def run_single_stock_full_analysis(stock, logger, log_dir, timestamp):
    """æ‰§è¡Œå•åªè‚¡ç¥¨çš„å…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ"""
    
    print(f"ğŸš€ å¼€å§‹{stock}å…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ...")
    logger.info(f"å¼€å§‹{stock}å…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = VectorbtWFOAnalyzer()
    
    # è®¾ç½®æµ‹è¯•å‚æ•° - ç¡®ä¿æ—¶åŒºä¸€è‡´æ€§
    analyzer.start_date = pd.to_datetime("2024-01-01").tz_localize('Asia/Hong_Kong')
    analyzer.end_date = pd.to_datetime("2025-09-01").tz_localize('Asia/Hong_Kong')
    analyzer.memory_limit_gb = 16.0
    
    # åŠ¨æ€è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°
    import psutil
    analyzer.n_workers = min(6, psutil.cpu_count(logical=False) or 4)
    
    # å…¨æ—¶é—´æ¡†æ¶
    all_timeframes = ['1m', '5m', '15m', '30m', '1h', '4h', '1d']
    
    # å…¨å› å­
    all_factors = ["RSI", "Price_Position", "Momentum_ROC", "MACD", "Volume_Ratio"]
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   è‚¡ç¥¨: {stock}")
    print(f"   æ—¶é—´æ¡†æ¶: {all_timeframes}")
    print(f"   å› å­: {all_factors}")
    print(f"   æ€»æµ‹è¯•æ•°: {len(all_timeframes) * len(all_factors)}")
    print(f"   æ—¶é—´èŒƒå›´: {analyzer.start_date} åˆ° {analyzer.end_date}")
    
    logger.info(f"æµ‹è¯•é…ç½®: {stock}, {all_timeframes}, {all_factors}")
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}
    
    # ä¸´æ—¶ä¿®æ”¹æ•°æ®åŠ è½½æ–¹æ³• - å…³é”®ï¼šåœ¨è‚¡ç¥¨çº§åˆ«ä¿®æ”¹
    original_load_method = analyzer.load_timeframe_data
    
    def load_single_stock(timeframe, symbols=None):
        return original_load_method(timeframe, [stock])
    
    analyzer.load_timeframe_data = load_single_stock
    
    # åˆ›å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    test_tasks = []
    for factor in all_factors:
        for timeframe in all_timeframes:
            test_tasks.append((factor, timeframe))
    
    print(f"ğŸ”„ å¼€å§‹æ‰§è¡Œ {len(test_tasks)} ä¸ªæµ‹è¯•...")
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    completed_count = 0
    successful_count = 0
    
    for factor, timeframe in test_tasks:
        try:
            print(f"   ğŸ“Š æµ‹è¯•: {factor} - {timeframe}")
            logger.info(f"å¼€å§‹æµ‹è¯•: {factor} - {timeframe}")
            
            # æ‰§è¡Œå•ä¸ªæµ‹è¯•
            result = analyzer.analyze_single_factor(factor, timeframe)
            
            # è®°å½•ç»“æœ
            key = f"{stock}_{factor}_{timeframe}"
            all_results[key] = {
                'success': True,
                'result': result,
                'factor': factor,
                'timeframe': timeframe,
                'robustness_score': result.get('robustness_score', 0),
                'ic_stats': result.get('ic_stats', {}),
                'signal_analysis': result.get('signal_analysis', {}),
                'vectorbt_results': result.get('vectorbt_results', {})
            }
            
            successful_count += 1
            logger.info(f"æµ‹è¯•æˆåŠŸ: {factor} - {timeframe}")
            
        except Exception as e:
            key = f"{stock}_{factor}_{timeframe}"
            error_msg = f"æ‰§è¡Œå¼‚å¸¸: {str(e)}\n{traceback.format_exc()}"
            all_results[key] = {
                'success': False,
                'error': error_msg,
                'factor': factor,
                'timeframe': timeframe
            }
            logger.error(f"æµ‹è¯•å¤±è´¥: {factor} - {timeframe} - {error_msg}")
        
        completed_count += 1
        
        # æ˜¾ç¤ºè¿›åº¦
        progress = completed_count / len(test_tasks) * 100
        print(f"      è¿›åº¦: {progress:.1f}% ({completed_count}/{len(test_tasks)})")
        
        # å®šæœŸæ¸…ç†å†…å­˜
        if completed_count % 5 == 0:
            gc.collect()
            analyzer.clear_cache()
    
    # æ¢å¤æ•°æ®åŠ è½½æ–¹æ³•
    analyzer.load_timeframe_data = original_load_method
    
    print(f"\nğŸ“Š {stock}å…¨é‡åˆ†æå®Œæˆ!")
    print(f"   æ€»æµ‹è¯•æ•°: {len(all_results)}")
    print(f"   æˆåŠŸæ•°: {successful_count}")
    print(f"   å¤±è´¥æ•°: {len(all_results) - successful_count}")
    print(f"   æˆåŠŸç‡: {successful_count/len(all_results)*100:.1f}%")
    
    logger.info(f"å…¨é‡åˆ†æå®Œæˆ: æ€»æµ‹è¯•{len(all_results)}, æˆåŠŸ{successful_count}")
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_single_stock_comprehensive_report(all_results, stock, all_factors, all_timeframes, log_dir, timestamp)
    
    return all_results

def generate_single_stock_comprehensive_report(all_results, stock, factors, timeframes, log_dir, timestamp):
    """ç”Ÿæˆå•åªè‚¡ç¥¨çš„ç»¼åˆåˆ†ææŠ¥å‘Š"""
    print(f"\nğŸ“‹ ç”Ÿæˆ{stock}ç»¼åˆåˆ†ææŠ¥å‘Š...")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_tests = len(all_results)
    successful_tests = sum(1 for r in all_results.values() if r.get('success', False))
    failed_tests = total_tests - successful_tests
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_content = []
    report_content.append(f"# {stock}å…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†ææŠ¥å‘Š\n")
    report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_content.append(f"åˆ†ææ‰¹æ¬¡: {timestamp}\n")
    
    # æµ‹è¯•æ‘˜è¦
    report_content.append("## æµ‹è¯•æ‘˜è¦\n")
    report_content.append(f"- åˆ†æè‚¡ç¥¨: {stock}\n")
    report_content.append(f"- æ€»æµ‹è¯•æ•°: {total_tests}\n")
    report_content.append(f"- æˆåŠŸæµ‹è¯•æ•°: {successful_tests}\n")
    report_content.append(f"- å¤±è´¥æµ‹è¯•æ•°: {failed_tests}\n")
    report_content.append(f"- æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%\n")
    report_content.append(f"- æµ‹è¯•å› å­æ•°: {len(factors)}\n")
    report_content.append(f"- æµ‹è¯•æ—¶é—´æ¡†æ¶æ•°: {len(timeframes)}\n\n")
    
    # æŒ‰å› å­åˆ†æç»“æœ
    report_content.append("## æŒ‰å› å­åˆ†æç»“æœ\n")
    factor_results = {}
    for key, result in all_results.items():
        if result.get('success', False):
            factor = result['factor']
            timeframe = result['timeframe']
            score = result['robustness_score']
            
            if factor not in factor_results:
                factor_results[factor] = []
            
            factor_results[factor].append({
                'timeframe': timeframe,
                'score': score,
                'ic_stats': result.get('ic_stats', {}),
                'signal_analysis': result.get('signal_analysis', {})
            })
    
    for factor in factors:
        if factor in factor_results:
            report_content.append(f"### {factor}\n")
            report_content.append("| æ—¶é—´æ¡†æ¶ | ç¨³å¥æ€§å¾—åˆ† | å¹³å‡IC | IC IR | ä¿¡å·è´¨é‡ |\n")
            report_content.append("|----------|------------|--------|-------|----------|\n")
            
            for item in factor_results[factor]:
                timeframe = item['timeframe']
                score = item['score']
                ic_stats = item['ic_stats']
                signal_analysis = item['signal_analysis']
                
                mean_ic = ic_stats.get('mean_ic', 0)
                ic_ir = ic_stats.get('ic_ir', 0)
                signal_quality = signal_analysis.get('aggregated_stats', {}).get('signal_quality_score', 0)
                
                report_content.append(f"| {timeframe} | {score:.2f} | {mean_ic:.4f} | {ic_ir:.2f} | {signal_quality:.1f} |\n")
            
            report_content.append("\n")
    
    # æœ€ä½³è¡¨ç°ç»„åˆ
    report_content.append("## æœ€ä½³è¡¨ç°ç»„åˆ (Top 10)\n")
    best_combinations = []
    for key, result in all_results.items():
        if result.get('success', False):
            factor = result['factor']
            timeframe = result['timeframe']
            score = result['robustness_score']
            ic_stats = result.get('ic_stats', {})
            signal_analysis = result.get('signal_analysis', {})
            
            mean_ic = ic_stats.get('mean_ic', 0)
            signal_quality = signal_analysis.get('aggregated_stats', {}).get('signal_quality_score', 0)
            
            best_combinations.append({
                'factor': factor,
                'timeframe': timeframe,
                'score': score,
                'mean_ic': mean_ic,
                'signal_quality': signal_quality
            })
    
    best_combinations.sort(key=lambda x: x['score'], reverse=True)
    
    report_content.append("| æ’å | å› å­ | æ—¶é—´æ¡†æ¶ | ç¨³å¥æ€§å¾—åˆ† | å¹³å‡IC | ä¿¡å·è´¨é‡ |\n")
    report_content.append("|------|------|----------|------------|--------|----------|\n")
    
    for i, combo in enumerate(best_combinations[:10]):
        report_content.append(f"| {i+1} | {combo['factor']} | {combo['timeframe']} | {combo['score']:.2f} | {combo['mean_ic']:.4f} | {combo['signal_quality']:.1f} |\n")
    
    report_content.append("\n")
    
    # æ—¶é—´æ¡†æ¶è¡¨ç°å¯¹æ¯”
    report_content.append("## æ—¶é—´æ¡†æ¶è¡¨ç°å¯¹æ¯”\n")
    timeframe_stats = {}
    for key, result in all_results.items():
        if result.get('success', False):
            timeframe = result['timeframe']
            score = result['robustness_score']
            
            if timeframe not in timeframe_stats:
                timeframe_stats[timeframe] = []
            
            timeframe_stats[timeframe].append(score)
    
    report_content.append("| æ—¶é—´æ¡†æ¶ | å¹³å‡å¾—åˆ† | æœ€é«˜å¾—åˆ† | æœ€ä½å¾—åˆ† | æµ‹è¯•æ•° |\n")
    report_content.append("|----------|----------|----------|----------|--------|\n")
    
    for timeframe in timeframes:
        if timeframe in timeframe_stats:
            scores = timeframe_stats[timeframe]
            avg_score = np.mean(scores)
            max_score = np.max(scores)
            min_score = np.min(scores)
            count = len(scores)
            
            report_content.append(f"| {timeframe} | {avg_score:.2f} | {max_score:.2f} | {min_score:.2f} | {count} |\n")
    
    report_content.append("\n")
    
    # å› å­è¡¨ç°å¯¹æ¯”
    report_content.append("## å› å­è¡¨ç°å¯¹æ¯”\n")
    factor_stats = {}
    for key, result in all_results.items():
        if result.get('success', False):
            factor = result['factor']
            score = result['robustness_score']
            
            if factor not in factor_stats:
                factor_stats[factor] = []
            
            factor_stats[factor].append(score)
    
    report_content.append("| å› å­ | å¹³å‡å¾—åˆ† | æœ€é«˜å¾—åˆ† | æœ€ä½å¾—åˆ† | æµ‹è¯•æ•° |\n")
    report_content.append("|------|----------|----------|----------|--------|\n")
    
    for factor in factors:
        if factor in factor_stats:
            scores = factor_stats[factor]
            avg_score = np.mean(scores)
            max_score = np.max(scores)
            min_score = np.min(scores)
            count = len(scores)
            
            report_content.append(f"| {factor} | {avg_score:.2f} | {max_score:.2f} | {min_score:.2f} | {count} |\n")
    
    report_content.append("\n")
    
    # é”™è¯¯åˆ†æ
    if failed_tests > 0:
        report_content.append("## é”™è¯¯åˆ†æ\n")
        for key, result in all_results.items():
            if not result.get('success', False):
                factor = result['factor']
                timeframe = result['timeframe']
                error = result.get('error', 'Unknown error')
                
                report_content.append(f"### {factor} - {timeframe}\n")
                report_content.append(f"```\n{error}\n```\n\n")
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = log_dir / f"{stock}_comprehensive_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.writelines(report_content)
    
    # ä¿å­˜JSONæ•°æ®
    json_file = log_dir / f"{stock}_full_results.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
    
    # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
    summary = {
        "analysis_timestamp": timestamp,
        "stock": stock,
        "total_tests": total_tests,
        "successful_tests": successful_tests,
        "failed_tests": failed_tests,
        "success_rate": successful_tests/total_tests*100,
        "best_factor": best_combinations[0]['factor'] if best_combinations else None,
        "best_timeframe": best_combinations[0]['timeframe'] if best_combinations else None,
        "best_score": best_combinations[0]['score'] if best_combinations else None,
        "factor_stats": {factor: {"avg_score": np.mean(scores), "max_score": np.max(scores), "min_score": np.min(scores)} 
                        for factor, scores in factor_stats.items()},
        "timeframe_stats": {timeframe: {"avg_score": np.mean(scores), "max_score": np.max(scores), "min_score": np.min(scores)} 
                           for timeframe, scores in timeframe_stats.items()}
    }
    
    summary_file = log_dir / "analysis_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2, default=int)
    
    print(f"   ğŸ“„ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print(f"   ğŸ“Š å®Œæ•´æ•°æ®å·²ä¿å­˜: {json_file}")
    print(f"   ğŸ“ˆ åˆ†ææ‘˜è¦å·²ä¿å­˜: {summary_file}")
    print(f"   ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")
    
    return summary

def get_all_hk_stocks():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ¸¯è‚¡è‚¡ç¥¨åˆ—è¡¨"""
    data_dir = Path("/Users/zhangshenshen/longport/vectorbt_workspace/data/1m")
    
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return []
    
    # è·å–æ‰€æœ‰parquetæ–‡ä»¶
    stock_files = list(data_dir.glob("*.HK.parquet"))
    
    # æå–è‚¡ç¥¨ä»£ç å¹¶æ’åº
    hk_stocks = sorted([f.stem for f in stock_files])
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(hk_stocks)} åªæ¸¯è‚¡")
    
    return hk_stocks

def run_sequential_analysis(start_index=0, end_index=None, overall_results=None, timestamp=None):
    """é€ä¸ªæ‰§è¡Œè‚¡ç¥¨åˆ†æ"""
    # è·å–æ‰€æœ‰è‚¡ç¥¨
    all_stocks = get_all_hk_stocks()
    
    if not all_stocks:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¸¯è‚¡æ•°æ®")
        return
    
    # è¿‡æ»¤æ‰0700.HKï¼ˆå·²ç»åˆ†æè¿‡äº†ï¼‰
    stocks_to_analyze = [stock for stock in all_stocks if stock != "0700.HK"]
    
    if end_index is None:
        end_index = len(stocks_to_analyze)
    
    # ç¡®ä¿ç´¢å¼•èŒƒå›´æœ‰æ•ˆ
    start_index = max(0, min(start_index, len(stocks_to_analyze) - 1))
    end_index = max(start_index, min(end_index, len(stocks_to_analyze)))
    
    selected_stocks = stocks_to_analyze[start_index:end_index]
    
    # å¦‚æœæ²¡æœ‰æä¾›overall_resultsï¼Œåˆå§‹åŒ–ç©ºåˆ—è¡¨
    if overall_results is None:
        overall_results = []
    
    # å¦‚æœæ²¡æœ‰æä¾›timestampï¼Œç”Ÿæˆæ–°çš„
    if timestamp is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    print(f"ğŸš€ å¼€å§‹é€ä¸ªè‚¡ç¥¨åˆ†æ")
    print(f"   æ€»è‚¡ç¥¨æ•°: {len(stocks_to_analyze)}")
    print(f"   åˆ†æèŒƒå›´: ç¬¬{start_index+1}åˆ°ç¬¬{end_index}åªè‚¡ç¥¨")
    print(f"   æœ¬æ¬¡åˆ†æ: {len(selected_stocks)}åªè‚¡ç¥¨")
    print(f"   è‚¡ç¥¨åˆ—è¡¨: {selected_stocks}")
    if overall_results:
        print(f"   å·²å®Œæˆè‚¡ç¥¨æ•°: {len(overall_results)}")
    
    # è®°å½•æ€»ä½“ç»“æœ
    overall_results = overall_results or []
    
    for i, stock in enumerate(selected_stocks):
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ åˆ†æè‚¡ç¥¨ {i+1}/{len(selected_stocks)}: {stock}")
        print(f"{'='*80}")
        
        try:
            # è®¾ç½®æ—¥å¿—è®°å½•
            logger, log_dir, timestamp = setup_timestamp_logging_for_stock(stock)
            
            # ä¿å­˜åˆ†æå‚æ•°
            parameters = save_analysis_parameters(log_dir, timestamp, stock)
            
            # æ‰§è¡Œåˆ†æ
            start_time = time.time()
            results = run_single_stock_full_analysis(stock, logger, log_dir, timestamp)
            end_time = time.time()
            
            # è®°å½•ç»“æœ
            successful_tests = sum(1 for r in results.values() if r.get('success', False))
            total_tests = len(results)
            success_rate = successful_tests / total_tests * 100 if total_tests > 0 else 0
            
            stock_result = {
                "stock": stock,
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": success_rate,
                "execution_time": end_time - start_time,
                "log_dir": str(log_dir),
                "timestamp": timestamp
            }
            
            overall_results.append(stock_result)
            
            print(f"\nâœ… {stock}åˆ†æå®Œæˆ!")
            print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"   æ‰§è¡Œæ—¶é—´: {end_time - start_time:.1f}ç§’")
            print(f"   æ—¥å¿—ç›®å½•: {log_dir}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            save_checkpoint(overall_results, start_index, end_index, timestamp)
            
            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…ç³»ç»Ÿè¿‡è½½
            if i < len(selected_stocks) - 1:
                print(f"â³ ä¼‘æ¯5ç§’åç»§ç»­...")
                time.sleep(5)
            
        except Exception as e:
            print(f"âŒ {stock}åˆ†æå¤±è´¥: {str(e)}")
            overall_results.append({
                "stock": stock,
                "total_tests": 0,
                "successful_tests": 0,
                "success_rate": 0,
                "execution_time": 0,
                "log_dir": None,
                "timestamp": None,
                "error": str(e)
            })
    
    # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
    generate_overall_report(overall_results, start_index, end_index)
    
    return overall_results

def generate_overall_report(overall_results, start_index, end_index):
    """ç”Ÿæˆæ€»ä½“åˆ†ææŠ¥å‘Š"""
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ ç”Ÿæˆæ€»ä½“åˆ†ææŠ¥å‘Š")
    print(f"{'='*80}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_stocks = len(overall_results)
    successful_stocks = sum(1 for r in overall_results if r['success_rate'] > 0)
    total_tests = sum(r['total_tests'] for r in overall_results)
    successful_tests = sum(r['successful_tests'] for r in overall_results)
    total_time = sum(r['execution_time'] for r in overall_results)
    
    overall_success_rate = successful_tests / total_tests * 100 if total_tests > 0 else 0
    
    # ç”ŸæˆæŠ¥å‘Š
    report_content = []
    report_content.append(f"# 53åªæ¸¯è‚¡é€ä¸ªåˆ†ææ€»ä½“æŠ¥å‘Š\n")
    report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report_content.append(f"åˆ†æèŒƒå›´: ç¬¬{start_index+1}åˆ°ç¬¬{end_index}åªè‚¡ç¥¨\n\n")
    
    # æ€»ä½“ç»Ÿè®¡
    report_content.append("## æ€»ä½“ç»Ÿè®¡\n")
    report_content.append(f"- åˆ†æè‚¡ç¥¨æ•°: {total_stocks}\n")
    report_content.append(f"- æˆåŠŸè‚¡ç¥¨æ•°: {successful_stocks}\n")
    report_content.append(f"- æ€»æµ‹è¯•æ•°: {total_tests}\n")
    report_content.append(f"- æˆåŠŸæµ‹è¯•æ•°: {successful_tests}\n")
    report_content.append(f"- æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.1f}%\n")
    report_content.append(f"- æ€»æ‰§è¡Œæ—¶é—´: {total_time:.1f}ç§’\n")
    report_content.append(f"- å¹³å‡æ¯åªè‚¡ç¥¨æ—¶é—´: {total_time/total_stocks:.1f}ç§’\n\n")
    
    # æ¯åªè‚¡ç¥¨çš„ç»“æœ
    report_content.append("## å„è‚¡ç¥¨åˆ†æç»“æœ\n")
    report_content.append("| è‚¡ç¥¨ | æ€»æµ‹è¯•æ•° | æˆåŠŸæµ‹è¯•æ•° | æˆåŠŸç‡ | æ‰§è¡Œæ—¶é—´(ç§’) | çŠ¶æ€ |\n")
    report_content.append("|------|----------|------------|--------|-------------|------|\n")
    
    for result in overall_results:
        stock = result['stock']
        total_tests = result['total_tests']
        successful_tests = result['successful_tests']
        success_rate = result['success_rate']
        execution_time = result['execution_time']
        status = "âœ… æˆåŠŸ" if success_rate > 0 else "âŒ å¤±è´¥"
        
        report_content.append(f"| {stock} | {total_tests} | {successful_tests} | {success_rate:.1f}% | {execution_time:.1f} | {status} |\n")
    
    report_content.append("\n")
    
    # æœ€ä½³è¡¨ç°è‚¡ç¥¨
    successful_results = [r for r in overall_results if r['success_rate'] > 0]
    if successful_results:
        successful_results.sort(key=lambda x: x['success_rate'], reverse=True)
        
        report_content.append("## æœ€ä½³è¡¨ç°è‚¡ç¥¨ (Top 10)\n")
        report_content.append("| æ’å | è‚¡ç¥¨ | æˆåŠŸç‡ | æˆåŠŸæµ‹è¯•æ•° | æ€»æµ‹è¯•æ•° | æ‰§è¡Œæ—¶é—´(ç§’) |\n")
        report_content.append("|------|------|--------|------------|----------|-------------|\n")
        
        for i, result in enumerate(successful_results[:10]):
            report_content.append(f"| {i+1} | {result['stock']} | {result['success_rate']:.1f}% | {result['successful_tests']} | {result['total_tests']} | {result['execution_time']:.1f} |\n")
    
    # ä¿å­˜æŠ¥å‘Š
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_dir = Path(f"logs/sequential_analysis_summary_{timestamp}")
    report_dir.mkdir(parents=True, exist_ok=True)
    
    report_file = report_dir / "overall_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.writelines(report_content)
    
    # ä¿å­˜JSONæ•°æ®
    json_file = report_dir / "overall_results.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(overall_results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"ğŸ“„ æ€»ä½“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print(f"ğŸ“Š å®Œæ•´æ•°æ®å·²ä¿å­˜: {json_file}")
    print(f"ğŸ“ æŠ¥å‘Šç›®å½•: {report_dir}")
    
    # æ‰“å°æ‘˜è¦
    print(f"\nğŸ‰ é€ä¸ªè‚¡ç¥¨åˆ†æå®Œæˆ!")
    print(f"   åˆ†æè‚¡ç¥¨æ•°: {total_stocks}")
    print(f"   æˆåŠŸè‚¡ç¥¨æ•°: {successful_stocks}")
    print(f"   æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.1f}%")
    print(f"   æ€»æ‰§è¡Œæ—¶é—´: {total_time:.1f}ç§’")

def main():
    parser = argparse.ArgumentParser(description='53åªæ¸¯è‚¡é€ä¸ªå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ')
    parser.add_argument('--start', type=int, default=0, help='å¼€å§‹ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰')
    parser.add_argument('--end', type=int, help='ç»“æŸç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰')
    parser.add_argument('--stock', type=str, help='æŒ‡å®šå•ä¸ªè‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š0005.HKï¼‰')
    parser.add_argument('--resume', type=str, help='ä»æ£€æŸ¥ç‚¹æ¢å¤åˆ†æï¼ˆæŒ‡å®šæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼‰')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸš€ 53åªæ¸¯è‚¡é€ä¸ªå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ")
    print("=" * 80)
    
    if args.stock:
        # åˆ†æå•ä¸ªè‚¡ç¥¨
        print(f"ğŸ“Š åˆ†æå•ä¸ªè‚¡ç¥¨: {args.stock}")
        
        try:
            # è®¾ç½®æ—¥å¿—è®°å½•
            logger, log_dir, timestamp = setup_timestamp_logging_for_stock(args.stock)
            
            # ä¿å­˜åˆ†æå‚æ•°
            parameters = save_analysis_parameters(log_dir, timestamp, args.stock)
            
            # æ‰§è¡Œåˆ†æ
            results = run_single_stock_full_analysis(args.stock, logger, log_dir, timestamp)
            
            print(f"\n" + "=" * 80)
            print(f"ğŸ‰ {args.stock}åˆ†æå®Œæˆ!")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {log_dir}")
            print("=" * 80)
            
        except Exception as e:
            print(f"âŒ {args.stock}åˆ†æå¤±è´¥: {str(e)}")
            raise
    elif args.resume:
        # ä»æ£€æŸ¥ç‚¹æ¢å¤åˆ†æ
        print(f"ğŸ“‚ ä»æ£€æŸ¥ç‚¹æ¢å¤åˆ†æ: {args.resume}")
        
        checkpoint_data = load_checkpoint(args.resume)
        if checkpoint_data:
            # ä»æ£€æŸ¥ç‚¹æ¢å¤åˆ†æ
            run_sequential_analysis(
                start_index=checkpoint_data['start_index'],
                end_index=checkpoint_data['end_index'],
                overall_results=checkpoint_data['overall_results'],
                timestamp=checkpoint_data['timestamp']
            )
        else:
            print("âŒ æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹ï¼Œé€€å‡º")
            sys.exit(1)
    else:
        # é€ä¸ªåˆ†æå¤šä¸ªè‚¡ç¥¨
        run_sequential_analysis(args.start, args.end)

if __name__ == "__main__":
    main()