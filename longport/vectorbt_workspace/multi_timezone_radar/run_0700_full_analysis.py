#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
0700.HKå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æå›æµ‹è„šæœ¬
åŸºäºæˆåŠŸçš„0700å•è‚¡æµ‹è¯•æ¨¡å¼ï¼Œé‡‡ç”¨å®Œæ•´çš„æ—¶é—´æ¡†æ¶å’Œå› å­è¦†ç›–
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging
import json
import gc
import traceback
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from core.vectorbt_wfo_analyzer import VectorbtWFOAnalyzer

def setup_timestamp_logging():
    """è®¾ç½®åŸºäºæ—¶é—´æˆ³çš„æ—¥å¿—è®°å½•"""
    # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = Path(f"logs/0700_analysis_{timestamp}")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file = log_dir / "0700_full_analysis.log"
    
    # æ¸…é™¤ç°æœ‰çš„æ—¥å¿—é…ç½®
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8', mode='w'),
            logging.StreamHandler()
        ],
        force=True
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç›®å½•: {log_dir}")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    # æµ‹è¯•æ—¥å¿—å†™å…¥
    logger.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    return logger, log_dir, timestamp

def save_analysis_parameters(log_dir, timestamp):
    """ä¿å­˜åˆ†æå‚æ•°é…ç½®"""
    parameters = {
        "analysis_type": "0700.HKå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ",
        "timestamp": timestamp,
        "stock": "0700.HK",
        "timeframes": ['1m', '5m', '15m', '30m', '1h', '4h', '1d'],
        "factors": ["RSI", "Price_Position", "Momentum_ROC", "MACD", "Volume_Ratio"],
        "start_date": "2024-01-01",
        "end_date": "2025-09-01",
        "memory_limit_gb": 16.0,
        "n_workers": 6,
        "analysis_mode": "single_stock_comprehensive"
    }
    
    params_file = log_dir / "analysis_parameters.json"
    with open(params_file, 'w', encoding='utf-8') as f:
        json.dump(parameters, f, ensure_ascii=False, indent=2)
    
    return parameters

def run_0700_comprehensive_analysis(logger, log_dir, timestamp):
    """æ‰§è¡Œ0700.HKå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ"""
    
    print("ğŸš€ å¼€å§‹0700.HKå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ...")
    logger.info("å¼€å§‹0700.HKå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = VectorbtWFOAnalyzer()
    
    # è®¾ç½®æµ‹è¯•å‚æ•°
    analyzer.start_date = pd.to_datetime("2024-01-01")
    analyzer.end_date = pd.to_datetime("2025-09-01")
    analyzer.memory_limit_gb = 16.0
    analyzer.n_workers = 6
    
    # æµ‹è¯•è‚¡ç¥¨
    test_stock = "0700.HK"
    
    # å…¨æ—¶é—´æ¡†æ¶
    all_timeframes = ['1m', '5m', '15m', '30m', '1h', '4h', '1d']
    
    # å…¨å› å­
    all_factors = ["RSI", "Price_Position", "Momentum_ROC", "MACD", "Volume_Ratio"]
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   è‚¡ç¥¨: {test_stock}")
    print(f"   æ—¶é—´æ¡†æ¶: {all_timeframes}")
    print(f"   å› å­: {all_factors}")
    print(f"   æ€»æµ‹è¯•æ•°: {len(all_timeframes) * len(all_factors)}")
    print(f"   æ—¶é—´èŒƒå›´: {analyzer.start_date} åˆ° {analyzer.end_date}")
    
    logger.info(f"æµ‹è¯•é…ç½®: {test_stock}, {all_timeframes}, {all_factors}")
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}
    
    # ä¸´æ—¶ä¿®æ”¹æ•°æ®åŠ è½½æ–¹æ³• - å…³é”®ï¼šåœ¨è‚¡ç¥¨çº§åˆ«ä¿®æ”¹
    original_load_method = analyzer.load_timeframe_data
    
    def load_single_stock(timeframe, symbols=None):
        return original_load_method(timeframe, [test_stock])
    
    analyzer.load_timeframe_data = load_single_stock
    
    # åˆ›å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    test_tasks = []
    for factor in all_factors:
        for timeframe in all_timeframes:
            test_tasks.append((factor, timeframe))
    
    print(f"ğŸ”„ å¼€å§‹æ‰§è¡Œ {len(test_tasks)} ä¸ªæµ‹è¯•...")
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    completed_count = 0
    successful_count = 0
    
    for factor, timeframe in test_tasks:
        try:
            print(f"   ğŸ“Š æµ‹è¯•: {factor} - {timeframe}")
            logger.info(f"å¼€å§‹æµ‹è¯•: {factor} - {timeframe}")
            
            # æ‰§è¡Œå•ä¸ªæµ‹è¯•
            result = analyzer.analyze_single_factor(factor, timeframe)
            
            # è®°å½•ç»“æœ
            key = f"{test_stock}_{factor}_{timeframe}"
            all_results[key] = {
                'success': True,
                'result': result,
                'factor': factor,
                'timeframe': timeframe,
                'robustness_score': result.get('robustness_score', 0),
                'ic_stats': result.get('ic_stats', {}),
                'signal_analysis': result.get('signal_analysis', {}),
                'vectorbt_results': result.get('vectorbt_results', {})
            }
            
            successful_count += 1
            logger.info(f"æµ‹è¯•æˆåŠŸ: {factor} - {timeframe}")
            
        except Exception as e:
            key = f"{test_stock}_{factor}_{timeframe}"
            error_msg = f"æ‰§è¡Œå¼‚å¸¸: {str(e)}\n{traceback.format_exc()}"
            all_results[key] = {
                'success': False,
                'error': error_msg,
                'factor': factor,
                'timeframe': timeframe
            }
            logger.error(f"æµ‹è¯•å¤±è´¥: {factor} - {timeframe} - {error_msg}")
        
        completed_count += 1
        
        # æ˜¾ç¤ºè¿›åº¦
        progress = completed_count / len(test_tasks) * 100
        print(f"      è¿›åº¦: {progress:.1f}% ({completed_count}/{len(test_tasks)})")
        
        # å®šæœŸæ¸…ç†å†…å­˜
        if completed_count % 5 == 0:
            gc.collect()
    
    # æ¢å¤æ•°æ®åŠ è½½æ–¹æ³•
    analyzer.load_timeframe_data = original_load_method
    
    print(f"\nğŸ“Š 0700.HKå…¨é‡åˆ†æå®Œæˆ!")
    print(f"   æ€»æµ‹è¯•æ•°: {len(all_results)}")
    print(f"   æˆåŠŸæ•°: {successful_count}")
    print(f"   å¤±è´¥æ•°: {len(all_results) - successful_count}")
    print(f"   æˆåŠŸç‡: {successful_count/len(all_results)*100:.1f}%")
    
    logger.info(f"å…¨é‡åˆ†æå®Œæˆ: æ€»æµ‹è¯•{len(all_results)}, æˆåŠŸ{successful_count}")
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_0700_comprehensive_report(all_results, test_stock, all_factors, all_timeframes, log_dir, timestamp)
    
    return all_results

def generate_0700_comprehensive_report(all_results, stock, factors, timeframes, log_dir, timestamp):
    """ç”Ÿæˆ0700.HKç»¼åˆåˆ†ææŠ¥å‘Š"""
    print(f"\nğŸ“‹ ç”Ÿæˆ0700.HKç»¼åˆåˆ†ææŠ¥å‘Š...")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_tests = len(all_results)
    successful_tests = sum(1 for r in all_results.values() if r.get('success', False))
    failed_tests = total_tests - successful_tests
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_content = []
    report_content.append(f"# 0700.HKå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†ææŠ¥å‘Š\n")
    report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_content.append(f"åˆ†ææ‰¹æ¬¡: {timestamp}\n")
    
    # æµ‹è¯•æ‘˜è¦
    report_content.append("## æµ‹è¯•æ‘˜è¦\n")
    report_content.append(f"- åˆ†æè‚¡ç¥¨: {stock}\n")
    report_content.append(f"- æ€»æµ‹è¯•æ•°: {total_tests}\n")
    report_content.append(f"- æˆåŠŸæµ‹è¯•æ•°: {successful_tests}\n")
    report_content.append(f"- å¤±è´¥æµ‹è¯•æ•°: {failed_tests}\n")
    report_content.append(f"- æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%\n")
    report_content.append(f"- æµ‹è¯•å› å­æ•°: {len(factors)}\n")
    report_content.append(f"- æµ‹è¯•æ—¶é—´æ¡†æ¶æ•°: {len(timeframes)}\n\n")
    
    # æŒ‰å› å­åˆ†æç»“æœ
    report_content.append("## æŒ‰å› å­åˆ†æç»“æœ\n")
    factor_results = {}
    for key, result in all_results.items():
        if result.get('success', False):
            factor = result['factor']
            timeframe = result['timeframe']
            score = result['robustness_score']
            
            if factor not in factor_results:
                factor_results[factor] = []
            
            factor_results[factor].append({
                'timeframe': timeframe,
                'score': score,
                'ic_stats': result.get('ic_stats', {}),
                'signal_analysis': result.get('signal_analysis', {})
            })
    
    for factor in factors:
        if factor in factor_results:
            report_content.append(f"### {factor}\n")
            report_content.append("| æ—¶é—´æ¡†æ¶ | ç¨³å¥æ€§å¾—åˆ† | å¹³å‡IC | IC IR | ä¿¡å·è´¨é‡ |\n")
            report_content.append("|----------|------------|--------|-------|----------|\n")
            
            for item in factor_results[factor]:
                timeframe = item['timeframe']
                score = item['score']
                ic_stats = item['ic_stats']
                signal_analysis = item['signal_analysis']
                
                mean_ic = ic_stats.get('mean_ic', 0)
                ic_ir = ic_stats.get('ic_ir', 0)
                signal_quality = signal_analysis.get('aggregated_stats', {}).get('signal_quality_score', 0)
                
                report_content.append(f"| {timeframe} | {score:.2f} | {mean_ic:.4f} | {ic_ir:.2f} | {signal_quality:.1f} |\n")
            
            report_content.append("\n")
    
    # æœ€ä½³è¡¨ç°ç»„åˆ
    report_content.append("## æœ€ä½³è¡¨ç°ç»„åˆ (Top 10)\n")
    best_combinations = []
    for key, result in all_results.items():
        if result.get('success', False):
            factor = result['factor']
            timeframe = result['timeframe']
            score = result['robustness_score']
            ic_stats = result.get('ic_stats', {})
            signal_analysis = result.get('signal_analysis', {})
            
            mean_ic = ic_stats.get('mean_ic', 0)
            signal_quality = signal_analysis.get('aggregated_stats', {}).get('signal_quality_score', 0)
            
            best_combinations.append({
                'factor': factor,
                'timeframe': timeframe,
                'score': score,
                'mean_ic': mean_ic,
                'signal_quality': signal_quality
            })
    
    best_combinations.sort(key=lambda x: x['score'], reverse=True)
    
    report_content.append("| æ’å | å› å­ | æ—¶é—´æ¡†æ¶ | ç¨³å¥æ€§å¾—åˆ† | å¹³å‡IC | ä¿¡å·è´¨é‡ |\n")
    report_content.append("|------|------|----------|------------|--------|----------|\n")
    
    for i, combo in enumerate(best_combinations[:10]):
        report_content.append(f"| {i+1} | {combo['factor']} | {combo['timeframe']} | {combo['score']:.2f} | {combo['mean_ic']:.4f} | {combo['signal_quality']:.1f} |\n")
    
    report_content.append("\n")
    
    # æ—¶é—´æ¡†æ¶è¡¨ç°å¯¹æ¯”
    report_content.append("## æ—¶é—´æ¡†æ¶è¡¨ç°å¯¹æ¯”\n")
    timeframe_stats = {}
    for key, result in all_results.items():
        if result.get('success', False):
            timeframe = result['timeframe']
            score = result['robustness_score']
            
            if timeframe not in timeframe_stats:
                timeframe_stats[timeframe] = []
            
            timeframe_stats[timeframe].append(score)
    
    report_content.append("| æ—¶é—´æ¡†æ¶ | å¹³å‡å¾—åˆ† | æœ€é«˜å¾—åˆ† | æœ€ä½å¾—åˆ† | æµ‹è¯•æ•° |\n")
    report_content.append("|----------|----------|----------|----------|--------|\n")
    
    for timeframe in timeframes:
        if timeframe in timeframe_stats:
            scores = timeframe_stats[timeframe]
            avg_score = np.mean(scores)
            max_score = np.max(scores)
            min_score = np.min(scores)
            count = len(scores)
            
            report_content.append(f"| {timeframe} | {avg_score:.2f} | {max_score:.2f} | {min_score:.2f} | {count} |\n")
    
    report_content.append("\n")
    
    # å› å­è¡¨ç°å¯¹æ¯”
    report_content.append("## å› å­è¡¨ç°å¯¹æ¯”\n")
    factor_stats = {}
    for key, result in all_results.items():
        if result.get('success', False):
            factor = result['factor']
            score = result['robustness_score']
            
            if factor not in factor_stats:
                factor_stats[factor] = []
            
            factor_stats[factor].append(score)
    
    report_content.append("| å› å­ | å¹³å‡å¾—åˆ† | æœ€é«˜å¾—åˆ† | æœ€ä½å¾—åˆ† | æµ‹è¯•æ•° |\n")
    report_content.append("|------|----------|----------|----------|--------|\n")
    
    for factor in factors:
        if factor in factor_stats:
            scores = factor_stats[factor]
            avg_score = np.mean(scores)
            max_score = np.max(scores)
            min_score = np.min(scores)
            count = len(scores)
            
            report_content.append(f"| {factor} | {avg_score:.2f} | {max_score:.2f} | {min_score:.2f} | {count} |\n")
    
    report_content.append("\n")
    
    # é”™è¯¯åˆ†æ
    if failed_tests > 0:
        report_content.append("## é”™è¯¯åˆ†æ\n")
        for key, result in all_results.items():
            if not result.get('success', False):
                factor = result['factor']
                timeframe = result['timeframe']
                error = result.get('error', 'Unknown error')
                
                report_content.append(f"### {factor} - {timeframe}\n")
                report_content.append(f"```\n{error}\n```\n\n")
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = log_dir / "0700_comprehensive_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.writelines(report_content)
    
    # ä¿å­˜JSONæ•°æ®
    json_file = log_dir / "0700_full_results.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
    
    # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
    summary = {
        "analysis_timestamp": timestamp,
        "stock": stock,
        "total_tests": total_tests,
        "successful_tests": successful_tests,
        "failed_tests": failed_tests,
        "success_rate": successful_tests/total_tests*100,
        "best_factor": best_combinations[0]['factor'] if best_combinations else None,
        "best_timeframe": best_combinations[0]['timeframe'] if best_combinations else None,
        "best_score": best_combinations[0]['score'] if best_combinations else None,
        "factor_stats": {factor: {"avg_score": np.mean(scores), "max_score": np.max(scores), "min_score": np.min(scores)} 
                        for factor, scores in factor_stats.items()},
        "timeframe_stats": {timeframe: {"avg_score": np.mean(scores), "max_score": np.max(scores), "min_score": np.min(scores)} 
                           for timeframe, scores in timeframe_stats.items()}
    }
    
    summary_file = log_dir / "analysis_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ“„ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print(f"   ğŸ“Š å®Œæ•´æ•°æ®å·²ä¿å­˜: {json_file}")
    print(f"   ğŸ“ˆ åˆ†ææ‘˜è¦å·²ä¿å­˜: {summary_file}")
    print(f"   ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")
    
    return summary

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸš€ 0700.HKå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æ")
    print("=" * 80)
    
    # è®¾ç½®æ—¥å¿—è®°å½•
    logger, log_dir, timestamp = setup_timestamp_logging()
    
    # ä¿å­˜åˆ†æå‚æ•°
    parameters = save_analysis_parameters(log_dir, timestamp)
    
    try:
        # æ‰§è¡Œå…¨é‡åˆ†æ
        results = run_0700_comprehensive_analysis(logger, log_dir, timestamp)
        
        print(f"\n" + "=" * 80)
        print("ğŸ‰ 0700.HKå…¨æ—¶é—´æ¡†æ¶å…¨å› å­åˆ†æå®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {log_dir}")
        print("=" * 80)
        
    except Exception as e:
        logger.error(f"åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        print(f"âŒ åˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}")
        raise