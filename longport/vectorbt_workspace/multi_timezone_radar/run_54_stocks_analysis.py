#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
54åªæ¸¯è‚¡å…¨é‡å› å­åˆ†æå›æµ‹è„šæœ¬
åŸºäº0700å•è‚¡æµ‹è¯•çš„æˆåŠŸæ¨¡å¼ï¼Œé‡‡ç”¨å…¨å±€æ•°æ®åŠ è½½æ–¹æ³•ä¿®æ”¹
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging
import json
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from core.vectorbt_wfo_analyzer import VectorbtWFOAnalyzer

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('full_54_stocks_analysis_fixed.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

def load_hk_stocks():
    """åŠ è½½æ¸¯è‚¡è‚¡ç¥¨åˆ—è¡¨ - åŸºäºå®é™…å¯ç”¨çš„æ•°æ®"""
    # ä»æ•°æ®ç›®å½•ä¸­è¯»å–å®é™…å¯ç”¨çš„è‚¡ç¥¨
    data_dir = Path("/Users/zhangshenshen/longport/vectorbt_workspace/data/1m")
    
    if not data_dir.exists():
        logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return []
    
    # è·å–æ‰€æœ‰parquetæ–‡ä»¶
    stock_files = list(data_dir.glob("*.parquet"))
    
    # æå–è‚¡ç¥¨ä»£ç 
    hk_stocks = [f.stem for f in stock_files if f.stem.endswith('.HK')]
    
    logger.info(f"ä»æ•°æ®ç›®å½•åŠ è½½äº† {len(hk_stocks)} åªè‚¡ç¥¨")
    
    return hk_stocks

def run_full_analysis():
    """æ‰§è¡Œ54åªè‚¡ç¥¨å…¨é‡åˆ†æ"""
    
    print("ğŸš€ å¼€å§‹54åªæ¸¯è‚¡å…¨é‡å› å­åˆ†æ...")
    logger.info("å¼€å§‹54åªæ¸¯è‚¡å…¨é‡å› å­åˆ†æ")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = VectorbtWFOAnalyzer()
    
    # è®¾ç½®æµ‹è¯•å‚æ•°
    analyzer.start_date = pd.to_datetime("2024-01-01")  # æ‰©å¤§æ—¶é—´èŒƒå›´
    analyzer.end_date = pd.to_datetime("2025-09-01")
    analyzer.memory_limit_gb = 16.0
    analyzer.n_workers = 6  # å‡å°‘å·¥ä½œè¿›ç¨‹é¿å…å†…å­˜è¿‡è½½
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    all_stocks = load_hk_stocks()
    
    # æµ‹è¯•å› å­ - é€‰æ‹©æ ¸å¿ƒå› å­
    test_factors = ["RSI", "Price_Position", "Momentum_ROC", "MACD", "Volume_Ratio"]
    
    # ä¸»è¦æ—¶é—´æ¡†æ¶ - é€‰æ‹©å…³é”®æ—¶é—´æ¡†æ¶
    key_timeframes = ['1m', '5m', '15m', '30m', '1h', '4h', '1d']
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   è‚¡ç¥¨æ•°é‡: {len(all_stocks)}")
    print(f"   å› å­: {test_factors}")
    print(f"   æ—¶é—´æ¡†æ¶: {key_timeframes}")
    print(f"   æ€»æµ‹è¯•æ•°: {len(all_stocks) * len(test_factors) * len(key_timeframes)}")
    print(f"   æ—¶é—´èŒƒå›´: {analyzer.start_date} åˆ° {analyzer.end_date}")
    
    logger.info(f"æµ‹è¯•é…ç½®: {len(all_stocks)}åªè‚¡ç¥¨, {test_factors}, {key_timeframes}")
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}
    
    # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨ï¼Œé¿å…å†…å­˜è¿‡è½½
    batch_size = 9  # æ¯æ‰¹9åªè‚¡ç¥¨ï¼Œå…±6æ‰¹
    total_batches = (len(all_stocks) + batch_size - 1) // batch_size
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(all_stocks))
        batch_stocks = all_stocks[start_idx:end_idx]
        
        print(f"\nğŸ”„ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹è‚¡ç¥¨: {batch_stocks}")
        logger.info(f"å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹è‚¡ç¥¨: {batch_stocks}")
        
        # æ‰¹é‡åˆ†æ
        batch_results = analyze_stock_batch(analyzer, batch_stocks, test_factors, key_timeframes)
        
        # åˆå¹¶ç»“æœ
        all_results.update(batch_results)
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        
        print(f"âœ… ç¬¬ {batch_idx + 1} æ‰¹å®Œæˆ")
        logger.info(f"ç¬¬ {batch_idx + 1} æ‰¹å®Œæˆ")
    
    print(f"\nğŸ“Š å…¨é‡åˆ†æå®Œæˆ!")
    successful_count = sum(1 for r in all_results.values() if r.get('success', False))
    print(f"   æ€»æµ‹è¯•æ•°: {len(all_results)}")
    print(f"   æˆåŠŸæ•°: {successful_count}")
    print(f"   å¤±è´¥æ•°: {len(all_results) - successful_count}")
    
    logger.info(f"å…¨é‡åˆ†æå®Œæˆ: æ€»æµ‹è¯•{len(all_results)}, æˆåŠŸ{successful_count}")
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_comprehensive_report(all_results, all_stocks, test_factors, key_timeframes)
    
    return all_results

def analyze_stock_batch(analyzer, stocks, factors, timeframes):
    """åˆ†æä¸€æ‰¹è‚¡ç¥¨ - å‚è€ƒ0700å•è‚¡æµ‹è¯•æ¨¡å¼"""
    batch_results = {}
    
    # ä¸ºæ¯åªè‚¡ç¥¨åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    for stock in stocks:
        print(f"   ğŸ“Š å¤„ç†è‚¡ç¥¨: {stock}")
        
        # ä¸´æ—¶ä¿®æ”¹æ•°æ®åŠ è½½æ–¹æ³• - å…³é”®ï¼šåœ¨è‚¡ç¥¨çº§åˆ«ä¿®æ”¹ï¼Œä¸æ˜¯åœ¨æµ‹è¯•çº§åˆ«
        original_load_method = analyzer.load_timeframe_data
        
        def load_single_stock(timeframe, symbols=None):
            return original_load_method(timeframe, [stock])
        
        analyzer.load_timeframe_data = load_single_stock
        
        # åˆ›å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
        test_tasks = []
        for factor in factors:
            for timeframe in timeframes:
                test_tasks.append((factor, timeframe))
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è¿™åªè‚¡ç¥¨çš„æ‰€æœ‰æµ‹è¯•
        completed_count = 0
        successful_count = 0
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(run_single_test_for_stock, analyzer, factor, timeframe): (factor, timeframe)
                for factor, timeframe in test_tasks
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_task):
                factor, timeframe = future_to_task[future]
                try:
                    result = future.result()
                    key = f"{stock}_{factor}_{timeframe}"
                    batch_results[key] = result
                    
                    completed_count += 1
                    success = result.get('success', False)
                    if success:
                        successful_count += 1
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if completed_count % 10 == 0:
                        progress = completed_count / len(test_tasks) * 100
                        print(f"      {stock} è¿›åº¦: {progress:.1f}% ({completed_count}/{len(test_tasks)})")
                    
                    # è®°å½•é”™è¯¯
                    if not success:
                        error_msg = result.get('error', 'Unknown error')
                        logger.error(f"æµ‹è¯•å¤±è´¥ {stock} {factor} {timeframe}: {error_msg}")
                
                except Exception as e:
                    key = f"{stock}_{factor}_{timeframe}"
                    error_msg = f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                    batch_results[key] = {'success': False, 'error': error_msg, 'stock': stock, 'factor': factor, 'timeframe': timeframe}
                    logger.error(f"æµ‹è¯•å¼‚å¸¸ {stock} {factor} {timeframe}: {error_msg}")
                    completed_count += 1
        
        # æ¢å¤æ•°æ®åŠ è½½æ–¹æ³•
        analyzer.load_timeframe_data = original_load_method
        
        print(f"   âœ… {stock} å®Œæˆ: {successful_count}/{len(test_tasks)} æˆåŠŸ")
    
    return batch_results

def run_single_test_for_stock(analyzer, factor, timeframe):
    """ä¸ºå•åªè‚¡ç¥¨è¿è¡Œå•ä¸ªæµ‹è¯• - å‚è€ƒ0700çš„run_single_testå‡½æ•°"""
    try:
        # ç›´æ¥è°ƒç”¨analyze_single_factorï¼Œä¸éœ€è¦ä¼ é€’symbolså‚æ•°
        # å› ä¸ºå·²ç»åœ¨è‚¡ç¥¨çº§åˆ«ä¿®æ”¹äº†load_timeframe_dataæ–¹æ³•
        result = analyzer.analyze_single_factor(factor, timeframe)
        
        if 'error' in result:
            return {
                'success': False,
                'error': result['error'],
                'factor': factor,
                'timeframe': timeframe
            }
        
        return {
            'success': True,
            'result': result,
            'factor': factor,
            'timeframe': timeframe,
            'robustness_score': result.get('robustness_score', 0),
            'ic_stats': result.get('ic_stats', {}),
            'signal_analysis': result.get('signal_analysis', {}),
            'vectorbt_results': result.get('vectorbt_results', {})
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': f"æ‰§è¡Œå¼‚å¸¸: {str(e)}\n{traceback.format_exc()}",
            'factor': factor,
            'timeframe': timeframe
        }

def generate_comprehensive_report(all_results, stocks, factors, timeframes):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    print(f"\nğŸ“‹ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
    
    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    report_dir = Path(f"full_54_stocks_analysis_fixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    report_dir.mkdir(exist_ok=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_tests = len(all_results)
    successful_tests = sum(1 for r in all_results.values() if r.get('success', False))
    failed_tests = total_tests - successful_tests
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_content = []
    report_content.append(f"# 54åªæ¸¯è‚¡å…¨é‡å› å­åˆ†ææŠ¥å‘Š (ä¿®æ­£ç‰ˆ)\n")
    report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # æµ‹è¯•æ‘˜è¦
    report_content.append("## æµ‹è¯•æ‘˜è¦\n")
    report_content.append(f"- æ€»æµ‹è¯•æ•°: {total_tests}\n")
    report_content.append(f"- æˆåŠŸæµ‹è¯•æ•°: {successful_tests}\n")
    report_content.append(f"- å¤±è´¥æµ‹è¯•æ•°: {failed_tests}\n")
    report_content.append(f"- æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%\n")
    report_content.append(f"- æµ‹è¯•è‚¡ç¥¨æ•°: {len(stocks)}\n")
    report_content.append(f"- æµ‹è¯•å› å­æ•°: {len(factors)}\n")
    report_content.append(f"- æµ‹è¯•æ—¶é—´æ¡†æ¶æ•°: {len(timeframes)}\n\n")
    
    # æœ€ä½³å› å­åˆ†æ
    report_content.append("## æœ€ä½³å› å­åˆ†æ\n")
    best_factors = analyze_best_factors(all_results)
    for i, (factor, stats) in enumerate(best_factors):
        if stats['count'] > 0:  # åªæ˜¾ç¤ºæœ‰æ•°æ®çš„å› å­
            report_content.append(f"{i+1}. **{factor}**: å¹³å‡å¾—åˆ† {stats['avg_score']:.2f}, æˆåŠŸç‡ {stats['success_rate']:.1f}%\n")
    report_content.append("\n")
    
    # æœ€ä½³æ—¶é—´æ¡†æ¶åˆ†æ
    report_content.append("## æœ€ä½³æ—¶é—´æ¡†æ¶åˆ†æ\n")
    best_timeframes = analyze_best_timeframes(all_results)
    for i, (timeframe, stats) in enumerate(best_timeframes):
        if stats['count'] > 0:  # åªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¶é—´æ¡†æ¶
            report_content.append(f"{i+1}. **{timeframe}**: å¹³å‡å¾—åˆ† {stats['avg_score']:.2f}, æˆåŠŸç‡ {stats['success_rate']:.1f}%\n")
    report_content.append("\n")
    
    # æœ€ä½³è‚¡ç¥¨åˆ†æ
    report_content.append("## æœ€ä½³è‚¡ç¥¨åˆ†æ (Top 10)\n")
    best_stocks = analyze_best_stocks(all_results)
    for i, (stock, stats) in enumerate(best_stocks[:10]):
        if stats['count'] > 0:  # åªæ˜¾ç¤ºæœ‰æ•°æ®çš„è‚¡ç¥¨
            report_content.append(f"{i+1}. **{stock}**: å¹³å‡å¾—åˆ† {stats['avg_score']:.2f}, æˆåŠŸç‡ {stats['success_rate']:.1f}%\n")
    report_content.append("\n")
    
    # æœ€ä½³ç»„åˆåˆ†æ
    report_content.append("## æœ€ä½³ç»„åˆåˆ†æ (Top 20)\n")
    best_combinations = analyze_best_combinations(all_results)
    report_content.append("| è‚¡ç¥¨ | æ—¶é—´æ¡†æ¶ | å› å­ | ç¨³å¥æ€§å¾—åˆ† | å¹³å‡IC | ä¿¡å·è´¨é‡ |\n")
    report_content.append("|------|----------|------|------------|--------|----------|\n")
    
    for i, combo in enumerate(best_combinations[:20]):
        stock = combo['stock']
        timeframe = combo['timeframe']
        factor = combo['factor']
        score = combo['score']
        avg_ic = combo.get('avg_ic', 0)
        signal_quality = combo.get('signal_quality', 0)
        
        report_content.append(f"| {stock} | {timeframe} | {factor} | {score:.2f} | {avg_ic:.4f} | {signal_quality:.1f} |\n")
    
    report_content.append("\n")
    
    # é”™è¯¯åˆ†æ
    if failed_tests > 0:
        report_content.append("## é”™è¯¯åˆ†æ\n")
        error_analysis = analyze_errors(all_results)
        for error_type, count in error_analysis.items():
            report_content.append(f"- **{error_type}**: {count}æ¬¡\n")
        report_content.append("\n")
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = report_dir / "comprehensive_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.writelines(report_content)
    
    # ä¿å­˜JSONæ•°æ®
    json_file = report_dir / "full_results.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"   ğŸ“„ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print(f"   ğŸ“Š å®Œæ•´æ•°æ®å·²ä¿å­˜: {json_file}")
    print(f"   ğŸ“ æŠ¥å‘Šç›®å½•: {report_dir}")
    
    logger.info(f"ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def analyze_best_factors(all_results):
    """åˆ†ææœ€ä½³å› å­"""
    factor_stats = {}
    
    for key, result in all_results.items():
        if result.get('success', False):
            factor = result['factor']
            score = result['robustness_score']
            
            if factor not in factor_stats:
                factor_stats[factor] = {'scores': [], 'count': 0, 'success': 0}
            
            factor_stats[factor]['scores'].append(score)
            factor_stats[factor]['count'] += 1
            factor_stats[factor]['success'] += 1
        else:
            factor = result['factor']
            if factor not in factor_stats:
                factor_stats[factor] = {'scores': [], 'count': 0, 'success': 0}
            factor_stats[factor]['count'] += 1
    
    # è®¡ç®—å¹³å‡å¾—åˆ†å’ŒæˆåŠŸç‡
    best_factors = []
    for factor, stats in factor_stats.items():
        if stats['scores']:
            avg_score = np.mean(stats['scores'])
            success_rate = stats['success'] / stats['count'] * 100
            best_factors.append((factor, {'avg_score': avg_score, 'success_rate': success_rate, 'count': stats['count']}))
    
    best_factors.sort(key=lambda x: x[1]['avg_score'], reverse=True)
    return best_factors

def analyze_best_timeframes(all_results):
    """åˆ†ææœ€ä½³æ—¶é—´æ¡†æ¶"""
    timeframe_stats = {}
    
    for key, result in all_results.items():
        if result.get('success', False):
            timeframe = result['timeframe']
            score = result['robustness_score']
            
            if timeframe not in timeframe_stats:
                timeframe_stats[timeframe] = {'scores': [], 'count': 0, 'success': 0}
            
            timeframe_stats[timeframe]['scores'].append(score)
            timeframe_stats[timeframe]['count'] += 1
            timeframe_stats[timeframe]['success'] += 1
        else:
            timeframe = result['timeframe']
            if timeframe not in timeframe_stats:
                timeframe_stats[timeframe] = {'scores': [], 'count': 0, 'success': 0}
            timeframe_stats[timeframe]['count'] += 1
    
    # è®¡ç®—å¹³å‡å¾—åˆ†å’ŒæˆåŠŸç‡
    best_timeframes = []
    for timeframe, stats in timeframe_stats.items():
        if stats['scores']:
            avg_score = np.mean(stats['scores'])
            success_rate = stats['success'] / stats['count'] * 100
            best_timeframes.append((timeframe, {'avg_score': avg_score, 'success_rate': success_rate, 'count': stats['count']}))
    
    best_timeframes.sort(key=lambda x: x[1]['avg_score'], reverse=True)
    return best_timeframes

def analyze_best_stocks(all_results):
    """åˆ†ææœ€ä½³è‚¡ç¥¨"""
    stock_stats = {}
    
    for key, result in all_results.items():
        if result.get('success', False):
            # ä»keyä¸­æå–è‚¡ç¥¨ä»£ç ï¼Œæ ¼å¼ä¸º "stock_factor_timeframe"
            stock = key.split('_')[0]
            score = result['robustness_score']
            
            if stock not in stock_stats:
                stock_stats[stock] = {'scores': [], 'count': 0, 'success': 0}
            
            stock_stats[stock]['scores'].append(score)
            stock_stats[stock]['count'] += 1
            stock_stats[stock]['success'] += 1
        else:
            stock = key.split('_')[0]
            if stock not in stock_stats:
                stock_stats[stock] = {'scores': [], 'count': 0, 'success': 0}
            stock_stats[stock]['count'] += 1
    
    # è®¡ç®—å¹³å‡å¾—åˆ†å’ŒæˆåŠŸç‡
    best_stocks = []
    for stock, stats in stock_stats.items():
        if stats['scores']:
            avg_score = np.mean(stats['scores'])
            success_rate = stats['success'] / stats['count'] * 100
            best_stocks.append((stock, {'avg_score': avg_score, 'success_rate': success_rate, 'count': stats['count']}))
    
    best_stocks.sort(key=lambda x: x[1]['avg_score'], reverse=True)
    return best_stocks

def analyze_best_combinations(all_results):
    """åˆ†ææœ€ä½³ç»„åˆ"""
    best_combinations = []
    
    for key, result in all_results.items():
        if result.get('success', False):
            # ä»keyä¸­è§£æä¿¡æ¯
            parts = key.split('_')
            stock = parts[0]
            factor = parts[1]
            timeframe = parts[2]
            score = result['robustness_score']
            
            ic_stats = result.get('ic_stats', {})
            signal_analysis = result.get('signal_analysis', {})
            
            avg_ic = ic_stats.get('mean_ic', 0)
            signal_quality = signal_analysis.get('aggregated_stats', {}).get('signal_quality_score', 0)
            
            best_combinations.append({
                'stock': stock,
                'factor': factor,
                'timeframe': timeframe,
                'score': score,
                'avg_ic': avg_ic,
                'signal_quality': signal_quality
            })
    
    best_combinations.sort(key=lambda x: x['score'], reverse=True)
    return best_combinations

def analyze_errors(all_results):
    """åˆ†æé”™è¯¯"""
    error_stats = {}
    
    for key, result in all_results.items():
        if not result.get('success', False):
            error = result.get('error', 'Unknown error')
            
            # ç®€åŒ–é”™è¯¯åˆ†ç±»
            if 'æ•°æ®åŠ è½½å¤±è´¥' in error:
                error_type = 'æ•°æ®åŠ è½½å¤±è´¥'
            elif 'å†…å­˜ä¸è¶³' in error:
                error_type = 'å†…å­˜ä¸è¶³'
            elif 'è¶…æ—¶' in error:
                error_type = 'è¶…æ—¶'
            elif 'é™¤é›¶' in error:
                error_type = 'é™¤é›¶é”™è¯¯'
            else:
                error_type = 'å…¶ä»–é”™è¯¯'
            
            error_stats[error_type] = error_stats.get(error_type, 0) + 1
    
    return error_stats

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸš€ 54åªæ¸¯è‚¡å…¨é‡å› å­åˆ†æ (ä¿®æ­£ç‰ˆ)")
    print("=" * 80)
    
    # æ‰§è¡Œå…¨é‡åˆ†æ
    results = run_full_analysis()
    
    print(f"\n" + "=" * 80)
    print("ğŸ‰ 54åªæ¸¯è‚¡å…¨é‡åˆ†æå®Œæˆ!")
    print("=" * 80)