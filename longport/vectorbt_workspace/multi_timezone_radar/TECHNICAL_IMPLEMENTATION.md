# ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

## ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„

```
multi_timezone_radar/
â”œâ”€â”€ ğŸ“ core/                          # æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
â”‚   â”œâ”€â”€ hk_comprehensive_analysis.py   # æ¸¯è‚¡ç»¼åˆåˆ†æå™¨
â”‚   â”œâ”€â”€ out_of_sample_tester_fixed.py  # æ ·æœ¬å¤–æµ‹è¯•å™¨(ä¿®å¤ç‰ˆ)
â”‚   â””â”€â”€ data_download/                 # æ•°æ®ä¸‹è½½æ¨¡å—
â”œâ”€â”€ ğŸ“ src/                           # æºä»£ç 
â”‚   â””â”€â”€ æ ¸å¿ƒç­›é€‰-overfitting_protection_system.py  # è¿‡æ‹Ÿåˆä¿æŠ¤ç³»ç»Ÿ
â”œâ”€â”€ ğŸ“ tools/                         # å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ regression_test.py            # å›å½’æµ‹è¯•è„šæœ¬
â”‚   â”œâ”€â”€ validate_multiple_testing.py  # å¤šé‡æ£€éªŒéªŒè¯
â”‚   â””â”€â”€ validate_timeframe_thresholds.py  # æ—¶é—´æ¡†æ¶éªŒè¯
â”œâ”€â”€ ğŸ“ results/                       # åˆ†æç»“æœ
â”‚   â””â”€â”€ hk_analysis_results_*/        # æ¸¯è‚¡åˆ†æç»“æœ
â”œâ”€â”€ ğŸ“ docs/                          # æ–‡æ¡£
â”‚   â”œâ”€â”€ PROJECT_DOCUMENTATION.md       # é¡¹ç›®æ–‡æ¡£(æœ¬æ–‡æ¡£)
â”‚   â””â”€â”€ API_REFERENCE.md              # APIå‚è€ƒæ–‡æ¡£
â””â”€â”€ ğŸ“„ requirements.txt              # ä¾èµ–åŒ…åˆ—è¡¨
```

## ğŸ¯ æ ¸å¿ƒç±»ä¸æ–¹æ³•

### 1. HKComprehensiveAnalysis ç±»

**ä¸»è¦åŠŸèƒ½**: æ¸¯è‚¡æŠ€æœ¯å› å­è®¡ç®—å’Œç»¼åˆåˆ†æ

**æ ¸å¿ƒæ–¹æ³•**:
```python
class HKComprehensiveAnalysis:
    def __init__(self, data_dir: str, logger=None)
    def load_all_stocks_data(self) -> Dict
    def calculate_technical_factors(self, data: Dict) -> Dict
    def check_survivorship_bias(self, factors: Dict, data: Dict) -> Dict
    def handle_missing_values(self, factors: Dict, data: Dict) -> Dict
    def check_factor_direction_consistency(self, factors: Dict, data: Dict) -> Dict
    def run_comprehensive_analysis(self) -> Dict
```

**å…³é”®ç‰¹æ€§**:
- æ”¯æŒ10ç§æŠ€æœ¯å› å­è®¡ç®—
- å¹¸å­˜è€…åå·®æ£€æŸ¥
- ç¼ºå¤±å€¼å¤„ç†
- å› å­æ–¹å‘ä¸€è‡´æ€§éªŒè¯
- å¤šæ—¶é—´æ¡†æ¶æ”¯æŒ

### 2. OutOfSampleTester ç±»

**ä¸»è¦åŠŸèƒ½**: æ ·æœ¬å¤–æµ‹è¯•å’Œç¨³å¥æ€§éªŒè¯

**æ ¸å¿ƒæ–¹æ³•**:
```python
class OutOfSampleTester:
    def __init__(self, data: Dict, test_ratio: float = 0.3, 
                 transaction_cost: float = 0.001, timeframe: str = 'unknown')
    def test_factors(self, factors: Dict) -> Dict
    def _split_by_time(self) -> Tuple[pd.Timestamp, pd.Timestamp]
    def _get_time_split_data(self, split_date: pd.Timestamp) -> Tuple[Dict, Dict]
    def calculate_turnover_metrics(self, factors: Dict, data: Dict) -> Dict
    def apply_multiple_testing_correction(self, oos_metrics: Dict, method: str) -> Dict
    def _get_decay_thresholds(self) -> Dict
```

**å…³é”®ç‰¹æ€§**:
- æ—¶é—´åˆ‡åˆ†é¿å…æœªæ¥å‡½æ•°
- å¤šé‡ICè®¡ç®—(Raw, Rank, Multi-period, Cost-adjusted)
- å¤šé‡æ£€éªŒæ ¡æ­£(Bonferroni, FDR)
- æ¢æ‰‹ç‡åˆ†æ
- æ—¶é—´æ¡†æ¶ç‰¹å®šé˜ˆå€¼

## ğŸ”¬ ç®—æ³•å®ç°ç»†èŠ‚

### 1. æŠ€æœ¯å› å­è®¡ç®—ç®—æ³•

#### RSI (ç›¸å¯¹å¼ºå¼±æŒ‡æ•°)
```python
def calculate_rsi(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
    """
    è®¡ç®—RSIæŒ‡æ ‡
    
    Args:
        df: åŒ…å«ä»·æ ¼æ•°æ®çš„DataFrame
        period: è®¡ç®—å‘¨æœŸï¼Œé»˜è®¤14
        
    Returns:
        RSIå€¼åºåˆ—
    """
    delta = df['close'].diff()
    gain = delta.where(delta > 0, 0).rolling(period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi
```

#### MACD (æŒ‡æ•°å¹³æ»‘å¼‚åŒç§»åŠ¨å¹³å‡çº¿)
```python
def calculate_macd(self, df: pd.DataFrame, 
                   fast: int = 12, slow: int = 26, signal: int = 9) -> pd.Series:
    """
    è®¡ç®—MACDæŒ‡æ ‡
    
    Args:
        df: åŒ…å«ä»·æ ¼æ•°æ®çš„DataFrame
        fast: å¿«çº¿å‘¨æœŸï¼Œé»˜è®¤12
        slow: æ…¢çº¿å‘¨æœŸï¼Œé»˜è®¤26
        signal: ä¿¡å·çº¿å‘¨æœŸï¼Œé»˜è®¤9
        
    Returns:
        MACDæŸ±çŠ¶å›¾åºåˆ—
    """
    exp1 = df['close'].ewm(span=fast).mean()
    exp2 = df['close'].ewm(span=slow).mean()
    macd = exp1 - exp2
    signal_line = macd.ewm(span=signal).mean()
    histogram = macd - signal_line
    return histogram
```

#### å¸ƒæ—å¸¦ä½ç½®æŒ‡æ ‡
```python
def calculate_bollinger_position(self, df: pd.DataFrame, 
                                 period: int = 20, std_dev: int = 2) -> pd.Series:
    """
    è®¡ç®—å¸ƒæ—å¸¦ä½ç½®æŒ‡æ ‡
    
    Args:
        df: åŒ…å«ä»·æ ¼æ•°æ®çš„DataFrame
        period: ç§»åŠ¨å¹³å‡å‘¨æœŸï¼Œé»˜è®¤20
        std_dev: æ ‡å‡†å·®å€æ•°ï¼Œé»˜è®¤2
        
    Returns:
        å¸ƒæ—å¸¦ä½ç½®æŒ‡æ ‡(-1åˆ°1ä¹‹é—´)
    """
    ma = df['close'].rolling(period).mean()
    std = df['close'].rolling(period).std()
    upper_band = ma + (std * std_dev)
    lower_band = ma - (std * std_dev)
    
    # è®¡ç®—ä»·æ ¼åœ¨å¸ƒæ—å¸¦ä¸­çš„ä½ç½®
    position = (df['close'] - lower_band) / (upper_band - lower_band)
    return (position - 0.5) * 2  # æ ‡å‡†åŒ–åˆ°[-1, 1]
```

### 2. ç»Ÿè®¡æ£€éªŒç®—æ³•

#### ä¿¡æ¯ç³»æ•°è®¡ç®—
```python
def calculate_information_coefficient(self, factor_values: pd.Series, 
                                    returns: pd.Series) -> Dict[str, float]:
    """
    è®¡ç®—å¤šç§ä¿¡æ¯ç³»æ•°
    
    Args:
        factor_values: å› å­å€¼åºåˆ—
        returns: æ”¶ç›Šç‡åºåˆ—
        
    Returns:
        åŒ…å«å¤šç§ICæŒ‡æ ‡çš„å­—å…¸
    """
    # æ•°æ®å¯¹é½å’Œæ¸…æ´—
    common_index = factor_values.index.intersection(returns.index)
    aligned_factor = factor_values.loc[common_index]
    aligned_returns = returns.loc[common_index]
    
    # ç§»é™¤å¼‚å¸¸å€¼
    valid_mask = ~(np.isnan(aligned_factor) | np.isnan(aligned_returns))
    clean_factor = aligned_factor[valid_mask]
    clean_returns = aligned_returns[valid_mask]
    
    # åŸå§‹IC
    raw_ic = clean_factor.corr(clean_returns)
    
    # Rank IC
    rank_factor = clean_factor.rank(pct=True)
    rank_returns = clean_returns.rank(pct=True)
    rank_ic = rank_factor.corr(rank_returns)
    
    # å¤šæœŸIC (5æœŸ)
    multi_period_returns = clean_returns.rolling(5).sum().shift(-5)
    multi_valid_mask = ~(np.isnan(clean_factor) | np.isnan(multi_period_returns))
    if multi_valid_mask.sum() > 10:
        clean_multi_factor = clean_factor[multi_valid_mask]
        clean_multi_returns = multi_period_returns[multi_valid_mask]
        multi_ic = clean_multi_factor.corr(clean_multi_returns)
    else:
        multi_ic = np.nan
    
    # æˆæœ¬è°ƒæ•´IC
    transaction_cost = 0.001
    cost_adjusted_returns = clean_returns - np.sign(clean_factor) * transaction_cost
    cost_adj_ic = clean_factor.corr(cost_adjusted_returns)
    
    return {
        'raw_ic': raw_ic if np.isfinite(raw_ic) else np.nan,
        'rank_ic': rank_ic if np.isfinite(rank_ic) else np.nan,
        'multi_period_ic': multi_ic if np.isfinite(multi_ic) else np.nan,
        'cost_adjusted_ic': cost_adj_ic if np.isfinite(cost_adj_ic) else np.nan
    }
```

#### å¤šé‡æ£€éªŒæ ¡æ­£
```python
def apply_multiple_testing_correction(self, oos_metrics: Dict, method: str = 'fdr') -> Dict:
    """
    åº”ç”¨å¤šé‡æ£€éªŒæ ¡æ­£
    
    Args:
        oos_metrics: æ ·æœ¬å¤–æŒ‡æ ‡å­—å…¸
        method: æ ¡æ­£æ–¹æ³• ('bonferroni' æˆ– 'fdr')
        
    Returns:
        æ ¡æ­£åçš„ç»“æœå­—å…¸
    """
    if not oos_metrics:
        return {}
    
    # æå–på€¼
    factors = list(oos_metrics.keys())
    ic_values = [oos_metrics[factor].get('ic', 0) for factor in factors]
    n_observations = [oos_metrics[factor].get('n_observations', 0) for factor in factors]
    
    # è®¡ç®—på€¼ (åŒå°¾æ£€éªŒ)
    p_values = []
    for ic, n in zip(ic_values, n_observations):
        if n > 3:
            # Fisherå˜æ¢
            z_score = 0.5 * np.log((1 + ic) / (1 - ic)) if abs(ic) < 1 else 0
            se = 1 / np.sqrt(n - 3)
            p_value = 2 * (1 - stats.norm.cdf(abs(z_score / se)))
        else:
            p_value = 1.0
        p_values.append(p_value)
    
    # åº”ç”¨æ ¡æ­£
    if method == 'bonferroni':
        corrected_p = [min(p * len(factors), 1.0) for p in p_values]
        alpha = 0.05 / len(factors)
    elif method == 'fdr':
        # Benjamini-Hochbergè¿‡ç¨‹
        sorted_p = sorted(p_values)
        m = len(p_values)
        corrected_p = []
        for i, p in enumerate(p_values):
            rank = sorted_p.index(p) + 1
            critical_value = rank / m * 0.05
            if p <= critical_value:
                corrected_p.append(min(p * m / rank, 1.0))
            else:
                corrected_p.append(p)
        alpha = 0.05
    
    # æ„å»ºç»“æœ
    results = {}
    for i, factor in enumerate(factors):
        results[factor] = {
            'original_p_value': p_values[i],
            'corrected_p_value': corrected_p[i],
            'significant_after_correction': corrected_p[i] <= alpha,
            'alpha': alpha
        }
    
    return results
```

### 3. æ¢æ‰‹ç‡åˆ†æç®—æ³•

```python
def calculate_turnover_metrics(self, factors: Dict, data: Dict) -> Dict:
    """
    è®¡ç®—æ¢æ‰‹ç‡ç›¸å…³æŒ‡æ ‡
    
    Args:
        factors: å› å­å­—å…¸
        data: æ•°æ®å­—å…¸
        
    Returns:
        æ¢æ‰‹ç‡æŒ‡æ ‡å­—å…¸
    """
    turnover_metrics = {}
    
    for factor_name, factor_series in factors.items():
        try:
            # å¯¹æ¯ä¸ªè‚¡ç¥¨è®¡ç®—æ¢æ‰‹ç‡
            stock_turnover_rates = []
            autocorrelations = []
            half_lives = []
            
            for symbol in data.keys():
                # æå–è¯¥è‚¡ç¥¨çš„å› å­æ•°æ®
                if hasattr(factor_series.index, 'get_level_values'):
                    symbol_mask = factor_series.index.get_level_values(0) == symbol
                    if not symbol_mask.any():
                        continue
                    
                    symbol_factor = factor_series[symbol_mask]
                    time_indices = symbol_factor.index.get_level_values(1)
                    symbol_factor.index = time_indices
                else:
                    continue
                
                # è®¡ç®—å› å­å˜åŒ–
                factor_changes = symbol_factor.diff().abs()
                turnover_rate = factor_changes.mean()
                
                # è®¡ç®—è‡ªç›¸å…³
                autocorr = symbol_factor.autocorr()
                
                # è®¡ç®—åŠè¡°æœŸ
                if abs(autocorr) > 0.01:
                    half_life = abs(np.log(0.5) / np.log(abs(autocorr)))
                else:
                    half_life = 1.0
                
                if np.isfinite(turnover_rate) and np.isfinite(autocorr):
                    stock_turnover_rates.append(turnover_rate)
                    autocorrelations.append(autocorr)
                    half_lives.append(half_life)
            
            if stock_turnover_rates:
                turnover_metrics[factor_name] = {
                    'turnover_rate': np.mean(stock_turnover_rates),
                    'autocorrelation': np.mean(autocorrelations),
                    'half_life': np.mean(half_lives),
                    'n_stocks': len(stock_turnover_rates)
                }
            
        except Exception as e:
            self.logger.warning(f"è®¡ç®—{factor_name}æ¢æ‰‹ç‡å¤±è´¥: {str(e)}")
            continue
    
    return turnover_metrics
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. å¹¶è¡Œå¤„ç†å®ç°

```python
def _calculate_metrics_parallel(self, factors: Dict, data_subset: Dict) -> Dict:
    """
    å¹¶è¡Œè®¡ç®—å› å­æŒ‡æ ‡
    
    Args:
        factors: å› å­å­—å…¸
        data_subset: æ•°æ®å­é›†
        
    Returns:
        å› å­æŒ‡æ ‡å­—å…¸
    """
    metrics = {}
    
    # å†…å­˜ç®¡ç†
    self._manage_memory_usage()
    
    # å¹¶è¡Œè®¡ç®—
    with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
        futures = []
        
        for factor_name, factor_series in factors.items():
            # æäº¤ä»»åŠ¡
            future = executor.submit(
                self._calculate_single_factor_metrics,
                factor_name, factor_series, data_subset
            )
            futures.append((factor_name, future))
        
        # æ”¶é›†ç»“æœ
        for factor_name, future in futures:
            try:
                result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                if result:
                    metrics[factor_name] = result
            except Exception as e:
                self.logger.error(f"å› å­{factor_name}å¹¶è¡Œè®¡ç®—å¤±è´¥: {str(e)}")
                continue
    
    return metrics
```

### 2. å†…å­˜ç®¡ç†ç­–ç•¥

```python
def _manage_memory_usage(self):
    """å†…å­˜ç®¡ç†"""
    try:
        current_memory = psutil.Process().memory_info().rss / (1024**3)
        
        if current_memory > self.memory_limit_gb * 0.8:
            self.logger.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {current_memory:.1f}GB > {self.memory_limit_gb * 0.8:.1f}GB")
            gc.collect()
            
            # æ£€æŸ¥å†…å­˜æ˜¯å¦ä»ç„¶è¿‡é«˜
            current_memory = psutil.Process().memory_info().rss / (1024**3)
            if current_memory > self.memory_limit_gb * 0.9:
                self.logger.error(f"å†…å­˜ä¸¥é‡ä¸è¶³: {current_memory:.1f}GB")
                raise MemoryError("å†…å­˜ä¸è¶³ï¼Œè¯·å‡å°‘æ•°æ®é‡æˆ–å¢åŠ å†…å­˜é™åˆ¶")
                
    except Exception as e:
        self.logger.error(f"å†…å­˜ç®¡ç†å¤±è´¥: {str(e)}")
```

### 3. æ•°æ®å¤„ç†ä¼˜åŒ–

```python
def _process_stock_chunk(self, stock_chunk: List, factor_series: pd.Series, 
                        data_subset: Dict) -> List[Dict]:
    """
    å¤„ç†è‚¡ç¥¨åˆ†å—
    
    Args:
        stock_chunk: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        factor_series: å› å­æ•°æ®
        data_subset: æ•°æ®å­é›†
        
    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨
    """
    results = []
    
    for symbol in stock_chunk:
        try:
            # æ£€æŸ¥æ•°æ®å­˜åœ¨æ€§
            if symbol not in data_subset:
                continue
                
            df = data_subset[symbol]
            if 'close' not in df.columns:
                continue
            
            # è®¡ç®—æ”¶ç›Šç‡
            returns = df['close'].pct_change().shift(-1)
            multi_period_returns = df['close'].pct_change(5).shift(-5)
            
            # æå–å› å­æ•°æ®
            symbol_factor = self._extract_symbol_factor(factor_series, symbol)
            if symbol_factor is None:
                continue
            
            # æ•°æ®å¯¹é½
            common_index = symbol_factor.index.intersection(returns.index)
            if len(common_index) < 10:
                continue
            
            # è®¡ç®—ICæŒ‡æ ‡
            ic_result = self._calculate_single_stock_ic(
                symbol_factor.loc[common_index],
                returns.loc[common_index],
                multi_period_returns.loc[common_index]
            )
            
            if ic_result:
                results.append(ic_result)
                
        except Exception as e:
            self.logger.warning(f"å¤„ç†è‚¡ç¥¨{symbol}å¤±è´¥: {str(e)}")
            continue
    
    return results
```

## ğŸ” é”™è¯¯å¤„ç†ä¸æ—¥å¿—

### 1. å¼‚å¸¸å¤„ç†ç­–ç•¥

```python
def safe_calculate_factor(self, calculation_func, *args, **kwargs):
    """
    å®‰å…¨çš„å› å­è®¡ç®—åŒ…è£…å™¨
    
    Args:
        calculation_func: è®¡ç®—å‡½æ•°
        *args: ä½ç½®å‚æ•°
        **kwargs: å…³é”®å­—å‚æ•°
        
    Returns:
        è®¡ç®—ç»“æœæˆ–None
    """
    try:
        result = calculation_func(*args, **kwargs)
        if result is not None and len(result) > 0:
            return result
        else:
            self.logger.warning(f"å› å­è®¡ç®—è¿”å›ç©ºç»“æœ")
            return None
    except Exception as e:
        self.logger.error(f"å› å­è®¡ç®—å¤±è´¥: {str(e)}")
        return None
```

### 2. æ—¥å¿—è®°å½•ç³»ç»Ÿ

```python
def setup_logging(self, log_level: str = 'INFO'):
    """
    è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        log_level: æ—¥å¿—çº§åˆ«
    """
    # åˆ›å»ºæ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è®¾ç½®æ ¹æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler('factor_analysis.log')
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)
    
    self.logger = logging.getLogger(__name__)
```

## ğŸ“Š é…ç½®å‚æ•°è¯´æ˜

### 1. ç³»ç»Ÿé…ç½®

```python
# config.py
SYSTEM_CONFIG = {
    # æ•°æ®è®¾ç½®
    'data_dir': '/path/to/data',
    'timeframes': ['1m', '5m', '15m', '30m', '1h', '1d'],
    
    # æµ‹è¯•è®¾ç½®
    'test_ratio': 0.3,  # æµ‹è¯•é›†æ¯”ä¾‹
    'transaction_cost': 0.001,  # äº¤æ˜“æˆæœ¬
    'min_samples': 100,  # æœ€å°æ ·æœ¬æ•°
    
    # æ€§èƒ½è®¾ç½®
    'n_workers': 4,  # å¹¶è¡Œè¿›ç¨‹æ•°
    'memory_limit_gb': 4.0,  # å†…å­˜é™åˆ¶
    'chunk_size': 100,  # æ•°æ®åˆ†å—å¤§å°
    
    # ç»Ÿè®¡è®¾ç½®
    'significance_level': 0.05,  # æ˜¾è‘—æ€§æ°´å¹³
    'decay_thresholds': {  # è¡°å‡é˜ˆå€¼
        '1m': {'mild': 0.5, 'moderate': 1.0, 'severe': 1.5},
        '1h': {'mild': 0.3, 'moderate': 0.6, 'severe': 1.0},
        '1d': {'mild': 0.2, 'moderate': 0.4, 'severe': 0.6}
    }
}
```

### 2. å› å­é…ç½®

```python
FACTOR_CONFIG = {
    # å› å­æ–¹å‘è®¾ç½®
    'factor_directions': {
        'RSI': 'negative',
        'MACD': 'positive',
        'Volume_Ratio': 'positive',
        'Momentum_ROC': 'positive',
        'Bollinger_Position': 'mean_reverting',
        'Z_Score': 'mean_reverting',
        'ADX': 'positive',
        'Stochastic': 'mean_reverting',
        'Williams_R': 'negative',
        'CCI': 'mean_reverting'
    },
    
    # å› å­å‚æ•°è®¾ç½®
    'factor_parameters': {
        'RSI': {'period': 14},
        'MACD': {'fast': 12, 'slow': 26, 'signal': 9},
        'Bollinger': {'period': 20, 'std_dev': 2},
        'ADX': {'period': 14},
        'Stochastic': {'k_period': 14, 'd_period': 3}
    }
}
```

è¿™ä¸ªæŠ€æœ¯å®ç°ç»†èŠ‚æ–‡æ¡£æä¾›äº†ç³»ç»Ÿçš„æ·±å…¥æŠ€æœ¯è¯´æ˜ï¼ŒåŒ…æ‹¬æ ¸å¿ƒç®—æ³•ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ã€é”™è¯¯å¤„ç†æœºåˆ¶ç­‰å…³é”®å®ç°ç»†èŠ‚ã€‚