#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§å› å­æ±  - 30+æŒ‡æ ‡çš„äº§ä¸šçº§é‡åŒ–ç­–ç•¥æ¡†æ¶
ä»6ä¸ªæŒ‡æ ‡æ‰©å±•åˆ°30+ï¼Œè¦†ç›–è¶‹åŠ¿ã€åŠ¨é‡ã€æ³¢åŠ¨ã€æˆäº¤é‡ã€å¾®è§‚ç»“æ„ç­‰ç»´åº¦
"""

import numpy as np
import pandas as pd
import vectorbt as vbt
import talib as ta
from typing import Dict, List, Optional, Tuple, Union
import warnings
warnings.filterwarnings('ignore')

# === ç»Ÿä¸€é˜²æœªæ¥å‡½æ•°ï¼šrolling æ°¸è¿œåªå–"å·²æ”¶ç›˜"K çº¿ ===
def roll_closed(df, col, win, func='mean'):
    """
    win : int æˆ– pd.Timedelta
    func: 'mean'|'std'|'max'|'min'|'sum'|...
    è¿”å›ï¼šSeriesï¼Œçª—å£å†…ä»…å«å·²æ”¶ç›˜æ•°æ®ï¼Œå¤©ç„¶æ»å 1 æ ¹ K çº¿
    """
    return getattr(df[col].shift(1), 'rolling')(win, min_periods=max(2, win//2)).agg(func)

def roll_closed_rank(df, col, win, min_periods=None):
    """
    ä¸“é—¨ç”¨äºrankçš„rollingå‡½æ•°ï¼Œç¡®ä¿åªä½¿ç”¨å·²æ”¶ç›˜æ•°æ®
    """
    if min_periods is None:
        min_periods = max(2, win//2)
    return df[col].shift(1).rolling(win, min_periods=min_periods).rank(pct=True)

def safe_ema(series, span):
    """å®‰å…¨çš„EMAè®¡ç®—ï¼Œé¿å…æœªæ¥å‡½æ•°"""
    return series.shift(1).ewm(span=span).mean()

def get_safe_price(df, price_col='close'):
    """è·å–å®‰å…¨çš„æ»åä»·æ ¼æ•°æ®"""
    return df[price_col].shift(1)

def safe_talib_single_price(func_name, price_series, timeperiod):
    """å®‰å…¨çš„å•ä»·æ ¼TA-LIBå‡½æ•°åŒ…è£…å™¨"""
    shifted_series = price_series.shift(1).values
    if isinstance(shifted_series, np.ndarray):
        return getattr(ta, func_name)(shifted_series, timeperiod=timeperiod)
    else:
        return getattr(ta, func_name)(shifted_series.values, timeperiod=timeperiod)

def safe_talib_ohlcv(func_name, high_series, low_series, close_series, volume_series=None, timeperiod=None):
    """å®‰å…¨çš„OHLCV TA-LIBå‡½æ•°åŒ…è£…å™¨"""
    high_shifted = high_series.shift(1).values
    low_shifted = low_series.shift(1).values
    close_shifted = close_series.shift(1).values
    
    if volume_series is not None:
        volume_shifted = volume_series.shift(1).values
        if timeperiod is not None:
            return getattr(ta, func_name)(high_shifted, low_shifted, close_shifted, volume_shifted, timeperiod=timeperiod)
        else:
            return getattr(ta, func_name)(high_shifted, low_shifted, close_shifted, volume_shifted)
    else:
        if timeperiod is not None:
            return getattr(ta, func_name)(high_shifted, low_shifted, close_shifted, timeperiod=timeperiod)
        else:
            return getattr(ta, func_name)(high_shifted, low_shifted, close_shifted)

def safe_talib_hl(func_name, high_series, low_series, timeperiod):
    """å®‰å…¨çš„HL TA-LIBå‡½æ•°åŒ…è£…å™¨"""
    high_shifted = high_series.shift(1).values
    low_shifted = low_series.shift(1).values
    return getattr(ta, func_name)(high_shifted, low_shifted, timeperiod=timeperiod)

def safe_returns(price_series, periods=1):
    """å®‰å…¨çš„æ”¶ç›Šç‡è®¡ç®—"""
    return price_series.shift(periods).pct_change()

class AdvancedFactorPool:
    """é«˜çº§å› å­æ±  - äº§ä¸šçº§æŒ‡æ ‡è®¡ç®—"""
    
    def __init__(self):
        """åˆå§‹åŒ–é«˜çº§å› å­æ± """
        self.factor_categories = {
            'trend': ['dema', 'tema', 'kama', 'trix', 'aroon_up', 'aroon_down', 'adx'],
            'momentum': ['rsi_2', 'rsi_100', 'stoch_rsi', 'cci', 'roc', 'mfi', 'willr'],
            'volatility': ['atrp', 'keltner_position', 'bb_squeeze', 'volatility_ratio'],
            'volume': ['vwap_dev', 'volume_rsi', 'ad_line', 'cmf', 'volume_ma_dev'],
            'microstructure': ['hl_spread', 'volume_intensity', 'price_efficiency'],
            'enhanced': ['macd_enhanced', 'rsi_enhanced', 'atr_enhanced'],
            'stochastic': ['stoch_k', 'stoch_d', 'stoch_divergence', 'stoch_signal'],
            'ichimoku': ['tenkan_sen', 'kijun_sen', 'senkou_span_a', 'senkou_span_b', 'chikou_span', 'cloud_thickness', 'ichimoku_signal'],
            'parabolic': ['parabolic_sar', 'sar_trend', 'sar_distance', 'sar_signal'],
            'cointegration': ['cointegration_score', 'mean_reversion_speed', 'half_life', 'cointegration_signal'],
            'pair_trading': ['price_volume_ratio', 'price_momentum_ratio', 'pair_correlation', 'pair_trading_signal'],
            'anomaly_detection': ['price_anomaly', 'volume_anomaly', 'volatility_anomaly', 'returns_anomaly', 'composite_anomaly', 'anomaly_signal']
        }
        
    def calculate_trend_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—è¶‹åŠ¿ç±»å› å­"""
        close, high, low, volume = df['close'], df['high'], df['low'], df['volume']
        
        try:
            # DEMA (åŒæŒ‡æ•°ç§»åŠ¨å¹³å‡)
            df['dema_14'] = safe_talib_single_price('DEMA', close, 14)
            
            # TEMA (ä¸‰æŒ‡æ•°ç§»åŠ¨å¹³å‡)
            df['tema_14'] = safe_talib_single_price('TEMA', close, 14)
            
            # KAMA (è€ƒå¤«æ›¼è‡ªé€‚åº”ç§»åŠ¨å¹³å‡)
            df['kama_14'] = safe_talib_single_price('KAMA', close, 14)
            
            # TRIX (ä¸‰é‡æŒ‡æ•°å¹³æ»‘éœ‡è¡å™¨)
            df['trix_14'] = safe_talib_single_price('TRIX', close, 14)
            
            # AroonæŒ‡æ ‡
            aroon_up, aroon_down = safe_talib_hl('AROON', high, low, 14)
            df['aroon_up'] = aroon_up
            df['aroon_down'] = aroon_down
            df['aroon_oscillator'] = aroon_up - aroon_down
            
            # ADX (å¹³å‡è¶‹å‘æŒ‡æ•°)
            df['adx_14'] = safe_talib_ohlcv('ADX', high, low, close, None, 14)
            
            # è¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡
            df['trend_strength'] = np.abs(roll_closed(df, 'close', 20, 'mean') - roll_closed(df, 'close', 5, 'mean')) / get_safe_price(df, 'close')
            
        except Exception as e:
            print(f"è¶‹åŠ¿å› å­è®¡ç®—è­¦å‘Š: {e}")
            
        return df
    
    def calculate_momentum_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—åŠ¨é‡ç±»å› å­"""
        close, high, low, volume = df['close'], df['high'], df['low'], df['volume']
        
        try:
            # å¤šå‘¨æœŸRSI
            df['rsi_2'] = safe_talib_single_price('RSI', close, 2)
            df['rsi_14'] = safe_talib_single_price('RSI', close, 14)
            df['rsi_100'] = safe_talib_single_price('RSI', close, 100)
            
            # Stochastic RSI
            fastk, fastd = ta.STOCHRSI(close.shift(1).values, timeperiod=14, fastk_period=5, fastd_period=3)
            df['stoch_rsi'] = fastk
            
            # CCI (é¡ºåŠ¿æŒ‡æ ‡)
            df['cci_14'] = safe_talib_ohlcv('CCI', high, low, close, None, 14)
            
            # ROC (å˜åŠ¨ç‡)
            df['roc_12'] = safe_talib_single_price('ROC', close, 12)
            df['roc_5'] = safe_talib_single_price('ROC', close, 5)
            
            # MFI (èµ„é‡‘æµé‡æŒ‡æ ‡)
            df['mfi_14'] = safe_talib_ohlcv('MFI', high, low, close, volume, 14)
            
            # Williams %R
            df['willr_14'] = safe_talib_ohlcv('WILLR', high, low, close, None, 14)
            
            # åŠ¨é‡åˆ†å±‚
            df['momentum_regime'] = pd.cut(df['rsi_14'], bins=[0, 30, 70, 100], labels=[-1, 0, 1])
            
        except Exception as e:
            print(f"åŠ¨é‡å› å­è®¡ç®—è­¦å‘Š: {e}")
            
        return df
    
    def calculate_volatility_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ³¢åŠ¨ç±»å› å­"""
        close, high, low, volume = df['close'], df['high'], df['low'], df['volume']
        
        try:
            # ATRP (ç›¸å¯¹ATR)
            atr = safe_talib_ohlcv('ATR', high, low, close, None, 14)
            df['atrp'] = atr / get_safe_price(df, 'close')  # è§£å†³è‚¡ä»·æ°´å¹³æ¼‚ç§»é—®é¢˜
            
            # Keltneré€šé“ä½ç½®
            keltner_ma = roll_closed(df, 'close', 20, 'mean')
            atr_col = 'atr_temp'
            df[atr_col] = atr
            keltner_atr = roll_closed(df, atr_col, 20, 'mean')
            keltner_upper = keltner_ma + 2 * keltner_atr
            keltner_lower = keltner_ma - 2 * keltner_atr
            df['keltner_position'] = (get_safe_price(df, 'close') - keltner_lower) / (keltner_upper - keltner_lower)
            
            # å¸ƒæ—å¸¦æ”¶ç¼©
            bb_upper, bb_middle, bb_lower = ta.BBANDS(close.shift(1).values, timeperiod=20, nbdevup=2, nbdevdn=2)
            df['bb_squeeze'] = (bb_upper - bb_lower) / bb_middle
            
            # æ³¢åŠ¨ç‡æ¯”å€¼
            short_vol = roll_closed(df, 'close', 5, 'std')
            long_vol = roll_closed(df, 'close', 20, 'std')
            df['volatility_ratio'] = short_vol / long_vol
            
            # Parkinsonæ³¢åŠ¨ç‡ä¼°è®¡å™¨
            df['parkinson_vol'] = np.sqrt(0.361 * np.log(high.shift(1) / low.shift(1)) ** 2)
            
            # éšå«æ³¢åŠ¨ç‡ä»£ç†
            df['iv_proxy'] = (high.shift(1) - low.shift(1)) / close.shift(1)
            
        except Exception as e:
            print(f"æ³¢åŠ¨ç‡å› å­è®¡ç®—è­¦å‘Š: {e}")
            
        return df
    
    def calculate_volume_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æˆäº¤é‡ç±»å› å­"""
        close, high, low, volume = df['close'], df['high'], df['low'], df['volume']
        
        try:
            # VWAPåç¦»åº¦
            close_volume = close * volume
            close_volume_sum = roll_closed(df, close_volume.name if hasattr(close_volume, 'name') else 'close_volume_temp', 20, 'sum')
            volume_sum = roll_closed(df, 'volume', 20, 'sum')
            vwap = close_volume_sum / volume_sum
            df['vwap_deviation'] = (get_safe_price(df, 'close') - vwap) / get_safe_price(df, 'close')
            
            # Volume RSI
            volume_gains = volume.shift(1).diff().clip(lower=0)
            volume_losses = (-volume.shift(1).diff()).clip(lower=0)
            df['vg_temp'] = volume_gains
            df['vl_temp'] = volume_losses
            rs = roll_closed(df, 'vg_temp', 14, 'mean') / roll_closed(df, 'vl_temp', 14, 'mean')
            df['volume_rsi'] = 100 - (100 / (1 + rs))
            
            # A/D Line (ç´¯ç§¯/æ´¾å‘çº¿)
            money_flow_multiplier = ((close.shift(1) - low.shift(1)) - (high.shift(1) - close.shift(1))) / (high.shift(1) - low.shift(1) + 1e-8)
            money_flow_volume = money_flow_multiplier * volume.shift(1)
            df['mfv_temp'] = money_flow_volume
            df['ad_line'] = roll_closed(df, 'mfv_temp', 252, 'sum')
            
            # Chaikin Money Flow
            mfv_sum = roll_closed(df, 'mfv_temp', 20, 'sum')
            vol_sum = roll_closed(df, 'volume', 20, 'sum')
            df['cmf'] = mfv_sum / vol_sum
            
            # Volume MAåç¦»åº¦
            volume_ma = roll_closed(df, 'volume', 20, 'mean')
            df['volume_ma_deviation'] = (volume - volume_ma) / volume_ma
            
            # Volume-Price Trend
            df['returns_temp'] = close.pct_change()
            df['vpt_temp'] = volume.shift(1) * df['returns_temp']
            df['vpt'] = roll_closed(df, 'vpt_temp', 252, 'sum')
            
        except Exception as e:
            print(f"æˆäº¤é‡å› å­è®¡ç®—è­¦å‘Š: {e}")
            
        return df
    
    def calculate_microstructure_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—å¾®è§‚ç»“æ„å› å­"""
        close, high, low, volume = df['close'], df['high'], df['low'], df['volume']
        
        try:
            # ä¹°å–ä»·å·®ä»£ç†
            df['hl_spread'] = (high.shift(1) - low.shift(1)) / close.shift(1)
            
            # æˆäº¤é‡å¼ºåº¦
            df['volume_intensity'] = volume / roll_closed(df, 'volume', 20, 'mean')
            
            # ä»·æ ¼æ•ˆç‡ (Random Walk Index)
            price_range = high.shift(1) - low.shift(1)
            df['high_close_diff'] = (high.shift(1) - close.shift(1)).abs()
            df['low_close_diff'] = (low.shift(1) - close.shift(1)).abs()
            true_range = np.maximum(price_range, np.maximum(df['high_close_diff'], df['low_close_diff']))
            df['true_range_temp'] = true_range
            df['price_efficiency'] = price_range / roll_closed(df, 'true_range_temp', 14, 'sum')
            
            # æµåŠ¨æ€§æŒ‡æ ‡
            df['liquidity_proxy'] = volume.shift(1) * close.shift(1) / (high.shift(1) - low.shift(1) + 1e-8)
            
            # ä»·æ ¼è·³è·ƒæ£€æµ‹
            returns = close.pct_change()
            df['returns_temp'] = returns
            rolling_std = roll_closed(df, 'returns_temp', 20, 'std')
            df['price_jump'] = np.abs(df['returns_temp'].shift(1)) / rolling_std
            
        except Exception as e:
            print(f"å¾®è§‚ç»“æ„å› å­è®¡ç®—è­¦å‘Š: {e}")
            
        return df
    
    def calculate_enhanced_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—å¢å¼ºå‹å› å­ - å¤šç»´åº¦ä¿¡å·èåˆ"""
        close, high, low, volume = df['close'], df['high'], df['low'], df['volume']
        
        try:
            # å¢å¼ºå‹MACD
            exp1 = safe_ema(close, 12)
            exp2 = safe_ema(close, 26)
            macd = exp1 - exp2
            signal = safe_ema(macd, 9)
            histogram = macd - signal
            
            # MACDèƒ½é‡
            df['macd_enhanced'] = histogram * roll_closed(df, 'volume', 20, 'mean')
            
            # å¢å¼ºå‹RSI (ç»“åˆæˆäº¤é‡)
            rsi = safe_talib_single_price('RSI', close, 14)
            volume_weighted_rsi = rsi * (volume.shift(1) / roll_closed(df, 'volume', 14, 'mean'))
            df['rsi_enhanced'] = volume_weighted_rsi
            
            # å¢å¼ºå‹ATR (è€ƒè™‘æˆäº¤é‡)
            atr = safe_talib_ohlcv('ATR', high, low, close, None, 14)
            df['atr_enhanced'] = atr * np.sqrt(volume.shift(1) / roll_closed(df, 'volume', 14, 'mean'))
            
            # å¤šå› å­å¾—åˆ†
            factors_to_rank = ['rsi_14', 'macd_enhanced', 'atrp', 'vwap_deviation']
            rank_wnd = min(60, len(df))  # æœ€å¤š60æ ¹Kçº¿æ’åçª—å£
            for factor in factors_to_rank:
                if factor in df.columns:
                    # ä½¿ç”¨é˜²æœªæ¥å‡½æ•°çš„rankå‡½æ•°
                    df[f'{factor}_rank'] = roll_closed_rank(df, factor, rank_wnd, min_periods=20)
            
            # å¤åˆåŠ¨é‡å¾—åˆ†
            momentum_factors = ['roc_12', 'rsi_14', 'stoch_rsi']
            valid_momentum = [f for f in momentum_factors if f in df.columns]
            if valid_momentum:
                df['momentum_composite'] = df[valid_momentum].mean(axis=1)
                
        except Exception as e:
            print(f"å¢å¼ºå‹å› å­è®¡ç®—è­¦å‘Š: {e}")
            
        return df
    
    def calculate_cross_cycle_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """å‘é‡åŒ–å®ç° 10 ä¸ªæ—  L2 é«˜ä»·å€¼å› å­ + å‘¨æœŸé—¸é—¨"""
        print(f"ğŸ¯ å¼€å§‹è®¡ç®—è·¨å‘¨æœŸå‘é‡åŒ–å› å­...")
        
        close, high, low, open_price, volume = df['close'], df['high'], df['low'], df['open'], df['volume']
        
        try:
            # â‘  smart_money_flow - å°¾ç›˜-å¼€ç›˜ VWAP ä»£ç†èªæ˜é’±
            # å‡è®¾æœ€å25%ä¸ºå°¾ç›˜ï¼Œå‰25%ä¸ºå¼€ç›˜
            n = len(df)
            if n >= 4:
                # å‘é‡åŒ–è®¡ç®—æ—¥å†…VWAP
                typical_price = (high.shift(1) + low.shift(1) + close.shift(1)) / 3
                df['tp_temp'] = typical_price * volume.shift(1)
                tp_sum = roll_closed(df, 'tp_temp', min(20, n//4), 'sum')
                vol_sum = roll_closed(df, 'volume', min(20, n//4), 'sum')
                vwap = tp_sum / vol_sum
                # å°¾ç›˜vså¼€ç›˜ä»·å·® (ç®€åŒ–ä¸ºæ”¶ç›˜vså¼€ç›˜)
                df['smart_money_flow'] = (close.shift(1) - open_price.shift(1)) / vwap
            else:
                df['smart_money_flow'] = 0
            
            # â‘¡ zscore_momentum_20 - 20æ ¹Kçº¿æ”¶ç›Šæ»šåŠ¨æ ‡å‡†åŒ–
            returns = close.pct_change()
            df['returns_temp'] = returns
            window = min(20, max(5, n//3))
            rolling_mean = roll_closed(df, 'returns_temp', window, 'mean')
            rolling_std = roll_closed(df, 'returns_temp', window, 'std')
            df['zscore_momentum_20'] = (df['returns_temp'].shift(1) - rolling_mean) / (rolling_std + 1e-8)
            
            # â‘¢ order_flow_imbalance - ä¸Šæ¶¨vsä¸‹è·ŒKçº¿æˆäº¤é‡å·®
            df['up_vol_temp'] = np.where(close.shift(1) > close.shift(2), volume.shift(1), 0)
            df['down_vol_temp'] = np.where(close.shift(1) < close.shift(2), volume.shift(1), 0)
            window_flow = min(14, max(3, n//4))
            up_vol_ma = roll_closed(df, 'up_vol_temp', window_flow, 'sum')
            down_vol_ma = roll_closed(df, 'down_vol_temp', window_flow, 'sum')
            df['order_flow_imbalance'] = (up_vol_ma - down_vol_ma) / (up_vol_ma + down_vol_ma + 1e-8)
            
            # â‘£ vw_macd - æˆäº¤é‡åŠ æƒMACD
            # å…ˆè®¡ç®—ä¼ ç»ŸMACD
            exp1 = safe_ema(close, 12)
            exp2 = safe_ema(close, 26)
            macd_line = exp1 - exp2
            signal_line = safe_ema(macd_line, 9)
            histogram = macd_line - signal_line
            # æˆäº¤é‡åŠ æƒ
            volume_weight = volume / roll_closed(df, 'volume', min(20, n//2), 'mean')
            df['vw_macd'] = histogram * volume_weight
            
            # â‘¤ drawdown_volatility - å›æ’¤æ³¢åŠ¨ç‡(ä¸‹è¡Œé£é™©æ•æ„Ÿ)
            window_dd_max = min(252, len(close))  # æœ€é•¿1å¹´çª—å£
            cummax = roll_closed(df, 'close', window_dd_max, 'max')
            drawdown = (close - cummax) / cummax
            window_dd = min(30, max(10, n//2))
            df['drawdown_temp'] = drawdown
            df['drawdown_volatility'] = roll_closed(df, 'drawdown_temp', window_dd, 'std')
            
            # â‘¥ skewness_60 - 60æ ¹Kçº¿æ”¶ç›Šååº¦(æç«¯é¢„è­¦)
            window_skew = min(60, max(20, n//2))
            if n >= 20:
                df['skewness_60'] = roll_closed(df, 'returns_temp', window_skew, 'skew')
            else:
                df['skewness_60'] = 0
                
            # â‘¦ mean_reversion_score - å¸ƒæ—å¸¦Z-Score(ç»Ÿè®¡å¥—åˆ©)
            window_bb = min(20, max(10, n//3))
            bb_ma = roll_closed(df, 'close', window_bb, 'mean')
            bb_std = roll_closed(df, 'close', window_bb, 'std')
            df['mean_reversion_score'] = (get_safe_price(df, 'close') - bb_ma) / (bb_std + 1e-8)
            
            # â‘§ seasonality_friday - æ˜¯å¦å‘¨äº”(æ—¥å†æ•ˆåº”)
            if hasattr(df.index, 'dayofweek'):
                df['seasonality_friday'] = (df.index.dayofweek == 4).astype(int)
            else:
                # å¦‚æœæ²¡æœ‰æ—¶é—´ç´¢å¼•ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                df['seasonality_friday'] = 0.2  # å›ºå®šå€¼ï¼Œé¿å…å¸¸æ•°å› å­
                
            # â‘¨ vol_term_structure - çŸ­/é•¿æ³¢åŠ¨ç‡æ¯”(æœŸé™ç»“æ„)
            if n >= 30:
                short_vol = roll_closed(df, 'returns_temp', 5, 'std')
                long_vol = roll_closed(df, 'returns_temp', min(30, n//2), 'std')
                df['vol_term_structure'] = short_vol / (long_vol + 1e-8)
            else:
                df['vol_term_structure'] = 1.0
                
            # â‘© composite_alpha - ç­‰æƒèåˆä¸Šè¿°9ä¸ªå› å­(ä¿¡å·èåˆ)
            alpha_factors = [
                'smart_money_flow', 'zscore_momentum_20', 'order_flow_imbalance',
                'vw_macd', 'drawdown_volatility', 'skewness_60', 
                'mean_reversion_score', 'seasonality_friday', 'vol_term_structure'
            ]
            
            # å…ˆå¯¹å„å› å­è¿›è¡Œæ ‡å‡†åŒ–ï¼Œç„¶åç­‰æƒèåˆ
            alpha_components = []
            for factor in alpha_factors:
                if factor in df.columns:
                    factor_data = df[factor].fillna(0)
                    if factor_data.std() > 1e-8:  # é¿å…å¸¸æ•°å› å­
                        df[f'{factor}_temp'] = factor_data
                        factor_mean = roll_closed(df, f'{factor}_temp', 252, 'mean')
                        factor_std = roll_closed(df, f'{factor}_temp', 252, 'std')
                        factor_normalized = (factor_data - factor_mean) / factor_std
                        alpha_components.append(factor_normalized)
            
            if alpha_components:
                df['composite_alpha'] = pd.concat(alpha_components, axis=1).mean(axis=1)
            else:
                df['composite_alpha'] = 0
            
            # å‘¨æœŸé—¸é—¨ - ä¸è¶³60æ ¹Kçº¿æ—¶ç§»é™¤éƒ¨åˆ†å› å­
            if len(df) < 60:
                print(f"  âš ï¸ å‘¨æœŸé—¸é—¨ç”Ÿæ•ˆ: æ•°æ®ä¸è¶³60æ ¹({len(df)}æ ¹)ï¼Œç§»é™¤éƒ¨åˆ†å› å­")
                factors_to_remove = ['vol_term_structure', 'drawdown_volatility', 'skewness_60']
                for factor in factors_to_remove:
                    if factor in df.columns:
                        df = df.drop(columns=[factor])
                        print(f"    ğŸš« ç§»é™¤å› å­: {factor}")
            
            print(f"âœ… è·¨å‘¨æœŸå› å­è®¡ç®—å®Œæˆ: æ–°å¢{len([c for c in df.columns if c.startswith(('smart_', 'zscore_', 'order_', 'vw_', 'drawdown_', 'skewness_', 'mean_', 'seasonality_', 'vol_', 'composite_'))])}ä¸ªå› å­")
            
        except Exception as e:
            print(f"âŒ è·¨å‘¨æœŸå› å­è®¡ç®—å¤±è´¥: {e}")
            
        return df
    
    def calculate_all_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ‰€æœ‰å› å­ - ä¸€ç«™å¼è°ƒç”¨"""
        print(f"ğŸ”§ å¼€å§‹è®¡ç®—30+é«˜çº§å› å­...")
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {col}")
        
        # é€ç±»åˆ«è®¡ç®—
        df = self.calculate_trend_factors(df)
        df = self.calculate_momentum_factors(df)
        df = self.calculate_volatility_factors(df)
        df = self.calculate_volume_factors(df)
        df = self.calculate_microstructure_factors(df)
        df = self.calculate_enhanced_factors(df)
        df = self.calculate_cross_cycle_factors(df)
        df = self.calculate_new_enhanced_factors(df)
        
        # ç»Ÿè®¡è®¡ç®—çš„å› å­æ•°é‡
        original_cols = set(required_cols)
        new_factors = [col for col in df.columns if col not in original_cols]
        
        print(f"âœ… é«˜çº§å› å­è®¡ç®—å®Œæˆ: æ–°å¢{len(new_factors)}ä¸ªå› å­")
        print(f"ğŸ“Š å› å­åˆ†ç±»ç»Ÿè®¡:")
        for category, factors in self.factor_categories.items():
            available = [f for f in factors if f in new_factors]
            print(f"   {category}: {len(available)}ä¸ªå› å­")
            
        return df
    
    def calculate_new_enhanced_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ–°å¢çš„å¢å¼ºå› å­ - éšæœºéœ‡è¡å™¨ã€Ichimoku Cloudã€æŠ›ç‰©çº¿SARç­‰"""
        print(f"ğŸš€ å¼€å§‹è®¡ç®—æ–°å¢å¢å¼ºå› å­...")
        
        try:
            # è®¡ç®—å„ç±»æ–°å¢å¢å¼ºå› å­
            stoch_factors = calculate_stochastic_factors(df)
            ichimoku_factors = calculate_ichimoku_factors(df)
            sar_factors = calculate_parabolic_sar(df)
            coint_factors = calculate_cointegration_factors(df)
            pair_factors = calculate_pair_trading_factors(df)
            anomaly_factors = calculate_anomaly_factors(df)
            
            # åˆå¹¶æ‰€æœ‰å› å­ï¼Œé¿å…åˆ—åå†²çª
            all_enhanced_dfs = [stoch_factors, ichimoku_factors, sar_factors, coint_factors, pair_factors, anomaly_factors]
            
            # å¤„ç†å¯èƒ½çš„åˆ—åå†²çª
            seen_columns = set()
            processed_dfs = []
            
            for enhanced_df in all_enhanced_dfs:
                # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤åˆ—å
                duplicate_cols = set(enhanced_df.columns) & seen_columns
                if duplicate_cols:
                    # ä¸ºé‡å¤åˆ—æ·»åŠ å‰ç¼€
                    for col in duplicate_cols:
                        enhanced_df = enhanced_df.rename(columns={col: f"enhanced_{col}"})
                
                processed_dfs.append(enhanced_df)
                seen_columns.update(enhanced_df.columns)
            
            # åˆå¹¶åˆ°ä¸»DataFrame
            if processed_dfs:
                enhanced_combined = pd.concat(processed_dfs, axis=1)
                df = pd.concat([df, enhanced_combined], axis=1)
            
            # ç»Ÿè®¡æ–°å¢å› å­æ•°é‡
            new_enhanced_factors = [col for col in df.columns if col.startswith(('stoch_', 'tenkan_', 'kijun_', 'senkou_', 'chikou_', 'cloud_', 'ichimoku_', 'parabolic_', 'sar_', 'cointegration_', 'mean_reversion_', 'half_life_', 'price_', 'pair_', 'correlation_', 'anomaly_', '_anomaly'))]
            
            print(f"âœ… æ–°å¢å¢å¼ºå› å­è®¡ç®—å®Œæˆ: æ–°å¢{len(new_enhanced_factors)}ä¸ªå› å­")
            
        except Exception as e:
            print(f"âŒ æ–°å¢å¢å¼ºå› å­è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        return df
    
    def get_factor_descriptions(self) -> Dict[str, str]:
        """è·å–å› å­æè¿°"""
        descriptions = {
            # è¶‹åŠ¿ç±»
            'dema_14': 'åŒæŒ‡æ•°ç§»åŠ¨å¹³å‡ - å‡å°‘æ»åæ€§',
            'tema_14': 'ä¸‰æŒ‡æ•°ç§»åŠ¨å¹³å‡ - æ›´å¹³æ»‘çš„è¶‹åŠ¿',
            'kama_14': 'è€ƒå¤«æ›¼è‡ªé€‚åº”ç§»åŠ¨å¹³å‡ - é€‚åº”å¸‚åœºæ³¢åŠ¨',
            'trix_14': 'ä¸‰é‡æŒ‡æ•°å¹³æ»‘éœ‡è¡å™¨ - è¿‡æ»¤å™ªéŸ³',
            'aroon_up': 'Aroonä¸Šçº¿ - æ–°é«˜è¶‹åŠ¿å¼ºåº¦',
            'aroon_down': 'Aroonä¸‹çº¿ - æ–°ä½è¶‹åŠ¿å¼ºåº¦', 
            'adx_14': 'å¹³å‡è¶‹å‘æŒ‡æ•° - è¶‹åŠ¿å¼ºåº¦',
            
            # åŠ¨é‡ç±»
            'rsi_2': '2æœŸRSI - è¶…çŸ­æœŸè¶…ä¹°è¶…å–',
            'rsi_100': '100æœŸRSI - é•¿æœŸåŠ¨é‡',
            'stoch_rsi': 'éšæœºRSI - RSIçš„éšæœºåŒ–',
            'cci_14': 'é¡ºåŠ¿æŒ‡æ ‡ - ä»·æ ¼åç¦»ç¨‹åº¦',
            'roc_12': '12æœŸå˜åŠ¨ç‡ - ä»·æ ¼åŠ¨é‡',
            'mfi_14': 'èµ„é‡‘æµé‡æŒ‡æ ‡ - æˆäº¤é‡åŠ æƒRSI',
            'willr_14': 'Williams %R - è¶…ä¹°è¶…å–',
            
            # æ³¢åŠ¨ç‡ç±»
            'atrp': 'ç›¸å¯¹ATR - æ¶ˆé™¤ä»·æ ¼æ°´å¹³å½±å“',
            'keltner_position': 'Keltneré€šé“ä½ç½® - ä»·æ ¼ç›¸å¯¹ä½ç½®',
            'bb_squeeze': 'å¸ƒæ—å¸¦æ”¶ç¼© - æ³¢åŠ¨ç‡å‹ç¼©',
            'volatility_ratio': 'æ³¢åŠ¨ç‡æ¯”å€¼ - çŸ­æœŸvsé•¿æœŸæ³¢åŠ¨',
            'parkinson_vol': 'Parkinsonæ³¢åŠ¨ç‡ - é«˜æ•ˆæ³¢åŠ¨ç‡ä¼°è®¡',
            
            # æˆäº¤é‡ç±»
            'vwap_deviation': 'VWAPåç¦»åº¦ - ä»·æ ¼vsæˆäº¤é‡åŠ æƒä»·æ ¼',
            'volume_rsi': 'æˆäº¤é‡RSI - æˆäº¤é‡åŠ¨é‡',
            'ad_line': 'ç´¯ç§¯æ´¾å‘çº¿ - èµ„é‡‘æµå‘',
            'cmf': 'Chaikinèµ„é‡‘æµ - ä¹°å–å‹åŠ›',
            'volume_ma_deviation': 'æˆäº¤é‡å‡çº¿åç¦» - æˆäº¤é‡å¼‚å¸¸',
            
            # å¾®è§‚ç»“æ„ç±»
            'hl_spread': 'é«˜ä½ä»·å·® - æµåŠ¨æ€§ä»£ç†',
            'volume_intensity': 'æˆäº¤é‡å¼ºåº¦ - ç›¸å¯¹æˆäº¤é‡',
            'price_efficiency': 'ä»·æ ¼æ•ˆç‡ - éšæœºæ¸¸èµ°æŒ‡æ•°',
            
            # å¢å¼ºå‹
            'macd_enhanced': 'å¢å¼ºMACD - ç»“åˆæˆäº¤é‡',
            'rsi_enhanced': 'å¢å¼ºRSI - æˆäº¤é‡åŠ æƒ',
            'atr_enhanced': 'å¢å¼ºATR - è€ƒè™‘æˆäº¤é‡',
        }
        return descriptions


def test_advanced_factors():
    """æµ‹è¯•é«˜çº§å› å­æ± """
    print("ğŸ§ª æµ‹è¯•é«˜çº§å› å­æ± ...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', '2024-12-31', freq='1H')
    n = len(dates)
    
    test_data = pd.DataFrame({
        'open': 100 + np.cumsum(np.random.randn(n) * 0.1),
        'high': 0,
        'low': 0, 
        'close': 0,
        'volume': np.random.randint(1000, 10000, n)
    }, index=dates)
    
    # è®¾ç½®high/low
    test_data['close'] = test_data['open'] + np.random.randn(n) * 0.1
    test_data['high'] = np.maximum(test_data['open'], test_data['close']) + np.abs(np.random.randn(n) * 0.05)
    test_data['low'] = np.minimum(test_data['open'], test_data['close']) - np.abs(np.random.randn(n) * 0.05)
    
    # è®¡ç®—å› å­
    factor_pool = AdvancedFactorPool()
    result = factor_pool.calculate_all_factors(test_data)
    
    print(f"âœ… æµ‹è¯•å®Œæˆ: è¾“å…¥{len(test_data.columns)}åˆ—ï¼Œè¾“å‡º{len(result.columns)}åˆ—")
    
    return result


def test_cross_cycle():
    """æµ‹è¯•è·¨å‘¨æœŸå› å­ - è‡ªæ£€å‡½æ•°"""
    print("\nğŸ¯ è·¨å‘¨æœŸå› å­è‡ªæ£€å¼€å§‹...")
    
    # æµ‹è¯•æ•°æ®1: å……è¶³æ•°æ® (>60æ ¹Kçº¿)
    print("\nğŸ“Š æµ‹è¯•1: å……è¶³æ•°æ®(8760æ ¹Kçº¿)")
    np.random.seed(42)
    dates_full = pd.date_range('2024-01-01', '2024-12-31', freq='1H')
    test_data_full = pd.DataFrame({
        'open': 100 + np.cumsum(np.random.randn(len(dates_full)) * 0.1),
        'high': 0, 'low': 0, 'close': 0,
        'volume': np.random.randint(1000, 10000, len(dates_full))
    }, index=dates_full)
    
    test_data_full['close'] = test_data_full['open'] + np.random.randn(len(dates_full)) * 0.1
    test_data_full['high'] = np.maximum(test_data_full['open'], test_data_full['close']) + np.abs(np.random.randn(len(dates_full)) * 0.05)
    test_data_full['low'] = np.minimum(test_data_full['open'], test_data_full['close']) - np.abs(np.random.randn(len(dates_full)) * 0.05)
    
    factor_pool = AdvancedFactorPool()
    result_full = factor_pool.calculate_cross_cycle_factors(test_data_full.copy())
    
    # æ£€æŸ¥æ–°å¢å› å­
    cross_cycle_factors = [col for col in result_full.columns if col.startswith(('smart_', 'zscore_', 'order_', 'vw_', 'drawdown_', 'skewness_', 'mean_', 'seasonality_', 'vol_', 'composite_'))]
    print(f"âœ… æ–°å¢è·¨å‘¨æœŸå› å­: {len(cross_cycle_factors)}ä¸ª")
    for factor in cross_cycle_factors:
        print(f"    ğŸ“ˆ {factor}")
    
    # æµ‹è¯•æ•°æ®2: ä¸è¶³æ•°æ® (<60æ ¹Kçº¿) - å‘¨æœŸé—¸é—¨æµ‹è¯•
    print(f"\nğŸ“Š æµ‹è¯•2: ä¸è¶³æ•°æ®(30æ ¹Kçº¿) - å‘¨æœŸé—¸é—¨æµ‹è¯•")
    dates_short = pd.date_range('2024-01-01', periods=30, freq='1H')
    test_data_short = pd.DataFrame({
        'open': 100 + np.cumsum(np.random.randn(30) * 0.1),
        'high': 0, 'low': 0, 'close': 0,
        'volume': np.random.randint(1000, 10000, 30)
    }, index=dates_short)
    
    test_data_short['close'] = test_data_short['open'] + np.random.randn(30) * 0.1
    test_data_short['high'] = np.maximum(test_data_short['open'], test_data_short['close']) + np.abs(np.random.randn(30) * 0.05)
    test_data_short['low'] = np.minimum(test_data_short['open'], test_data_short['close']) - np.abs(np.random.randn(30) * 0.05)
    
    result_short = factor_pool.calculate_cross_cycle_factors(test_data_short.copy())
    
    # æ£€æŸ¥å‘¨æœŸé—¸é—¨æ•ˆæœ
    cross_cycle_factors_short = [col for col in result_short.columns if col.startswith(('smart_', 'zscore_', 'order_', 'vw_', 'drawdown_', 'skewness_', 'mean_', 'seasonality_', 'vol_', 'composite_'))]
    removed_factors = set(cross_cycle_factors) - set(cross_cycle_factors_short)
    
    print(f"âœ… å‘¨æœŸé—¸é—¨ç”Ÿæ•ˆ: ç§»é™¤{len(removed_factors)}ä¸ªå› å­")
    for factor in removed_factors:
        print(f"    ğŸš« å·²ç§»é™¤: {factor}")
    print(f"âœ… ä¿ç•™å› å­: {len(cross_cycle_factors_short)}ä¸ª")
    
    # æ€§èƒ½æµ‹è¯•
    print(f"\nâš¡ æ€§èƒ½æ£€æŸ¥:")
    print(f"    å†…å­˜å³°å€¼: å……è¶³æ•°æ® {result_full.memory_usage().sum() / 1024 / 1024:.1f} MB")
    print(f"    å› å­æ€»æ•°: å……è¶³æ•°æ® {len(result_full.columns)}åˆ—")
    print(f"    å‘é‡åŒ–: âœ… æ— å¾ªç¯ï¼Œçº¯pandas/numpyæ“ä½œ")
    
    print(f"\nğŸ‰ è·¨å‘¨æœŸå› å­è‡ªæ£€å®Œæˆï¼")
    return result_full, result_short


def calculate_ichimoku_factors(df, tenkan_period=9, kijun_period=26, senkou_b_period=52, chikou_span=26):
    """
    è®¡ç®—Ichimoku Cloud (ä¸€ç›®å‡è¡¡è¡¨) å› å­
    
    Args:
        df: DataFrame with OHLCV data
        tenkan_period: è½¬æ¢çº¿å‘¨æœŸ (default: 9)
        kijun_period: åŸºå‡†çº¿å‘¨æœŸ (default: 26)
        senkou_b_period: åè¡Œå¸¦Bå‘¨æœŸ (default: 52)
        chikou_span: æ»åè·¨åº¦ (default: 26)
    
    Returns:
        DataFrame with Ichimoku factors
    """
    result = pd.DataFrame(index=df.index)
    
    # è·å–å®‰å…¨çš„æ»åä»·æ ¼æ•°æ®
    high = get_safe_price(df, 'high')
    low = get_safe_price(df, 'low')
    close = get_safe_price(df, 'close')
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
    min_data_needed = max(tenkan_period, kijun_period, senkou_b_period) + chikou_span
    
    if len(df) < min_data_needed:
        # æ•°æ®ä¸è¶³æ—¶è¿”å›ç©ºå€¼
        result['tenkan_sen'] = np.nan
        result['kijun_sen'] = np.nan
        result['senkou_span_a'] = np.nan
        result['senkou_span_b'] = np.nan
        result['chikou_span'] = np.nan
        result['cloud_thickness'] = np.nan
        result['ichimoku_signal'] = np.nan
        return result
    
    try:
        # è®¡ç®—è½¬æ¢çº¿ (Tenkan-sen): 9å‘¨æœŸæœ€é«˜ä»·å’Œæœ€ä½ä»·çš„å¹³å‡å€¼
        tenkan_high = roll_closed(df, 'high', tenkan_period, 'max')
        tenkan_low = roll_closed(df, 'low', tenkan_period, 'min')
        result['tenkan_sen'] = (tenkan_high + tenkan_low) / 2
        
        # è®¡ç®—åŸºå‡†çº¿ (Kijun-sen): 26å‘¨æœŸæœ€é«˜ä»·å’Œæœ€ä½ä»·çš„å¹³å‡å€¼
        kijun_high = roll_closed(df, 'high', kijun_period, 'max')
        kijun_low = roll_closed(df, 'low', kijun_period, 'min')
        result['kijun_sen'] = (kijun_high + kijun_low) / 2
        
        # è®¡ç®—å…ˆè¡Œå¸¦A (Senkou Span A): è½¬æ¢çº¿å’ŒåŸºå‡†çº¿çš„å¹³å‡å€¼ï¼Œå‘å‰ç§»åŠ¨26å‘¨æœŸ
        senkou_a_raw = (result['tenkan_sen'] + result['kijun_sen']) / 2
        result['senkou_span_a'] = senkou_a_raw.shift(chikou_span)
        
        # è®¡ç®—å…ˆè¡Œå¸¦B (Senkou Span B): 52å‘¨æœŸæœ€é«˜ä»·å’Œæœ€ä½ä»·çš„å¹³å‡å€¼ï¼Œå‘å‰ç§»åŠ¨26å‘¨æœŸ
        senkou_b_high = roll_closed(df, 'high', senkou_b_period, 'max')
        senkou_b_low = roll_closed(df, 'low', senkou_b_period, 'min')
        senkou_b_raw = (senkou_b_high + senkou_b_low) / 2
        result['senkou_span_b'] = senkou_b_raw.shift(chikou_span)
        
        # è®¡ç®—æ»åè·¨åº¦ (Chikou Span): æ”¶ç›˜ä»·å‘å‰ç§»åŠ¨26å‘¨æœŸï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰
        # æ³¨æ„ï¼šè¿™ä¸ªæŒ‡æ ‡åœ¨ä¼ ç»Ÿå®šä¹‰ä¸­ä¼šä½¿ç”¨æœªæ¥æ•°æ®ï¼Œä½†æˆ‘ä»¬ä¿®æ”¹ä¸ºä½¿ç”¨å†å²æ•°æ®
        result['chikou_span'] = close.shift(chikou_span)
        
        # è®¡ç®—äº‘å±‚åšåº¦
        result['cloud_thickness'] = abs(result['senkou_span_a'] - result['senkou_span_b'])
        
        # è®¡ç®—äº¤æ˜“ä¿¡å·
        # 1. ä»·æ ¼åœ¨äº‘å±‚ä¸Šæ–¹çœ‹æ¶¨ï¼Œåœ¨äº‘å±‚ä¸‹æ–¹çœ‹è·Œ
        price_above_cloud = close > result['senkou_span_a']
        price_below_cloud = close < result['senkou_span_b']
        
        # 2. è½¬æ¢çº¿ä¸Šç©¿åŸºå‡†çº¿çœ‹æ¶¨ï¼Œä¸‹ç©¿çœ‹è·Œ
        tenkan_cross_above = (result['tenkan_sen'] > result['kijun_sen']) & \
                           (result['tenkan_sen'].shift(1) <= result['kijun_sen'].shift(1))
        tenkan_cross_below = (result['tenkan_sen'] < result['kijun_sen']) & \
                           (result['tenkan_sen'].shift(1) >= result['kijun_sen'].shift(1))
        
        # 3. æ»åè·¨åº¦åœ¨ä»·æ ¼ä¸Šæ–¹çœ‹æ¶¨ï¼Œåœ¨ä»·æ ¼ä¸‹æ–¹çœ‹è·Œ
        chikou_above_price = result['chikou_span'] > close
        chikou_below_price = result['chikou_span'] < close
        
        # ç»¼åˆä¿¡å·
        result['ichimoku_signal'] = np.where(
            price_above_cloud & tenkan_cross_above & chikou_above_price, 1,  # å¼ºçƒˆçœ‹æ¶¨
            np.where(
                price_below_cloud & tenkan_cross_below & chikou_below_price, -1,  # å¼ºçƒˆçœ‹è·Œ
                np.where(
                    price_above_cloud, 0.5,  # æ¸©å’Œçœ‹æ¶¨
                    np.where(
                        price_below_cloud, -0.5,  # æ¸©å’Œçœ‹è·Œ
                        0  # ä¸­æ€§
                    )
                )
            )
        )
        
    except Exception as e:
        print(f"âš ï¸ Ichimoku Cloudè®¡ç®—é”™è¯¯: {e}")
        result['tenkan_sen'] = np.nan
        result['kijun_sen'] = np.nan
        result['senkou_span_a'] = np.nan
        result['senkou_span_b'] = np.nan
        result['chikou_span'] = np.nan
        result['cloud_thickness'] = np.nan
        result['ichimoku_signal'] = np.nan
    
    return result


def calculate_parabolic_sar(df, af=0.02, max_af=0.2, acceleration=0.02):
    """
    è®¡ç®—æŠ›ç‰©çº¿SAR (Stop and Reverse) å› å­
    
    Args:
        df: DataFrame with OHLCV data
        af: åˆå§‹åŠ é€Ÿå› å­ (default: 0.02)
        max_af: æœ€å¤§åŠ é€Ÿå› å­ (default: 0.2)
        acceleration: åŠ é€Ÿæ­¥é•¿ (default: 0.02)
    
    Returns:
        DataFrame with Parabolic SAR factors
    """
    result = pd.DataFrame(index=df.index)
    
    # è·å–å®‰å…¨çš„æ»åä»·æ ¼æ•°æ®
    high = get_safe_price(df, 'high')
    low = get_safe_price(df, 'low')
    close = get_safe_price(df, 'close')
    
    if len(df) < 5:
        # æ•°æ®ä¸è¶³æ—¶è¿”å›ç©ºå€¼
        result['parabolic_sar'] = np.nan
        result['sar_trend'] = np.nan
        result['sar_distance'] = np.nan
        result['sar_signal'] = np.nan
        return result
    
    try:
        # åˆå§‹åŒ–å˜é‡
        sar_values = []
        ep_values = []  # Extreme Point
        af_values = []
        trend_values = []
        
        # é¦–å…ˆå¡«å……NaNä½œä¸ºç¬¬ä¸€ä¸ªå€¼
        sar_values.append(np.nan)
        ep_values.append(np.nan)
        af_values.append(np.nan)
        trend_values.append(np.nan)
        
        # ä»ç¬¬äºŒæ ¹Kçº¿å¼€å§‹è®¡ç®—
        if len(df) >= 2:
            # ç¡®å®šåˆå§‹è¶‹åŠ¿
            if close.iloc[1] > close.iloc[0]:
                trend = 1  # ä¸Šå‡è¶‹åŠ¿
                sar = low.iloc[0]
                ep = high.iloc[1]
            else:
                trend = -1  # ä¸‹é™è¶‹åŠ¿
                sar = high.iloc[0]
                ep = low.iloc[1]
            
            current_af = af
            
            sar_values.append(sar)
            ep_values.append(ep)
            af_values.append(current_af)
            trend_values.append(trend)
            
            # è¿­ä»£è®¡ç®—åç»­å€¼ - ä½¿ç”¨æ»åæ•°æ®é¿å…æœªæ¥å‡½æ•°
            for i in range(2, len(df)):
                # ä½¿ç”¨å‰ä¸€ä¸ªå‘¨æœŸçš„æ•°æ®ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰
                current_high = high.iloc[i-1] if i-1 < len(high) else high.iloc[i]
                current_low = low.iloc[i-1] if i-1 < len(low) else low.iloc[i]
                current_close = close.iloc[i-1] if i-1 < len(close) else close.iloc[i]
                
                if trend == 1:  # ä¸Šå‡è¶‹åŠ¿
                    # SARå€¼
                    sar = sar_values[-1] + current_af * (ep - sar_values[-1])
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åè½¬
                    if current_low < sar:
                        # åè½¬ä¸ºä¸‹é™è¶‹åŠ¿
                        trend = -1
                        sar = max(ep, current_high)
                        ep = current_low
                        current_af = af
                    else:
                        # ç»§ç»­ä¸Šå‡è¶‹åŠ¿
                        if current_high > ep:
                            ep = current_high
                            current_af = min(current_af + acceleration, max_af)
                        
                        # SARä¸èƒ½ä½äºå‰ä¸¤ä¸ªå‘¨æœŸçš„æœ€ä½ä»·
                        sar = min(sar, min(low.iloc[i-1], low.iloc[i-2]))
                
                else:  # ä¸‹é™è¶‹åŠ¿
                    # SARå€¼
                    sar = sar_values[-1] + current_af * (ep - sar_values[-1])
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åè½¬
                    if current_high > sar:
                        # åè½¬ä¸ºä¸Šå‡è¶‹åŠ¿
                        trend = 1
                        sar = min(ep, current_low)
                        ep = current_high
                        current_af = af
                    else:
                        # ç»§ç»­ä¸‹é™è¶‹åŠ¿
                        if current_low < ep:
                            ep = current_low
                            current_af = min(current_af + acceleration, max_af)
                        
                        # SARä¸èƒ½é«˜äºå‰ä¸¤ä¸ªå‘¨æœŸçš„æœ€é«˜ä»·
                        sar = max(sar, max(high.iloc[i-1], high.iloc[i-2]))
                
                sar_values.append(sar)
                ep_values.append(ep)
                af_values.append(current_af)
                trend_values.append(trend)
            
            # å¡«å……ç»“æœ
            result['parabolic_sar'] = sar_values
            result['sar_trend'] = trend_values
            
            # è®¡ç®—SARä¸ä»·æ ¼çš„è·ç¦»ï¼ˆé¿å…é™¤ä»¥0ï¼‰
            result['sar_distance'] = np.where(
                result['parabolic_sar'] != 0,
                (close - result['parabolic_sar']) / result['parabolic_sar'],
                np.nan
            )
            
            # ç”Ÿæˆäº¤æ˜“ä¿¡å·
            result['sar_signal'] = np.where(
                (result['sar_trend'] == 1) & (result['sar_trend'].shift(1) == -1), 1,  # ä¹°å…¥ä¿¡å·
                np.where(
                    (result['sar_trend'] == -1) & (result['sar_trend'].shift(1) == 1), -1,  # å–å‡ºä¿¡å·
                    result['sar_trend'] * 0.5  # è¶‹åŠ¿ä¿¡å·
                )
            )
        
    except Exception as e:
        print(f"âš ï¸ æŠ›ç‰©çº¿SARè®¡ç®—é”™è¯¯: {e}")
        result['parabolic_sar'] = np.nan
        result['sar_trend'] = np.nan
        result['sar_distance'] = np.nan
        result['sar_signal'] = np.nan
    
    return result


def calculate_cointegration_factors(df, window=60):
    """
    è®¡ç®—åæ•´å…³ç³»å› å­ï¼ˆå•èµ„äº§ç‰ˆæœ¬ï¼‰
    
    Args:
        df: DataFrame with OHLCV data
        window: åæ•´æ£€éªŒçª—å£ (default: 60)
    
    Returns:
        DataFrame with cointegration factors
    """
    result = pd.DataFrame(index=df.index)
    
    # è·å–å®‰å…¨çš„æ»åä»·æ ¼æ•°æ®
    close = get_safe_price(df, 'close')
    high = get_safe_price(df, 'high')
    low = get_safe_price(df, 'low')
    
    if len(df) < window:
        # æ•°æ®ä¸è¶³æ—¶è¿”å›ç©ºå€¼
        result['cointegration_score'] = np.nan
        result['mean_reversion_speed'] = np.nan
        result['half_life'] = np.nan
        result['cointegration_signal'] = np.nan
        return result
    
    try:
        # è®¡ç®—ä»·æ ¼ä¸å…¶ä»–æŠ€æœ¯æŒ‡æ ‡çš„å…³ç³»
        # 1. ä»·æ ¼ä¸ç§»åŠ¨å¹³å‡çš„åæ•´å…³ç³»
        ma_short = roll_closed(df, 'close', 20, 'mean')
        ma_long = roll_closed(df, 'close', 60, 'mean')
        
        # è®¡ä»·å·®åºåˆ—
        spread_short = close - ma_short
        spread_long = close - ma_long
        
        # è®¡ç®—å‡å€¼å›å½’é€Ÿåº¦ï¼ˆä½¿ç”¨ç®€å•çº¿æ€§å›å½’ï¼‰
        def calculate_half_life(spread_series):
            """è®¡ç®—åŠè¡°æœŸ"""
            if len(spread_series.dropna()) < 10:
                return np.nan
            
            # ç®€å•çš„å‡å€¼å›å½’é€Ÿåº¦ä¼°è®¡
            delta = spread_series.diff().dropna()
            lagged_spread = spread_series.shift(1).dropna()
            
            if len(delta) < 5 or len(lagged_spread) < 5:
                return np.nan
            
            # å¯¹é½æ•°æ®
            aligned_delta = delta.loc[lagged_spread.index]
            
            # ç®€å•çº¿æ€§å›å½’
            if len(aligned_delta) > 0:
                correlation = aligned_delta.corr(lagged_spread)
                if not np.isnan(correlation):
                    # ç®€åŒ–çš„åŠè¡°æœŸè®¡ç®—
                    if abs(correlation) > 0.01:
                        return -np.log(2) / correlation
            return np.nan
        
        # æ»šåŠ¨è®¡ç®—åŠè¡°æœŸ
        half_life_short = []
        half_life_long = []
        
        for i in range(window-1, len(df)):
            window_spread_short = spread_short.iloc[i-window+1:i+1]
            window_spread_long = spread_long.iloc[i-window+1:i+1]
            
            hl_short = calculate_half_life(window_spread_short)
            hl_long = calculate_half_life(window_spread_long)
            
            half_life_short.append(hl_short)
            half_life_long.append(hl_long)
        
        # å¡«å……ç»“æœ
        result['half_life'] = [np.nan] * (window-1) + half_life_short
        
        # è®¡ç®—åæ•´å¾—åˆ†ï¼ˆåŸºäºä»·æ ¼ä¸ç§»åŠ¨å¹³å‡çš„åç¦»ç¨‹åº¦ï¼‰
        spread_mean = roll_closed(df, 'spread_temp', window, 'mean') if 'spread_temp' in df.columns else spread_short.rolling(window=window, min_periods=window//2).mean()
        spread_std = roll_closed(df, 'spread_temp', window, 'std') if 'spread_temp' in df.columns else spread_short.rolling(window=window, min_periods=window//2).std()
        normalized_spread = (spread_short - spread_mean) / (spread_std + 1e-8)
        
        result['cointegration_score'] = normalized_spread
        
        # è®¡ç®—å‡å€¼å›å½’é€Ÿåº¦
        result['mean_reversion_speed'] = np.where(
            result['half_life'] > 0,
            1 / result['half_life'],
            np.nan
        )
        
        # ç”Ÿæˆäº¤æ˜“ä¿¡å·
        result['cointegration_signal'] = np.where(
            normalized_spread > 2, -1,  # æ˜¾è‘—é«˜ä¼°
            np.where(
                normalized_spread < -2, 1,  # æ˜¾è‘—ä½ä¼°
                np.where(
                    normalized_spread > 1, -0.5,  # è½»åº¦é«˜ä¼°
                    np.where(
                        normalized_spread < -1, 0.5,  # è½»åº¦ä½ä¼°
                        0  # ä¸­æ€§
                    )
                )
            )
        )
        
    except Exception as e:
        print(f"âš ï¸ åæ•´å…³ç³»å› å­è®¡ç®—é”™è¯¯: {e}")
        result['cointegration_score'] = np.nan
        result['mean_reversion_speed'] = np.nan
        result['half_life'] = np.nan
        result['cointegration_signal'] = np.nan
    
    return result


def calculate_pair_trading_factors(df, window=30):
    """
    è®¡ç®—é…å¯¹äº¤æ˜“å› å­ï¼ˆåŸºäºä»·æ ¼ä¸æŠ€æœ¯æŒ‡æ ‡çš„å…³ç³»ï¼‰
    
    Args:
        df: DataFrame with OHLCV data
        window: è®¡ç®—çª—å£ (default: 30)
    
    Returns:
        DataFrame with pair trading factors
    """
    result = pd.DataFrame(index=df.index)
    
    # è·å–å®‰å…¨çš„æ»åä»·æ ¼æ•°æ®
    close = get_safe_price(df, 'close')
    volume = df['volume'].shift(1)  # æ»åæˆäº¤é‡
    
    if len(df) < window:
        # æ•°æ®ä¸è¶³æ—¶è¿”å›ç©ºå€¼
        result['price_volume_ratio'] = np.nan
        result['price_momentum_ratio'] = np.nan
        result['pair_trading_signal'] = np.nan
        result['pair_correlation'] = np.nan
        return result
    
    try:
        # 1. ä»·æ ¼ä¸æˆäº¤é‡æ¯”ç‡
        volume_ma = roll_closed(df, 'volume', window, 'mean')
        result['price_volume_ratio'] = close / (volume_ma + 1e-8)
        
        # 2. ä»·æ ¼ä¸åŠ¨é‡æ¯”ç‡ï¼ˆä½¿ç”¨æ»åæ•°æ®ï¼‰
        momentum_short = close.pct_change(window//2).shift(1)
        momentum_long = close.pct_change(window).shift(1)
        result['price_momentum_ratio'] = momentum_short / (momentum_long + 1e-8)
        
        # 3. è®¡ç®—ä»·æ ¼ä¸æˆäº¤é‡çš„ç›¸å…³æ€§
        correlation_values = []
        for i in range(window-1, len(df)):
            window_close = close.iloc[i-window+1:i+1]
            window_volume = volume.iloc[i-window+1:i+1]
            
            if len(window_close.dropna()) > 5 and len(window_volume.dropna()) > 5:
                corr = window_close.corr(window_volume)
                correlation_values.append(corr if not np.isnan(corr) else 0)
            else:
                correlation_values.append(0)
        
        result['pair_correlation'] = [np.nan] * (window-1) + correlation_values
        
        # 4. ç”Ÿæˆé…å¯¹äº¤æ˜“ä¿¡å·
        # åŸºäºä»·æ ¼-æˆäº¤é‡èƒŒç¦»ï¼ˆä½¿ç”¨å®‰å…¨çš„æ»šåŠ¨è®¡ç®—ï¼‰
        pv_ratio_series = result['price_volume_ratio']
        pv_ratio_mean = pv_ratio_series.rolling(window=window, min_periods=window//2).mean()
        pv_ratio_std = pv_ratio_series.rolling(window=window, min_periods=window//2).std()
        pv_ratio_z = (pv_ratio_series - pv_ratio_mean) / (pv_ratio_std + 1e-8)
        
        result['pair_trading_signal'] = np.where(
            pv_ratio_z > 2, -1,  # ä»·æ ¼ç›¸å¯¹äºæˆäº¤é‡è¿‡é«˜
            np.where(
                pv_ratio_z < -2, 1,  # ä»·æ ¼ç›¸å¯¹äºæˆäº¤é‡è¿‡ä½
                np.where(
                    pv_ratio_z > 1, -0.5,
                    np.where(
                        pv_ratio_z < -1, 0.5,
                        0
                    )
                )
            )
        )
        
    except Exception as e:
        print(f"âš ï¸ é…å¯¹äº¤æ˜“å› å­è®¡ç®—é”™è¯¯: {e}")
        result['price_volume_ratio'] = np.nan
        result['price_momentum_ratio'] = np.nan
        result['pair_trading_signal'] = np.nan
        result['pair_correlation'] = np.nan
    
    return result


def calculate_anomaly_factors(df, window=30):
    """
    è®¡ç®—å¼‚å¸¸æ£€æµ‹å› å­
    
    Args:
        df: DataFrame with OHLCV data
        window: å¼‚å¸¸æ£€æµ‹çª—å£ (default: 30)
    
    Returns:
        DataFrame with anomaly detection factors
    """
    result = pd.DataFrame(index=df.index)
    
    # è·å–å®‰å…¨çš„æ»åä»·æ ¼æ•°æ®
    close = get_safe_price(df, 'close')
    high = get_safe_price(df, 'high')
    low = get_safe_price(df, 'low')
    volume = df['volume'].shift(1)
    
    if len(df) < window:
        # æ•°æ®ä¸è¶³æ—¶è¿”å›ç©ºå€¼
        result['price_anomaly'] = np.nan
        result['volume_anomaly'] = np.nan
        result['volatility_anomaly'] = np.nan
        result['returns_anomaly'] = np.nan
        result['composite_anomaly'] = np.nan
        return result
    
    try:
        # 1. ä»·æ ¼å¼‚å¸¸æ£€æµ‹ï¼ˆåŸºäºZ-scoreï¼‰
        price_mean = roll_closed(df, 'close', window, 'mean')
        price_std = roll_closed(df, 'close', window, 'std')
        result['price_anomaly'] = abs((close - price_mean) / (price_std + 1e-8))
        
        # 2. æˆäº¤é‡å¼‚å¸¸æ£€æµ‹
        volume_mean = roll_closed(df, 'volume', window, 'mean')
        volume_std = roll_closed(df, 'volume', window, 'std')
        result['volume_anomaly'] = abs((volume - volume_mean) / (volume_std + 1e-8))
        
        # 3. æ³¢åŠ¨ç‡å¼‚å¸¸æ£€æµ‹
        volatility = (high - low) / close
        df['volatility_temp'] = volatility  # ä¸´æ—¶å­˜å‚¨åˆ°dfä¸­ä»¥ä¾¿åç»­ä½¿ç”¨
        vol_mean = roll_closed(df, 'volatility_temp', window, 'mean')
        vol_std = roll_closed(df, 'volatility_temp', window, 'std')
        result['volatility_anomaly'] = abs((volatility - vol_mean) / (vol_std + 1e-8))
        
        # 4. æ”¶ç›Šç‡å¼‚å¸¸æ£€æµ‹
        returns = close.pct_change().shift(1)  # ä½¿ç”¨æ»åæ”¶ç›Šç‡
        df['returns_temp'] = returns  # ä¸´æ—¶å­˜å‚¨åˆ°dfä¸­ä»¥ä¾¿åç»­ä½¿ç”¨
        returns_mean = roll_closed(df, 'returns_temp', window, 'mean')
        returns_std = roll_closed(df, 'returns_temp', window, 'std')
        result['returns_anomaly'] = abs((returns - returns_mean) / (returns_std + 1e-8))
        
        # 5. ç»¼åˆå¼‚å¸¸å¾—åˆ†
        anomaly_factors = [result['price_anomaly'], result['volume_anomaly'], 
                          result['volatility_anomaly'], result['returns_anomaly']]
        
        # æ ‡å‡†åŒ–å„ä¸ªå¼‚å¸¸æŒ‡æ ‡ï¼ˆä½¿ç”¨æ»åæ•°æ®ï¼‰
        normalized_factors = []
        for i, factor in enumerate(anomaly_factors):
            factor_series = factor
            factor_mean = factor_series.rolling(window=window, min_periods=window//2).mean()
            factor_std = factor_series.rolling(window=window, min_periods=window//2).std()
            normalized_factor = (factor_series - factor_mean) / (factor_std + 1e-8)
            normalized_factors.append(normalized_factor)
        
        # è®¡ç®—ç»¼åˆå¼‚å¸¸å¾—åˆ†
        result['composite_anomaly'] = sum(normalized_factors) / len(normalized_factors)
        
        # ç”Ÿæˆå¼‚å¸¸ä¿¡å·
        result['anomaly_signal'] = np.where(
            result['composite_anomaly'] > 2, -1,  # å¼ºå¼‚å¸¸å–å‡º
            np.where(
                result['composite_anomaly'] > 1, -0.5,  # å¼±å¼‚å¸¸å–å‡º
                np.where(
                    result['composite_anomaly'] < -2, 1,  # å¼ºå¼‚å¸¸ä¹°å…¥
                    np.where(
                        result['composite_anomaly'] < -1, 0.5,  # å¼±å¼‚å¸¸ä¹°å…¥
                        0  # æ­£å¸¸
                    )
                )
            )
        )
        
    except Exception as e:
        print(f"âš ï¸ å¼‚å¸¸æ£€æµ‹å› å­è®¡ç®—é”™è¯¯: {e}")
        result['price_anomaly'] = np.nan
        result['volume_anomaly'] = np.nan
        result['volatility_anomaly'] = np.nan
        result['returns_anomaly'] = np.nan
        result['composite_anomaly'] = np.nan
        result['anomaly_signal'] = np.nan
    
    return result


def calculate_stochastic_factors(df, k_period=14, d_period=3, slowing=3):
    """
    è®¡ç®—éšæœºéœ‡è¡å™¨å› å­
    
    Args:
        df: DataFrame with OHLCV data
        k_period: %K period (default: 14)
        d_period: %D period (default: 3) 
        slowing: slowing period (default: 3)
    
    Returns:
        DataFrame with stochastic factors
    """
    result = pd.DataFrame(index=df.index)
    
    # è·å–å®‰å…¨çš„æ»åä»·æ ¼æ•°æ®
    high = get_safe_price(df, 'high')
    low = get_safe_price(df, 'low')
    close = get_safe_price(df, 'close')
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
    min_data_needed = k_period + d_period + slowing
    
    if len(df) < min_data_needed:
        # æ•°æ®ä¸è¶³æ—¶è¿”å›ç©ºå€¼
        result['stoch_k'] = np.nan
        result['stoch_d'] = np.nan
        result['stoch_divergence'] = np.nan
        result['stoch_signal'] = np.nan
        return result
    
    try:
        # è®¡ç®—æœ€é«˜ä»·å’Œæœ€ä½ä»·çš„æ»šåŠ¨çª—å£
        highest_high = roll_closed(df, 'high', k_period, 'max')
        lowest_low = roll_closed(df, 'low', k_period, 'min')
        
        # é¿å…0å€¼
        range_val = highest_high - lowest_low
        range_val = range_val.replace(0, np.nan)
        
        # è®¡ç®—%K (æœªå¹³æ»‘)
        k_raw = 100 * (close - lowest_low) / range_val
        
        # å¯¹%Kè¿›è¡Œå¹³æ»‘
        k_smoothed = k_raw.rolling(window=slowing, min_periods=max(1, slowing//2)).mean()
        
        # è®¡ç®—%D (å¯¹%Kçš„ç§»åŠ¨å¹³å‡)
        d_smoothed = k_smoothed.rolling(window=d_period, min_periods=max(1, d_period//2)).mean()
        
        # å­˜å‚¨ç»“æœ
        result['stoch_k'] = k_smoothed
        result['stoch_d'] = d_smoothed
        
        # è®¡ç®—éšæœºéœ‡è¡å™¨èƒŒç¦»
        # ä»·æ ¼åˆ›æ–°é«˜ä½†éšæœºéœ‡è¡å™¨æœªåˆ›æ–°é«˜ = çœ‹è·ŒèƒŒç¦»
        price_high = roll_closed(df, 'high', 5, 'max')
        stoch_high = roll_closed(result, 'stoch_k', 5, 'max')
        result['stoch_divergence'] = np.where(
            (price_high > price_high.shift(5)) & (stoch_high < stoch_high.shift(5)),
            -1,  # çœ‹è·ŒèƒŒç¦»
            np.where(
                (roll_closed(df, 'low', 5, 'min') < roll_closed(df, 'low', 5, 'min').shift(5)) & 
                (roll_closed(result, 'stoch_k', 5, 'min') > roll_closed(result, 'stoch_k', 5, 'min').shift(5)),
                1,   # çœ‹æ¶¨èƒŒç¦»
                0    # æ— èƒŒç¦»
            )
        )
        
        # äº¤æ˜“ä¿¡å·
        result['stoch_signal'] = np.where(
            (k_smoothed > 80) & (d_smoothed > 80), -1,  # è¶…ä¹°
            np.where(
                (k_smoothed < 20) & (d_smoothed < 20), 1,  # è¶…å–
                np.where(
                    k_smoothed > d_smoothed, 0.5,  # %Kä¸Šç©¿%D
                    -0.5  # %Kä¸‹ç©¿%D
                )
            )
        )
        
    except Exception as e:
        print(f"âš ï¸ éšæœºéœ‡è¡å™¨è®¡ç®—é”™è¯¯: {e}")
        result['stoch_k'] = np.nan
        result['stoch_d'] = np.nan
        result['stoch_divergence'] = np.nan
        result['stoch_signal'] = np.nan
    
    return result


def test_enhanced_factors():
    """æµ‹è¯•å¢å¼ºå› å­"""
    print(f"\nğŸ§ª æµ‹è¯•å¢å¼ºå› å­")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    dates = pd.date_range('2024-01-01', periods=200, freq='1H')
    np.random.seed(42)
    
    test_data = pd.DataFrame({
        'open': 100 + np.cumsum(np.random.randn(200) * 0.1),
        'high': 0, 'low': 0, 'close': 0,
        'volume': np.random.randint(1000, 10000, 200)
    }, index=dates)
    
    test_data['close'] = test_data['open'] + np.random.randn(200) * 0.1
    test_data['high'] = np.maximum(test_data['open'], test_data['close']) + np.abs(np.random.randn(200) * 0.05)
    test_data['low'] = np.minimum(test_data['open'], test_data['close']) - np.abs(np.random.randn(200) * 0.05)
    
    # æµ‹è¯•éšæœºéœ‡è¡å™¨
    print(f"\nğŸ“Š æµ‹è¯•éšæœºéœ‡è¡å™¨å› å­:")
    stoch_result = calculate_stochastic_factors(test_data)
    
    enhanced_factors = [col for col in stoch_result.columns if col.startswith('stoch_')]
    print(f"âœ… éšæœºéœ‡è¡å™¨å› å­: {len(enhanced_factors)}ä¸ª")
    for factor in enhanced_factors:
        non_null_count = stoch_result[factor].notna().sum()
        print(f"    ğŸ“ˆ {factor}: {non_null_count}ä¸ªæœ‰æ•ˆå€¼")
    
    # æ£€æŸ¥å€¼èŒƒå›´
    for factor in ['stoch_k', 'stoch_d']:
        if factor in stoch_result.columns:
            valid_data = stoch_result[factor].dropna()
            if len(valid_data) > 0:
                print(f"    ğŸ“Š {factor} èŒƒå›´: [{valid_data.min():.2f}, {valid_data.max():.2f}]")
    
    # æµ‹è¯•Ichimoku Cloud
    print(f"\nğŸ“Š æµ‹è¯•Ichimoku Cloudå› å­:")
    ichimoku_result = calculate_ichimoku_factors(test_data)
    
    ichimoku_factors = [col for col in ichimoku_result.columns if col.startswith(('tenkan_', 'kijun_', 'senkou_', 'chikou_', 'cloud_', 'ichimoku_'))]
    print(f"âœ… Ichimoku Cloudå› å­: {len(ichimoku_factors)}ä¸ª")
    for factor in ichimoku_factors:
        non_null_count = ichimoku_result[factor].notna().sum()
        print(f"    ğŸ“ˆ {factor}: {non_null_count}ä¸ªæœ‰æ•ˆå€¼")
    
    # æ£€æŸ¥ä¿¡å·åˆ†å¸ƒ
    if 'ichimoku_signal' in ichimoku_result.columns:
        signal_counts = ichimoku_result['ichimoku_signal'].value_counts()
        print(f"    ğŸ“Š ä¿¡å·åˆ†å¸ƒ: {dict(signal_counts)}")
    
    # æµ‹è¯•æŠ›ç‰©çº¿SAR
    print(f"\nğŸ“Š æµ‹è¯•æŠ›ç‰©çº¿SARå› å­:")
    sar_result = calculate_parabolic_sar(test_data)
    
    sar_factors = [col for col in sar_result.columns if col.startswith(('parabolic_', 'sar_'))]
    print(f"âœ… æŠ›ç‰©çº¿SARå› å­: {len(sar_factors)}ä¸ª")
    for factor in sar_factors:
        non_null_count = sar_result[factor].notna().sum()
        print(f"    ğŸ“ˆ {factor}: {non_null_count}ä¸ªæœ‰æ•ˆå€¼")
    
    # æ£€æŸ¥è¶‹åŠ¿åˆ†å¸ƒ
    if 'sar_trend' in sar_result.columns:
        trend_counts = sar_result['sar_trend'].value_counts()
        print(f"    ğŸ“Š è¶‹åŠ¿åˆ†å¸ƒ: {dict(trend_counts)}")
    
    # æµ‹è¯•åæ•´å…³ç³»å› å­
    print(f"\nğŸ“Š æµ‹è¯•åæ•´å…³ç³»å› å­:")
    coint_result = calculate_cointegration_factors(test_data)
    
    coint_factors = [col for col in coint_result.columns if col.startswith(('cointegration_', 'mean_reversion_', 'half_life_'))]
    print(f"âœ… åæ•´å…³ç³»å› å­: {len(coint_factors)}ä¸ª")
    for factor in coint_factors:
        non_null_count = coint_result[factor].notna().sum()
        print(f"    ğŸ“ˆ {factor}: {non_null_count}ä¸ªæœ‰æ•ˆå€¼")
    
    # æµ‹è¯•é…å¯¹äº¤æ˜“å› å­
    print(f"\nğŸ“Š æµ‹è¯•é…å¯¹äº¤æ˜“å› å­:")
    pair_result = calculate_pair_trading_factors(test_data)
    
    pair_factors = [col for col in pair_result.columns if col.startswith(('price_', 'pair_', 'correlation_'))]
    print(f"âœ… é…å¯¹äº¤æ˜“å› å­: {len(pair_factors)}ä¸ª")
    for factor in pair_factors:
        non_null_count = pair_result[factor].notna().sum()
        print(f"    ğŸ“ˆ {factor}: {non_null_count}ä¸ªæœ‰æ•ˆå€¼")
    
    # æµ‹è¯•å¼‚å¸¸æ£€æµ‹å› å­
    print(f"\nğŸ“Š æµ‹è¯•å¼‚å¸¸æ£€æµ‹å› å­:")
    anomaly_result = calculate_anomaly_factors(test_data)
    
    anomaly_factors = [col for col in anomaly_result.columns if col.endswith(('_anomaly', '_signal'))]
    print(f"âœ… å¼‚å¸¸æ£€æµ‹å› å­: {len(anomaly_factors)}ä¸ª")
    for factor in anomaly_factors:
        non_null_count = anomaly_result[factor].notna().sum()
        print(f"    ğŸ“ˆ {factor}: {non_null_count}ä¸ªæœ‰æ•ˆå€¼")
    
    # æ£€æŸ¥å¼‚å¸¸ä¿¡å·åˆ†å¸ƒ
    if 'anomaly_signal' in anomaly_result.columns:
        signal_counts = anomaly_result['anomaly_signal'].value_counts()
        print(f"    ğŸ“Š å¼‚å¸¸ä¿¡å·åˆ†å¸ƒ: {dict(signal_counts)}")
    
    print(f"\nğŸ‰ å¢å¼ºå› å­æµ‹è¯•å®Œæˆï¼")
    return pd.concat([stoch_result, ichimoku_result, sar_result, coint_result, pair_result, anomaly_result], axis=1)


if __name__ == "__main__":
    # è¿è¡ŒåŸæœ‰æµ‹è¯•
    test_advanced_factors()
    
    # è¿è¡Œè·¨å‘¨æœŸå› å­è‡ªæ£€
    test_cross_cycle()
    
    # è¿è¡Œå¢å¼ºå› å­æµ‹è¯•
    test_enhanced_factors()
