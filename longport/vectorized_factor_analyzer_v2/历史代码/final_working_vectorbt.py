#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆå·¥ä½œç‰ˆVectorBTç³»ç»Ÿ
ç®€åŒ–é€»è¾‘ï¼Œç¡®ä¿ICè®¡ç®—æ­£å¸¸å·¥ä½œ
åŸºäºå·²éªŒè¯çš„å•è‚¡ç¥¨è°ƒè¯•ç»“æœ
"""

import os
import sys
import time
import json
import logging
import warnings
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import psutil

# VectorBTå’ŒæŠ€æœ¯æŒ‡æ ‡
try:
    import vectorbt as vbt
    import talib
    print("âœ… VectorBTå’ŒTA-LibåŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from factors.factor_pool import AdvancedFactorPool
from utils.dtype_fixer import CategoricalDtypeFixer
from strategies.cta_eval_v3 import CTAEvaluatorV3

# ç»Ÿè®¡å­¦åº“
try:
    from statsmodels.stats.diagnostic import acorr_ljungbox
    from statsmodels.tsa.stattools import acf
    from statsmodels.regression.linear_model import OLS
    from statsmodels.stats.sandwich_covariance import cov_hac
    print("âœ… StatsmodelsåŠ è½½æˆåŠŸ")
except ImportError:
    print("âš ï¸ Statsmodelsæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆIC_IRè®¡ç®—")

warnings.filterwarnings('ignore')

class FinalWorkingVectorBT:
    """
    æœ€ç»ˆå·¥ä½œç‰ˆVectorBTç³»ç»Ÿ
    åŸºäºè°ƒè¯•ç»“æœï¼Œç¡®ä¿ICè®¡ç®—æ­£å¸¸
    """
    
    def __init__(self, data_dir: str = "./data", capital: float = 300000):
        self.data_dir = data_dir
        self.capital = capital
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ğŸ”¥ æœ€ç»ˆå·¥ä½œé…ç½® - åŸºäºå®é™…æµ‹è¯•ç»“æœ
        self.working_config = {
            'test_timeframes': ['1m', '2m', '3m', '5m', '10m', '15m', '30m', '1h', '4h', '1d'],  # ğŸ”¥ å…¨æ—¶é—´æ¡†æ¶å†’çƒŸæµ‹è¯•
            'max_symbols': 54,  # ğŸ”¥ å…¨è‚¡ç¥¨åˆ†å±‚æ¢æŸ¥: 54åªè‚¡ç¥¨
            'evaluation_mode': 'cta',  # ğŸ”¥ æ–°å¢: CTAå›æµ‹æ¨¡å¼ vs 'ic'æ¨¡å¼
            
            # ğŸ”¥ åŸºäºè°ƒè¯•ç»“æœçš„å®½æ¾é˜ˆå€¼
            'min_ic_threshold': 0.005,   # åŸºäºå•è‚¡ç¥¨æµ‹è¯•ï¼ŒIC=0.02-0.11æ˜¯åˆç†çš„
            'min_ir_threshold': 0.01,    # éå¸¸å®½æ¾çš„IRè¦æ±‚
            'min_sample_size': 10,       # æœ€å°æ ·æœ¬é‡
            'min_supporting_stocks': 2,  # è‡³å°‘2åªè‚¡ç¥¨æ”¯æŒ
            
            # å¯ç”¨åŠŸèƒ½
            'full_factor_pool': True,
            'debug_mode': True
        }
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.factor_pool = AdvancedFactorPool()
        self.categorical_fixer = CategoricalDtypeFixer()
        
        # è·å–æµ‹è¯•è‚¡ç¥¨
        self.test_symbols = self._get_test_symbols()
        self.logger.info(f"âœ… æµ‹è¯•è‚¡ç¥¨: {self.test_symbols}")
        
    def _setup_logging(self):
        """è®¾ç½®ç®€å•æ—¥å¿—"""
        log_dir = f"logs/final_working_{self.timestamp}"
        os.makedirs(log_dir, exist_ok=True)
        
        self.logger = logging.getLogger('FinalWorking')
        self.logger.setLevel(logging.DEBUG)
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(
            f"{log_dir}/final_working.log", 
            encoding='utf-8'
        )
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        
        # æ ¼å¼å™¨
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def _get_test_symbols(self) -> List[str]:
        """è·å–æµ‹è¯•è‚¡ç¥¨"""
        # ä¼˜å…ˆé€‰æ‹©æœ‰ä»£è¡¨æ€§çš„è‚¡ç¥¨
        priority_symbols = ['0700.HK', '0005.HK', '0388.HK', '1211.HK', '0981.HK']
        
        available_symbols = []
        
        # æ£€æŸ¥1dæ•°æ®ç¡®ä¿è‚¡ç¥¨å¯ç”¨
        d1_dir = os.path.join(self.data_dir, '1d')
        if os.path.exists(d1_dir):
            for symbol in priority_symbols:
                file_path = os.path.join(d1_dir, f'{symbol}.parquet')
                if os.path.exists(file_path):
                    available_symbols.append(symbol)
        
        # è¡¥å……åˆ°æŒ‡å®šæ•°é‡
        if len(available_symbols) < self.working_config['max_symbols']:
            for file in os.listdir(d1_dir):
                if file.endswith('.parquet'):
                    symbol = file.replace('.parquet', '')
                    if symbol not in available_symbols:
                        available_symbols.append(symbol)
                        if len(available_symbols) >= self.working_config['max_symbols']:
                            break
        
        return available_symbols[:self.working_config['max_symbols']]
    
    def _load_symbol_data(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """åŠ è½½å•ä¸ªè‚¡ç¥¨æ•°æ®"""
        try:
            file_path = os.path.join(self.data_dir, timeframe, f'{symbol}.parquet')
            if not os.path.exists(file_path):
                return pd.DataFrame()
            
            df = pd.read_parquet(file_path)
            
            # æ ‡å‡†åŒ–ç´¢å¼•å’Œåˆ—
            if not isinstance(df.index, pd.DatetimeIndex):
                df.index = pd.to_datetime(df.index)
            
            # åˆ—åæ ‡å‡†åŒ–
            column_mapping = {
                'Close': 'close', 'Open': 'open', 'High': 'high', 
                'Low': 'low', 'Volume': 'volume', 'Turnover': 'turnover'
            }
            df = df.rename(columns=column_mapping)
            
            # ç¡®ä¿åŸºç¡€åˆ—å­˜åœ¨
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            available_cols = [col for col in required_cols if col in df.columns]
            
            if len(available_cols) >= 4:
                return df[available_cols].dropna()
            else:
                return pd.DataFrame()
                
        except Exception as e:
            self.logger.debug(f"åŠ è½½{symbol}-{timeframe}å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _safe_divide(self, numerator, denominator, fill_value=0):
        """å®‰å…¨é™¤æ³•ï¼Œé¿å…é™¤é›¶å’ŒNaN"""
        # å¤„ç†Serieså’Œnumpyæ•°ç»„
        if hasattr(numerator, 'values'):
            num_vals = numerator.values
        else:
            num_vals = np.asarray(numerator)
            
        if hasattr(denominator, 'values'):
            den_vals = denominator.values
        else:
            den_vals = np.asarray(denominator)
        
        # é¿å…é™¤é›¶
        den_vals = np.where(np.abs(den_vals) < 1e-10, 1e-10, den_vals)
        
        result = num_vals / den_vals
        result = np.where(np.isinf(result), fill_value, result)
        result = np.where(np.isnan(result), fill_value, result)
        
        # è¿”å›pandas Serieså¦‚æœè¾“å…¥æ˜¯Series
        if hasattr(numerator, 'index'):
            return pd.Series(result, index=numerator.index)
        else:
            return result
    
    def _align_multi_stock_data(self, symbol_data: Dict[str, pd.DataFrame], symbol_factors: Dict[str, pd.DataFrame]) -> Tuple[Dict, Dict, pd.DatetimeIndex]:
        """ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šå¤šè‚¡ç¥¨æ•°æ®å¯¹é½åˆ°å…±åŒæ—¶é—´èŒƒå›´"""
        if not symbol_data:
            return {}, {}, pd.DatetimeIndex([])
        
        # 1. æ‰¾åˆ°æ‰€æœ‰è‚¡ç¥¨çš„å…±åŒæ—¶é—´èŒƒå›´
        all_indices = [df.index for df in symbol_data.values()]
        common_index = all_indices[0]
        for idx in all_indices[1:]:
            common_index = common_index.intersection(idx)
        
        if len(common_index) == 0:
            self.logger.warning("âŒ æ— å…±åŒæ—¶é—´èŒƒå›´ï¼Œè·³è¿‡å¯¹é½")
            return {}, {}, pd.DatetimeIndex([])
        
        # 2. é˜²å¾¡å¼æ—¥å¿—ï¼šæ‰“å°å…±åŒæ ·æœ¬è¦†ç›–  
        start_dt = pd.to_datetime(common_index[0])
        end_dt = pd.to_datetime(common_index[-1])
        self.logger.debug("ğŸ” å…±åŒæ ·æœ¬åŒºé—´: %s è‡³ %s (å…± %d æ ¹Kçº¿)", 
                         start_dt.strftime('%Y-%m-%d %H:%M:%S'), 
                         end_dt.strftime('%Y-%m-%d %H:%M:%S'), 
                         len(common_index))
        
        # 3. æˆªæ–­æ‰€æœ‰æ•°æ®åˆ°å…±åŒèŒƒå›´
        aligned_data = {}
        aligned_factors = {}
        
        for symbol in symbol_data.keys():
            if symbol in symbol_factors:
                # å¯¹é½ä»·æ ¼æ•°æ®
                aligned_data[symbol] = symbol_data[symbol].reindex(common_index)
                # å¯¹é½å› å­æ•°æ®
                aligned_factors[symbol] = symbol_factors[symbol].reindex(common_index)
                
                self.logger.debug(f"    âœ… {symbol}: å¯¹é½åˆ°{len(common_index)}æ¡æ•°æ®")
        
        return aligned_data, aligned_factors, common_index
    
    def _filter_low_sample_factors(self, symbol_factors: Dict[str, pd.DataFrame], common_index: pd.DatetimeIndex) -> Dict[str, pd.DataFrame]:
        """ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šè¿‡æ»¤ä½æ ·æœ¬å› å­"""
        if len(common_index) == 0:
            return symbol_factors
        
        min_sample_threshold = int(len(common_index) * 0.8)  # 80%é˜ˆå€¼
        invalid_factors = {}
        filtered_factors = {}
        
        for symbol, factors_df in symbol_factors.items():
            valid_factors = {}
            
            for factor_name in factors_df.columns:
                if factor_name in ['open', 'high', 'low', 'close', 'volume', 'turnover', 'timestamp']:
                    continue
                
                valid_count = factors_df[factor_name].notna().sum()
                
                if valid_count >= min_sample_threshold:
                    valid_factors[factor_name] = factors_df[factor_name]
                else:
                    # è®°å½•æ— æ•ˆå› å­
                    if factor_name not in invalid_factors:
                        invalid_factors[factor_name] = []
                    invalid_factors[factor_name].append(symbol)
                    self.logger.debug(f"    âš ï¸ {symbol}-{factor_name}: æ ·æœ¬ä¸è¶³({valid_count}/{len(common_index)})")
            
            if valid_factors:
                # ä¿ç•™ä»·æ ¼åˆ—
                price_cols = {col: factors_df[col] for col in ['open', 'high', 'low', 'close', 'volume'] if col in factors_df.columns}
                filtered_factors[symbol] = pd.DataFrame({**price_cols, **valid_factors}, index=factors_df.index)
            
        # é˜²å¾¡å¼æ—¥å¿—ï¼šæ‰“å°æ— æ•ˆå› å­æ¸…å•
        if invalid_factors:
            self.logger.warning(f"âš ï¸ ä»¥ä¸‹å› å­å› æ ·æœ¬ä¸è¶³è¢«å‰”é™¤: {dict(invalid_factors)}")
        
        return filtered_factors
    
    def _calculate_newey_west_ic_ir(self, ic_series: pd.Series, lags: int = 3) -> float:
        """ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šNewey-Westè‡ªç›¸å…³ä¿®æ­£çš„IC_IR"""
        try:
            if len(ic_series) < 10:  # æ ·æœ¬å¤ªå°‘
                return 0.0
            
            # ç§»é™¤NaN
            clean_ic = ic_series.dropna()
            if len(clean_ic) < 10:
                return 0.0
            
            mean_ic = clean_ic.mean()
            
            # ä½¿ç”¨statsmodelsè®¡ç®—Newey-Westæ ‡å‡†è¯¯
            try:
                from statsmodels.regression.linear_model import OLS
                from statsmodels.stats.sandwich_covariance import cov_hac
                
                # æ„é€ ç®€å•çº¿æ€§æ¨¡å‹ (ICå¯¹å¸¸æ•°é¡¹å›å½’)
                X = np.ones((len(clean_ic), 1))
                y = clean_ic.values
                
                model = OLS(y, X).fit()
                nw_cov = cov_hac(model, nlags=lags)
                nw_std = np.sqrt(nw_cov[0, 0])
                
                ic_ir_adj = mean_ic / nw_std if nw_std > 0 else 0.0
                
                self.logger.debug(f"      Newey-West IC_IR: {ic_ir_adj:.4f} (åŸå§‹: {mean_ic/clean_ic.std():.4f})")
                return ic_ir_adj
            except ImportError:
                # é™çº§åˆ°æ™®é€šIC_IR
                return mean_ic / clean_ic.std() if clean_ic.std() > 0 else 0.0
                
        except Exception as e:
            self.logger.debug(f"      Newey-Westè®¡ç®—å¤±è´¥: {e}, ä½¿ç”¨åŸå§‹IC_IR")
            return ic_series.mean() / ic_series.std() if ic_series.std() > 0 else 0.0
    
    def _calculate_fixed_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—ä¿®å¤ç‰ˆå› å­ï¼Œè§£å†³NaNé—®é¢˜"""
        print(f"ğŸ”§ è®¡ç®—ä¿®å¤ç‰ˆå› å­: {df.shape}")
        
        # ç¡®ä¿æ•°å€¼ç±»å‹
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        high, low, close, volume = df.high, df.low, df.close, df.volume
        
        # ä½¿ç”¨åŸæœ‰çš„å› å­æ± ï¼Œä½†æ·»åŠ å…³é”®ä¿®å¤
        df = self.factor_pool.calculate_all_factors(df)
        
        # ä¿®å¤keltner_position
        try:
            import talib as ta  # ç¡®ä¿æœ¬åœ°å¯¼å…¥
            atr = ta.ATR(high.values, low.values, close.values, timeperiod=14)
            atr_series = pd.Series(atr, index=df.index)
            keltner_ma = close.rolling(20, min_periods=1).mean()
            keltner_atr = atr_series.rolling(20, min_periods=1).mean()
            keltner_upper = keltner_ma + 2 * keltner_atr
            keltner_lower = keltner_ma - 2 * keltner_atr
            
            # å®‰å…¨è®¡ç®—ä½ç½®
            band_width = keltner_upper - keltner_lower
            position_numerator = close - keltner_lower
            
            keltner_position = self._safe_divide(position_numerator, band_width, fill_value=0.5)
            keltner_position = np.clip(keltner_position, -1, 2)
            df['keltner_position'] = keltner_position
            
            valid_count = keltner_position.notna().sum()
            print(f"  âœ… Keltner Positionä¿®å¤: {valid_count}/{len(keltner_position)} æœ‰æ•ˆå€¼")
            
        except Exception as e:
            print(f"  âŒ Keltnerä¿®å¤å¤±è´¥: {e}")
            df['keltner_position'] = 0.5
        
        # ä¿®å¤VWAPåç¦»åº¦
        try:
            typical_price = (high + low + close) / 3
            window = min(20, len(df) // 2) if len(df) >= 40 else max(1, len(df) // 4)
            
            vwap = (typical_price * volume).rolling(window, min_periods=1).sum() / volume.rolling(window, min_periods=1).sum()
            vwap_deviation = self._safe_divide(close - vwap, vwap, fill_value=0.0)
            df['vwap_deviation'] = vwap_deviation
            
            valid_count = vwap_deviation.notna().sum()
            print(f"  âœ… VWAPåç¦»åº¦ä¿®å¤: {valid_count}/{len(vwap_deviation)} æœ‰æ•ˆå€¼")
            
        except Exception as e:
            print(f"  âŒ VWAPä¿®å¤å¤±è´¥: {e}")
            df['vwap_deviation'] = 0.0
        
        # ä¿®å¤åŠ¨æ€ranking
        try:
            # åŠ¨æ€ç¡®å®šçª—å£å¤§å°
            ranking_window = 288 * 2  # âœ…è¡¥ä¸5: 2å¤©çª—å£(5mæ•°æ®)
            print(f"  ğŸ”§ åŠ¨æ€rankingçª—å£: {ranking_window}")
            
            factors_to_rank = ['rsi_14', 'macd_enhanced', 'atrp', 'vwap_deviation']
            for factor in factors_to_rank:
                if factor in df.columns and not df[factor].isna().all():
                    try:
                        min_periods = max(1, ranking_window // 4)
                        factor_rank = df[factor].rolling(ranking_window, min_periods=min_periods).rank(pct=True)
                        df[f'{factor}_rank'] = factor_rank
                        valid_count = factor_rank.notna().sum()
                        print(f"    âœ… {factor}_rank: {valid_count}/{len(factor_rank)} æœ‰æ•ˆå€¼")
                    except Exception as rank_error:
                        print(f"    âŒ {factor}_rankå¤±è´¥: {rank_error}")
                        df[f'{factor}_rank'] = 0.5
                else:
                    df[f'{factor}_rank'] = 0.5
                    
        except Exception as e:
            print(f"  âŒ Rankingä¿®å¤å¤±è´¥: {e}")
        
        return df
    
    def _calculate_symbol_factors(self, symbol: str, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—å•ä¸ªè‚¡ç¥¨çš„å› å­ - NaNé—®é¢˜ä¿®å¤ç‰ˆ"""
        if data.empty:
            return pd.DataFrame()
        
        try:
            # è®¡ç®—å› å­ (ä½¿ç”¨ä¿®å¤ç‰ˆæ–¹æ³•)
            factors_df = self._calculate_fixed_factors(data.copy())
            
            # åªä¿ç•™å› å­åˆ—
            base_cols = ['open', 'high', 'low', 'close', 'volume', 'turnover']
            factor_cols = [col for col in factors_df.columns if col not in base_cols]
            
            if factor_cols:
                factor_only_df = factors_df[factor_cols].copy()
                
                # Categoricalä¿®å¤
                factor_only_df, _ = self.categorical_fixer.comprehensive_fix(factor_only_df)
                
                return factor_only_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            self.logger.debug(f"{symbol} å› å­è®¡ç®—å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _calculate_symbol_ic(self, symbol: str, factors_df: pd.DataFrame, price_df: pd.DataFrame) -> Dict[str, Any]:
        """ğŸ”¥ è®¡ç®—å•ä¸ªè‚¡ç¥¨çš„IC - åŸºäºè°ƒè¯•éªŒè¯çš„é€»è¾‘"""
        self.logger.debug(f"    ğŸ” å¼€å§‹è®¡ç®—{symbol}çš„IC")
        self.logger.debug(f"      å› å­æ•°æ®: {factors_df.shape}, ä»·æ ¼æ•°æ®: {price_df.shape}")
        
        if factors_df.empty or price_df.empty:
            self.logger.debug(f"      âŒ {symbol}: æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ICè®¡ç®—")
            return {}
        
        # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
        returns = price_df['close'].pct_change(1).shift(-1)
        self.logger.debug(f"      æ”¶ç›Šç‡è®¡ç®—: éç©ºæ•°é‡={returns.notna().sum()}, æ€»æ•°={len(returns)}")
        
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
        min_len = min(len(factors_df), len(returns))
        if min_len < 20:  # è‡³å°‘20ä¸ªæ ·æœ¬
            return {}
        
        # æˆªå–ç›¸åŒé•¿åº¦
        aligned_factors = factors_df.iloc[:min_len]
        aligned_returns = returns.iloc[:min_len]
        
        # ç§»é™¤ç¼ºå¤±å€¼
        valid_returns = aligned_returns.dropna()
        if len(valid_returns) < self.working_config['min_sample_size']:
            return {}
        
        symbol_ic_results = {}
        
        # ğŸ”¥ ä¸ºæ¯ä¸ªå› å­è®¡ç®—IC
        self.logger.debug(f"      å¾…è®¡ç®—å› å­: {len(aligned_factors.columns)}ä¸ª")
        for i, factor_name in enumerate(aligned_factors.columns):
            try:
                factor_values = aligned_factors[factor_name].iloc[:len(valid_returns)]
                
                # ç¡®ä¿factorå’Œreturnå¯¹é½
                valid_idx = factor_values.notna() & valid_returns.notna()
                
                if valid_idx.sum() < self.working_config['min_sample_size']:
                    self.logger.debug(f"        âš ï¸ {factor_name}: æœ‰æ•ˆæ ·æœ¬ä¸è¶³({valid_idx.sum()})")
                    continue
                
                clean_factor = factor_values[valid_idx]
                clean_returns = valid_returns[valid_idx]
                
                # è®¡ç®—IC (åŸºäºè°ƒè¯•éªŒè¯çš„æ–¹æ³•)
                ic_spearman = clean_factor.corr(clean_returns, method='spearman')
                ic_pearson = clean_factor.corr(clean_returns, method='pearson')
                self.logger.debug(f"        ğŸ“Š {factor_name}: Spearman={ic_spearman:.4f}, Pearson={ic_pearson:.4f}, æ ·æœ¬={len(clean_factor)}")
                
                # é€‰æ‹©æœ‰æ•ˆçš„IC
                ic = ic_spearman if not pd.isna(ic_spearman) else ic_pearson
                
                if not pd.isna(ic):
                    symbol_ic_results[factor_name] = {
                        'ic': float(ic),
                        'sample_size': len(clean_factor),
                        'method': 'spearman' if not pd.isna(ic_spearman) else 'pearson'
                    }
                    
                    if self.working_config['debug_mode']:
                        self.logger.debug(f"  {symbol}-{factor_name}: IC={ic:.4f}, æ ·æœ¬={len(clean_factor)}")
                
            except Exception as e:
                self.logger.debug(f"  {symbol}-{factor_name} ICè®¡ç®—å¤±è´¥: {e}")
                continue
        
        return symbol_ic_results
    
    def _aggregate_multi_stock_ic(self, all_symbol_ics: Dict[str, Dict]) -> Dict[str, Any]:
        """ğŸ”¥ æ±‡æ€»å¤šè‚¡ç¥¨ICç»“æœ"""
        self.logger.debug(f"  ğŸ¯ å¼€å§‹èšåˆ{len(all_symbol_ics)}åªè‚¡ç¥¨çš„ICç»“æœ")
        factor_ic_summary = {}
        
        # æŒ‰å› å­åæ±‡æ€»
        all_factor_names = set()
        for symbol_ics in all_symbol_ics.values():
            all_factor_names.update(symbol_ics.keys())
        
        self.logger.debug(f"    å‘ç°{len(all_factor_names)}ä¸ªä¸åŒå› å­")
        
        for factor_name in all_factor_names:
            ic_values = []
            sample_sizes = []
            supporting_symbols = []
            
            # æ”¶é›†è¯¥å› å­åœ¨å„è‚¡ç¥¨çš„IC
            for symbol, symbol_ics in all_symbol_ics.items():
                if factor_name in symbol_ics:
                    ic_data = symbol_ics[factor_name]
                    ic_values.append(ic_data['ic'])
                    sample_sizes.append(ic_data['sample_size'])
                    supporting_symbols.append(symbol)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ”¯æŒ
            if len(ic_values) < self.working_config['min_supporting_stocks']:
                continue
            
            # æ±‡æ€»ç»Ÿè®¡
            mean_ic = np.mean(ic_values)
            std_ic = np.std(ic_values) if len(ic_values) > 1 else 0
            ic_ir = mean_ic / std_ic if std_ic > 1e-6 else 0
            
            # ğŸ”¥ æ–°å¢ï¼šNewey-Westä¿®æ­£IC_IR
            ic_series = pd.Series(ic_values)
            ic_ir_adj = self._calculate_newey_west_ic_ir(ic_series)
            self.logger.debug(f"    ğŸ“Š {factor_name}: åŸå§‹IC_IR={ic_ir:.4f}, ä¿®æ­£IC_IR={ic_ir_adj:.4f}")
            
            positive_ic_count = sum(1 for ic in ic_values if ic > 0)
            positive_ic_ratio = positive_ic_count / len(ic_values)
            
            total_samples = sum(sample_sizes)
            
            # ğŸ”¥ å‡çº§ï¼šä½¿ç”¨ä¿®æ­£åçš„IC_IRè¿›è¡Œç­›é€‰ï¼Œæé«˜è´¨é‡æ ‡å‡†
            if (abs(mean_ic) >= self.working_config['min_ic_threshold'] and 
                abs(ic_ir_adj) >= 0.05):  # ä¿®æ­£åIC_IRæœ€å°é˜ˆå€¼
                
                factor_ic_summary[factor_name] = {
                    'ic': float(mean_ic),
                    'ic_ir': float(ic_ir),
                    'ic_ir_adj': float(ic_ir_adj),  # æ–°å¢ä¿®æ­£IC_IR
                    'ic_std': float(std_ic),
                    'positive_ic_ratio': float(positive_ic_ratio),
                    'total_sample_size': int(total_samples),
                    'supporting_stocks': len(supporting_symbols),
                    'stock_list': supporting_symbols,
                    'ic_values': ic_values
                }
            else:
                self.logger.debug(f"    âŒ {factor_name}: æœªé€šè¿‡è´¨é‡ç­›é€‰ (IC={mean_ic:.4f}, IC_IR_adj={ic_ir_adj:.4f})")
        
        return factor_ic_summary
    
    def _analyze_timeframe(self, timeframe: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ—¶é—´æ¡†æ¶"""
        self.logger.info(f"ğŸ” åˆ†æ{timeframe}æ—¶é—´æ¡†æ¶...")
        self.logger.debug(f"  ç›®æ ‡è‚¡ç¥¨æ•°: {len(self.test_symbols)}")
        
        # 1. åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
        symbol_data = {}
        symbol_factors = {}
        
        for symbol in self.test_symbols:
            data = self._load_symbol_data(symbol, timeframe)
            if not data.empty:
                symbol_data[symbol] = data
                self.logger.debug(f"    âœ… {symbol}: åŠ è½½{len(data)}æ¡æ•°æ®")
                
                # è®¡ç®—å› å­
                factors = self._calculate_symbol_factors(symbol, data)
                if not factors.empty:
                    symbol_factors[symbol] = factors
                    self.logger.debug(f"    ğŸ“Š {symbol}: è®¡ç®—{len(factors.columns)}ä¸ªå› å­")
                else:
                    self.logger.debug(f"    âŒ {symbol}: å› å­è®¡ç®—å¤±è´¥")
            else:
                self.logger.debug(f"    âŒ {symbol}: æ•°æ®åŠ è½½å¤±è´¥")
        
        if not symbol_factors:
            self.logger.warning(f"âŒ {timeframe} æ— æœ‰æ•ˆå› å­æ•°æ®")
            return {}
        
        self.logger.info(f"  âœ… {len(symbol_factors)}åªè‚¡ç¥¨æœ‰æ•ˆï¼Œå¹³å‡{np.mean([len(f.columns) for f in symbol_factors.values()]):.0f}ä¸ªå› å­")
        
        # ğŸ”¥ æ–°å¢ï¼šæ•°æ®å¯¹é½å’Œè´¨é‡è¿‡æ»¤
        aligned_data, aligned_factors, common_index = self._align_multi_stock_data(symbol_data, symbol_factors)
        if not aligned_data:
            self.logger.warning(f"âŒ {timeframe} æ•°æ®å¯¹é½å¤±è´¥")
            return {}
        
        # ğŸ”¥ æ–°å¢ï¼šè¿‡æ»¤ä½æ ·æœ¬å› å­
        filtered_factors = self._filter_low_sample_factors(aligned_factors, common_index)
        if not filtered_factors:
            self.logger.warning(f"âŒ {timeframe} å› å­è¿‡æ»¤åæ— å‰©ä½™")
            return {}
        
        self.logger.info(f"  âœ… å¯¹é½å{len(filtered_factors)}åªè‚¡ç¥¨ï¼Œå…±åŒæ ·æœ¬{len(common_index)}æ¡")
        
        # 2. è®¡ç®—å„è‚¡ç¥¨IC (ä½¿ç”¨å¯¹é½åçš„æ•°æ®)
        all_symbol_ics = {}
        self.logger.debug(f"  ğŸ”¢ å¼€å§‹è®¡ç®—{len(filtered_factors)}åªè‚¡ç¥¨çš„IC")
        
        for symbol in filtered_factors.keys():
            if symbol in aligned_data:
                symbol_ic = self._calculate_symbol_ic(symbol, filtered_factors[symbol], aligned_data[symbol])
                if symbol_ic:
                    all_symbol_ics[symbol] = symbol_ic
                    self.logger.debug(f"    âœ… {symbol}: è®¡ç®—{len(symbol_ic)}ä¸ªå› å­IC")
                else:
                    self.logger.debug(f"    âŒ {symbol}: ICè®¡ç®—å¤±è´¥")
        
        if not all_symbol_ics:
            self.logger.warning(f"âŒ {timeframe} æ— æœ‰æ•ˆICæ•°æ®")
            return {}
        
        self.logger.info(f"  âœ… {len(all_symbol_ics)}åªè‚¡ç¥¨æœ‰ICæ•°æ®")
        
        # 3. æ±‡æ€»ICç»“æœ
        ic_summary = self._aggregate_multi_stock_ic(all_symbol_ics)
        
        self.logger.info(f"  âœ… {timeframe} æœ€ç»ˆæœ‰æ•ˆå› å­: {len(ic_summary)}ä¸ª")
        
        return ic_summary
    
    def _run_cta_analysis(self, timeframe: str) -> Dict[str, Any]:
        """ğŸ”¥ CTAå›æµ‹æ¨¡å¼åˆ†æå•ä¸ªæ—¶é—´æ¡†æ¶"""
        self.logger.info(f"ğŸ¯ CTAå›æµ‹åˆ†æ{timeframe}æ—¶é—´æ¡†æ¶...")
        self.logger.debug(f"  ç›®æ ‡è‚¡ç¥¨æ•°: {len(self.test_symbols)}")
        
        # 1. åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®å’Œå› å­
        symbol_data = {}
        symbol_factors = {}
        
        for symbol in self.test_symbols:
            data = self._load_symbol_data(symbol, timeframe)
            if not data.empty:
                symbol_data[symbol] = data
                self.logger.debug(f"    âœ… {symbol}: åŠ è½½{len(data)}æ¡æ•°æ®")
                
                # è®¡ç®—å› å­
                factors = self._calculate_symbol_factors(symbol, data)
                if not factors.empty:
                    symbol_factors[symbol] = factors
                    self.logger.debug(f"    ğŸ“Š {symbol}: è®¡ç®—{len(factors.columns)}ä¸ªå› å­")
        
        if not symbol_factors:
            self.logger.warning(f"âŒ {timeframe} æ— æœ‰æ•ˆå› å­æ•°æ®")
            return {}
        
        self.logger.info(f"  âœ… {len(symbol_factors)}åªè‚¡ç¥¨æœ‰æ•ˆï¼Œå¹³å‡{np.mean([len(f.columns) for f in symbol_factors.values()]):.0f}ä¸ªå› å­")
        
        # âœ…ä¿®å¤5: æ·»åŠ å†…å­˜ç›‘æ§
        try:
            import psutil
            process = psutil.Process(os.getpid())
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.logger.debug(f"    ğŸ’¾ æ•°æ®åŠ è½½åå†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")
        except:
            pass
        
        # 2. ä½¿ç”¨V3å›åˆ°å·¥ä½œç‰ˆæœ¬ + 5è¡Œä»£ç "æ´»è¿‡æ¥"è¡¥ä¸
        from strategies.cta_eval_v3 import CTAEvaluatorV3
        cta_evaluator = CTAEvaluatorV3(
            look_ahead=6,                 # âœ…è¡¥ä¸1: æ‹¿6æ ¹Kçº¿(30åˆ†é’Ÿ)è®©åˆ©æ¶¦å¥”è·‘
            entry_percentile=0.90,        # âœ…è¡¥ä¸2: æç«¯ä¿¡å· 90%åˆ†ä½
            exit_percentile=0.10,         # âœ…è¡¥ä¸2: æç«¯ä¿¡å· 10%åˆ†ä½
            sl_stop=0.02,
            tp_stop=0.03,
            direction='both',
            slippage=0.001,               # âœ…è¡¥ä¸3: æ¸¯è‚¡å·®å¼‚åŒ– 0.1%æ»‘ç‚¹
            fees=0.0005,                  # âœ…è¡¥ä¸3: æ¸¯è‚¡å·®å¼‚åŒ– 0.05%æ‰‹ç»­è´¹(åˆè®¡0.15%)
            min_trades=30                 # âœ…è¡¥ä¸4: å…ˆç”¨30æ¬¡çœ‹æ•ˆæœï¼Œåç»­å¯è°ƒæ•´
        )
        
        # è·å–æ‰€æœ‰å› å­åç§°
        all_factors = set()
        for factors_df in symbol_factors.values():
            all_factors.update(factors_df.columns)
        factor_names = list(all_factors)
        
        self.logger.info(f"  ğŸ”¢ å¼€å§‹CTAè¯„ä¼°{len(symbol_factors)}åªè‚¡ç¥¨ Ã— {len(factor_names)}ä¸ªå› å­")
        
        # 3. æ‰¹é‡CTAè¯„ä¼° (ä¼ å…¥æ—¶é—´æ¡†æ¶)
        cta_results = cta_evaluator.batch_evaluate(
            symbols=list(symbol_factors.keys()),
            factor_data=symbol_factors,
            price_data=symbol_data,
            factor_names=factor_names,
            timeframe=timeframe  # ä¼ å…¥æ—¶é—´æ¡†æ¶ä»¥ä¾¿æ­£ç¡®è®¡ç®—çª—å£
        )
        
        if cta_results.empty:
            self.logger.warning(f"âŒ {timeframe} CTAè¯„ä¼°æ— ç»“æœ")
            return {}
        
        # âœ…ä¿®å¤5: è¯„ä¼°åå†…å­˜ç›‘æ§
        try:
            import psutil
            process = psutil.Process(os.getpid())
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.logger.debug(f"    ğŸ’¾ CTAè¯„ä¼°åå†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")
        except:
            pass
        
        # 4. å› å­æ’å (ä¿®å¤ç‰ˆè¯„ä¼°å™¨æœ‰å†…ç½®è¿‡æ»¤)
        # âœ…ä¿®å¤2: éªŒè¯CTAè¯„ä¼°ç»“æœæœ‰æ•ˆæ€§
        self.logger.debug(f"    ğŸ“Š CTAè¯„ä¼°ç»“æœéªŒè¯:")
        self.logger.debug(f"      ç»“æœå½¢çŠ¶: {cta_results.shape}")
        self.logger.debug(f"      åˆ—å: {list(cta_results.columns)}")
        if 'sharpe' in cta_results.columns:
            sharpe_stats = cta_results['sharpe'].describe()
            self.logger.debug(f"      å¤æ™®ç‡ç»Ÿè®¡: å‡å€¼={sharpe_stats['mean']:.4f}, æœ€å¤§å€¼={sharpe_stats['max']:.4f}, ä¸­ä½æ•°={sharpe_stats['50%']:.4f}")
            valid_sharpe_count = (cta_results['sharpe'] > 0.01).sum()
            self.logger.debug(f"      æœ‰æ•ˆå¤æ™®ç‡(>0.01): {valid_sharpe_count}/{len(cta_results)} ({valid_sharpe_count/len(cta_results)*100:.1f}%)")
        if 'trades' in cta_results.columns:
            trade_stats = cta_results['trades'].describe()
            self.logger.debug(f"      äº¤æ˜“æ¬¡æ•°ç»Ÿè®¡: å‡å€¼={trade_stats['mean']:.1f}, æœ€å¤§å€¼={trade_stats['max']:.0f}, ä¸­ä½æ•°={trade_stats['50%']:.0f}")
        
        factor_ranking = cta_evaluator.rank_factors(
            cta_results, 
            rank_by='sharpe'  # ä½¿ç”¨ä¿®å¤ç‰ˆçš„å†…ç½®è¿‡æ»¤æ¡ä»¶
        )
        
        # 5. ç»Ÿè®¡æœ‰æ•ˆå› å­
        if factor_ranking.empty:
            valid_factors = pd.DataFrame()
        else:
            # æ£€æŸ¥åˆ—åæ˜¯å¦å­˜åœ¨
            sharpe_col = 'sharpe_mean' if 'sharpe_mean' in factor_ranking.columns else 'sharpe'
            if sharpe_col in factor_ranking.columns:
                valid_factors = factor_ranking[factor_ranking[sharpe_col] >= 0.05]  # ä¿®å¤: æé«˜é˜ˆå€¼åˆ°0.05
            else:
                valid_factors = factor_ranking.head(10)  # å–å‰10ä¸ªå› å­ä½œä¸ºæœ‰æ•ˆå› å­
        
        self.logger.info(f"  âœ… {timeframe} å‘ç°{len(valid_factors)}ä¸ªä¼˜è´¨å› å­ (å¤æ™®â‰¥0.05ï¼Œä¿®å¤é˜ˆå€¼)")
        
        # ğŸ” ä¸Šçº¿å‰æœ€åä½“æ£€ï¼šTop5å› å­äººå·¥æŠ½æŸ¥
        if not factor_ranking.empty and len(factor_ranking) >= 1:
            top5 = factor_ranking.head(5)
            required_cols = ['factor', 'sharpe_mean', 'trades_sum', 'win_rate_mean']
            available_cols = [col for col in required_cols if col in top5.columns]
            
            # å¦‚æœåˆ—åä¸åŒï¼Œå°è¯•æ‰¾åˆ°å¯¹åº”çš„åˆ—
            col_mapping = {
                'sharpe_mean': 'sharpe' if 'sharpe' in top5.columns else 'sharpe_mean',
                'trades_sum': 'trades' if 'trades' in top5.columns else 'trades_sum', 
                'win_rate_mean': 'win_rate' if 'win_rate' in top5.columns else 'win_rate_mean'
            }
            
            display_cols = ['factor'] + [col_mapping.get(col, col) for col in required_cols[1:] if col_mapping.get(col, col) in top5.columns]
            
            self.logger.info(f"\nğŸ” {timeframe} Top5å› å­äººå·¥æŠ½æŸ¥:")
            self.logger.info("\n" + top5[display_cols].to_string(index=False))
            
            # å¼‚å¸¸å€¼è­¦å‘Š
            sharpe_col = col_mapping.get('sharpe_mean', 'sharpe_mean')
            trades_col = col_mapping.get('trades_sum', 'trades_sum')
            winrate_col = col_mapping.get('win_rate_mean', 'win_rate_mean')
            
            if sharpe_col in top5.columns:
                max_sharpe = top5[sharpe_col].max()
                min_sharpe = top5[sharpe_col].min()
                if max_sharpe > 0.8:
                    self.logger.warning(f"âš ï¸ å‘ç°è¶…é«˜å¤æ™®ç‡{max_sharpe:.3f}>0.8ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆï¼")
                elif min_sharpe < 0.02:
                    self.logger.warning(f"âš ï¸ å‘ç°è¶…ä½å¤æ™®ç‡{min_sharpe:.3f}<0.02ï¼Œå¯èƒ½æ˜¯å™ªéŸ³ï¼")
                    
            if trades_col in top5.columns:
                min_trades = top5[trades_col].min()
                max_trades = top5[trades_col].max()
                if min_trades < 20:
                    self.logger.warning(f"âš ï¸ å‘ç°è¶…ä½äº¤æ˜“æ¬¡æ•°{min_trades}<20ï¼Œæ ·æœ¬ä¸è¶³ï¼")
                elif max_trades > 2000:
                    self.logger.warning(f"âš ï¸ å‘ç°è¶…é«˜äº¤æ˜“æ¬¡æ•°{max_trades}>2000ï¼Œä¿¡å·è¿‡å¯†ï¼")
                    
            if winrate_col in top5.columns:
                max_winrate = top5[winrate_col].max()
                if max_winrate > 0.6:
                    self.logger.warning(f"âš ï¸ å‘ç°è¶…é«˜èƒœç‡{max_winrate:.1%}>60%ï¼Œå¤æŸ¥æ˜¯å¦å·ä»·ï¼")
        
        return {
            'timeframe': timeframe,
            'cta_results': cta_results,
            'factor_ranking': factor_ranking,
            'valid_factors': valid_factors,
            'summary': {
                'tested_symbols': len(symbol_factors),
                'tested_factors': len(factor_names),
                'total_evaluations': len(cta_results),
                'valid_factors_count': len(valid_factors),
                'best_factor': factor_ranking.iloc[0]['factor'] if not factor_ranking.empty else None,
                'best_sharpe': factor_ranking.iloc[0]['sharpe_mean'] if not factor_ranking.empty else 0
            }
        }
    
    def run_final_test(self):
        """è¿è¡Œæœ€ç»ˆæµ‹è¯•"""
        print("ğŸš€ å¯åŠ¨æœ€ç»ˆå·¥ä½œç‰ˆVectorBTæµ‹è¯•...")
        start_time = time.time()
        
        self.logger.info("æœ€ç»ˆæµ‹è¯•å¼€å§‹")
        self.logger.info(f"  æµ‹è¯•æ—¶é—´æ¡†æ¶: {self.working_config['test_timeframes']}")
        self.logger.info(f"  æµ‹è¯•è‚¡ç¥¨: {self.test_symbols}")
        
        # ç»“æœåˆå§‹åŒ–
        results = {
            'execution_time': 0,
            'analysis_approach': 'final_working_vectorbt',
            'tested_symbols': self.test_symbols,
            'tested_timeframes': self.working_config['test_timeframes'],
            'working_config': self.working_config,
            'timeframe_results': {}
        }
        
        # ğŸ”¥ æŒ‰æ—¶é—´æ¡†æ¶åˆ†æ - æ”¯æŒCTAå’ŒICä¸¤ç§æ¨¡å¼
        total_factors = 0
        evaluation_mode = self.working_config.get('evaluation_mode', 'ic')
        self.logger.info(f"  è¯„ä¼°æ¨¡å¼: {evaluation_mode.upper()}")
        
        for timeframe in self.working_config['test_timeframes']:
            if evaluation_mode == 'cta':
                analysis_results = self._run_cta_analysis(timeframe)
                if analysis_results:
                    results['timeframe_results'][timeframe] = analysis_results
                    total_factors += analysis_results['summary']['valid_factors_count']
            else:
                ic_results = self._analyze_timeframe(timeframe)
                if ic_results:
                    results['timeframe_results'][timeframe] = ic_results
                    total_factors += len(ic_results)
            
        # å®Œæˆç»Ÿè®¡
        execution_time = time.time() - start_time
        results['execution_time'] = execution_time
        
        self.logger.info(f"æœ€ç»ˆæµ‹è¯•å®Œæˆï¼Œè€—æ—¶{execution_time:.1f}ç§’")
        self.logger.info(f"æ€»æœ‰æ•ˆå› å­: {total_factors}ä¸ª")
        
        # ä¿å­˜ç»“æœ
        self._save_results(results)
        
        return results
    
    def _save_results(self, results: Dict):
        """ä¿å­˜ç»“æœ"""
        result_dir = f"results/final_working_{self.timestamp}"
        os.makedirs(result_dir, exist_ok=True)
        
        # ä¿å­˜JSONç»“æœ
        json_file = f"{result_dir}/final_working_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”Ÿæˆç®€è¦æŠ¥å‘Š
        self._generate_summary_report(results, result_dir)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {result_dir}")
    
    def _generate_summary_report(self, results: Dict, result_dir: str):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        report = [
            "# ğŸ¯ æœ€ç»ˆå·¥ä½œç‰ˆVectorBTæµ‹è¯•æŠ¥å‘Š",
            "",
            f"**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**æ‰§è¡Œæ—¶é—´**: {results['execution_time']:.1f}ç§’",
            f"**æµ‹è¯•è‚¡ç¥¨**: {len(results['tested_symbols'])}åª",
            f"**æµ‹è¯•æ—¶é—´æ¡†æ¶**: {len(results['tested_timeframes'])}ä¸ª",
            "",
            "## ğŸ“Š ç»“æœç»Ÿè®¡",
            ""
        ]
        
        timeframe_results = results.get('timeframe_results', {})
        
        if timeframe_results:
            report.append("| æ—¶é—´æ¡†æ¶ | æœ‰æ•ˆå› å­æ•° | ä¼˜ç§€å› å­ | æœ€ä½³IC |")
            report.append("|----------|------------|----------|--------|")
            
            total_factors = 0
            # ğŸ”¥ æ ¹æ®è¯„ä¼°æ¨¡å¼ç”Ÿæˆä¸åŒçš„ç»Ÿè®¡è¡¨
            evaluation_mode = results.get('working_config', {}).get('evaluation_mode', 'ic')
            
            for tf, result_data in timeframe_results.items():
                if evaluation_mode == 'cta':
                    # CTAæ¨¡å¼: å¤„ç†CTAç»“æœ
                    factor_count = result_data['summary']['valid_factors_count']
                    total_factors += factor_count
                    
                    # ç»Ÿè®¡ä¼˜ç§€å› å­ (å¤æ™®>0.5)
                    valid_factors = result_data.get('valid_factors', pd.DataFrame())
                    if not valid_factors.empty and 'sharpe_mean' in valid_factors.columns:
                        excellent_factors = len(valid_factors[valid_factors['sharpe_mean'] > 0.5])
                        best_sharpe = valid_factors['sharpe_mean'].max() if not valid_factors.empty else 0
                    else:
                        excellent_factors = 0
                        best_sharpe = 0
                    
                    report.append(f"| {tf} | {factor_count} | {excellent_factors} | {best_sharpe:.3f} |")
                else:
                    # ICæ¨¡å¼: åŸé€»è¾‘
                    factor_count = len(result_data)
                    total_factors += factor_count
                    
                    # ç»Ÿè®¡ä¼˜ç§€å› å­ (|IC| > 0.02)
                    excellent_factors = sum(1 for f in result_data.values() if abs(f['ic']) > 0.02)
                    
                    # æ‰¾æœ€ä½³IC
                    best_ic = max(result_data.values(), key=lambda x: abs(x['ic']))['ic'] if result_data else 0
                    
                    report.append(f"| {tf} | {factor_count} | {excellent_factors} | {best_ic:.3f} |")
            
            report.extend([
                "",
                f"**æ€»è®¡**: {total_factors}ä¸ªæœ‰æ•ˆå› å­",
                ""
            ])
            
            # ğŸ”¥ æ˜¾ç¤ºæœ€ä½³å› å­ - æ ¹æ®è¯„ä¼°æ¨¡å¼
            all_factors = []
            
            if evaluation_mode == 'cta':
                # CTAæ¨¡å¼: æå–å› å­æ’åæ•°æ®
                for tf, result_data in timeframe_results.items():
                    factor_ranking = result_data.get('factor_ranking', pd.DataFrame())
                    if not factor_ranking.empty:
                        for _, row in factor_ranking.head(10).iterrows():
                            all_factors.append({
                                'name': row.get('factor', 'unknown'),
                                'timeframe': tf,
                                'sharpe': row.get('sharpe_mean', 0),
                                'win_rate': row.get('win_rate_mean', 0),
                                'trades': row.get('trades_sum', 0)
                            })
                
                if all_factors:
                    all_factors.sort(key=lambda x: x['sharpe'], reverse=True)
                    
                    report.extend([
                        "## ğŸ† æœ€ä½³å› å­ (Top 10)",
                        "",
                        "| æ’å | å› å­åç§° | æ—¶é—´æ¡†æ¶ | å¤æ™®ç‡ | èƒœç‡ | äº¤æ˜“æ¬¡æ•° |",
                        "|------|----------|----------|--------|------|----------|"
                    ])
                    
                    for i, factor in enumerate(all_factors[:10], 1):
                        report.append(
                            f"| {i} | {factor['name']} | {factor['timeframe']} | "
                            f"{factor['sharpe']:.3f} | {factor['win_rate']:.1%} | {factor['trades']:.0f} |"
                        )
            else:
                # ICæ¨¡å¼: åŸé€»è¾‘
                for tf, ic_data in timeframe_results.items():
                    for factor_name, factor_data in ic_data.items():
                        all_factors.append({
                            'name': factor_name,
                            'timeframe': tf,
                            'ic': factor_data['ic'],
                            'ic_ir': factor_data['ic_ir'],
                            'supporting_stocks': factor_data['supporting_stocks']
                        })
                
                if all_factors:
                    all_factors.sort(key=lambda x: abs(x['ic']), reverse=True)
                    
                    report.extend([
                        "## ğŸ† æœ€ä½³å› å­ (Top 10)",
                        "",
                        "| æ’å | å› å­åç§° | æ—¶é—´æ¡†æ¶ | IC | IC_IR | æ”¯æŒè‚¡ç¥¨æ•° |",
                        "|------|----------|----------|-----|-------|------------|"
                    ])
                    
                    for i, factor in enumerate(all_factors[:10], 1):
                        report.append(
                            f"| {i} | {factor['name']} | {factor['timeframe']} | "
                            f"{factor['ic']:.3f} | {factor['ic_ir']:.3f} | {factor['supporting_stocks']} |"
                        )
        else:
            report.append("âŒ æœªå‘ç°æœ‰æ•ˆå› å­")
        
        # å†™å…¥æŠ¥å‘Š
        report_content = "\n".join(report)
        report_file = f"{result_dir}/final_working_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

def main():
    """ä¸»å‡½æ•°"""
    # âœ…ä¿®å¤4: è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
    np.random.seed(42)
    
    final_analyzer = FinalWorkingVectorBT()
    results = final_analyzer.run_final_test()
    
    if results:
        print("ğŸ‰ æœ€ç»ˆå·¥ä½œç‰ˆæµ‹è¯•å®Œæˆï¼")
        
        timeframe_results = results.get('timeframe_results', {})
        evaluation_mode = results.get('working_config', {}).get('evaluation_mode', 'ic')
        
        # è®¡ç®—æ€»å› å­æ•° - æ ¹æ®è¯„ä¼°æ¨¡å¼
        if evaluation_mode == 'cta':
            total_factors = sum(result_data['summary']['valid_factors_count'] for result_data in timeframe_results.values())
        else:
            total_factors = sum(len(ic_data) for ic_data in timeframe_results.values())
        
        if evaluation_mode == 'cta':
            print(f"ğŸ¯ CTAå›æµ‹æ¨¡å¼: å‘ç°{total_factors}ä¸ªä¼˜è´¨å› å­ (å¤æ™®â‰¥0.05ï¼Œä¿®å¤å®Œæˆ)")
            print(f"ğŸ“Š è¯„ä¼°ç»´åº¦: å¤æ™®ç‡ã€èƒœç‡ã€ç›ˆäºæ¯”ã€äº¤æ˜“æ¬¡æ•°")
        else:
            print(f"ğŸ“Š ICåˆ†ææ¨¡å¼: å‘ç°{total_factors}ä¸ªæœ‰æ•ˆå› å­")
            print(f"ğŸ“Š è¯„ä¼°ç»´åº¦: ICã€IC_IRã€æ­£å‘æ¯”ä¾‹")
        print(f"âš¡ è¦†ç›–{len(timeframe_results)}ä¸ªæ—¶é—´æ¡†æ¶")
        print(f"ğŸ“ˆ æµ‹è¯•{len(results['tested_symbols'])}åªè‚¡ç¥¨")
        
        if total_factors > 0:
            print("âœ… è¯„ä¼°ç³»ç»Ÿæ­£å¸¸å·¥ä½œï¼")
        else:
            print("âŒ ä»éœ€è¿›ä¸€æ­¥è°ƒè¯•...")

if __name__ == "__main__":
    main()
