#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å‘é‡åŒ–å¤šè‚¡ç¥¨åˆ†æå™¨ - å……åˆ†åˆ©ç”¨vectorbtçš„ä¼˜åŠ¿
çœŸæ­£çš„æ‰¹é‡å¤„ç†ï¼š53åªè‚¡ç¥¨ Ã— 7ä¸ªæ—¶é—´æ¡†æ¶ Ã— 5ä¸ªå› å­ = 1855ä¸ªç»„åˆå¹¶è¡Œåˆ†æ
"""

import sys
import pandas as pd
import numpy as np
import vectorbt as vbt
import talib
from pathlib import Path
from datetime import datetime
import logging
import json
import gc
import traceback
import time
from typing import Dict, List, Tuple, Optional, Union
import psutil
from concurrent.futures import ProcessPoolExecutor, as_completed
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

class VectorizedMultiStockAnalyzer:
    """å……åˆ†åˆ©ç”¨vectorbtä¼˜åŠ¿çš„å¤šè‚¡ç¥¨å‘é‡åŒ–åˆ†æå™¨"""
    
    def __init__(self, 
                 data_dir: str = "/Users/zhangshenshen/longport/vectorbt_workspace/data",
                 start_date: str = "2024-01-01",
                 end_date: str = "2025-09-01",
                 memory_limit_gb: float = 16.0):
        
        self.data_dir = Path(data_dir)
        self.start_date = pd.to_datetime(start_date).tz_localize('Asia/Hong_Kong')
        self.end_date = pd.to_datetime(end_date).tz_localize('Asia/Hong_Kong')
        self.memory_limit_gb = memory_limit_gb
        
        # æ‰€æœ‰æ”¯æŒçš„æ—¶é—´æ¡†æ¶
        self.all_timeframes = ['1m', '5m', '15m', '30m', '1h', '4h', '1d']
        
        # æ‰€æœ‰å› å­
        self.all_factors = ["RSI", "MACD", "Momentum_ROC", "Price_Position", "Volume_Ratio"]
        
        # æ•°æ®ç¼“å­˜
        self.data_cache = {}
        self.factors_cache = {}
        
        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logger()
        
        # è·å–æ‰€æœ‰å¯ç”¨è‚¡ç¥¨
        self.all_symbols = self._get_available_symbols()
        
        self.logger.info(f"åˆå§‹åŒ–å‘é‡åŒ–åˆ†æå™¨:")
        self.logger.info(f"  æ•°æ®ç›®å½•: {self.data_dir}")
        self.logger.info(f"  æ—¶é—´èŒƒå›´: {self.start_date} åˆ° {self.end_date}")
        self.logger.info(f"  å†…å­˜é™åˆ¶: {self.memory_limit_gb}GB")
        self.logger.info(f"  å¯ç”¨è‚¡ç¥¨æ•°: {len(self.all_symbols)}")
        self.logger.info(f"  æ—¶é—´æ¡†æ¶æ•°: {len(self.all_timeframes)}")
        self.logger.info(f"  å› å­æ•°: {len(self.all_factors)}")
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        logger = logging.getLogger(f"{__name__}.VectorizedAnalyzer")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def _get_available_symbols(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„è‚¡ç¥¨ä»£ç """
        symbols = []
        for timeframe in ['1m']:  # åªéœ€è¦æ£€æŸ¥ä¸€ä¸ªæ—¶é—´æ¡†æ¶
            timeframe_dir = self.data_dir / timeframe
            if timeframe_dir.exists():
                for file in timeframe_dir.glob("*.HK.parquet"):
                    symbol = file.stem
                    if symbol not in symbols:
                        symbols.append(symbol)
        
        symbols.sort()
        return symbols
    
    def load_timeframe_data_vectorized(self, 
                                     timeframe: str, 
                                     symbols: List[str] = None) -> pd.DataFrame:
        """å‘é‡åŒ–åŠ è½½æŒ‡å®šæ—¶é—´æ¡†æ¶çš„æ‰€æœ‰è‚¡ç¥¨æ•°æ®"""
        if symbols is None:
            symbols = self.all_symbols
        
        cache_key = f"{timeframe}_{len(symbols)}"
        if cache_key in self.data_cache:
            self.logger.info(f"ä½¿ç”¨ç¼“å­˜æ•°æ®: {timeframe}, {len(symbols)}åªè‚¡ç¥¨")
            return self.data_cache[cache_key]
        
        self.logger.info(f"å¼€å§‹åŠ è½½{timeframe}æ•°æ®: {len(symbols)}åªè‚¡ç¥¨")
        
        timeframe_dir = self.data_dir / timeframe
        if not timeframe_dir.exists():
            raise ValueError(f"æ—¶é—´æ¡†æ¶ç›®å½•ä¸å­˜åœ¨: {timeframe_dir}")
        
        # æ‰¹é‡åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
        dfs = []
        successful_symbols = []
        
        for symbol in symbols:
            file_path = timeframe_dir / f"{symbol}.parquet"
            if file_path.exists():
                try:
                    df = pd.read_parquet(file_path)
                    
                    # æ—¶åŒºå¤„ç†
                    if 'timestamp' in df.columns:
                        df['timestamp'] = pd.to_datetime(df['timestamp'])
                        if df['timestamp'].dt.tz is None:
                            df['timestamp'] = df['timestamp'].dt.tz_localize('Asia/Hong_Kong')
                        else:
                            df['timestamp'] = df['timestamp'].dt.tz_convert('Asia/Hong_Kong')
                        df.set_index('timestamp', inplace=True)
                    
                    # æ—¶é—´èŒƒå›´è¿‡æ»¤
                    df = df.loc[self.start_date:self.end_date]
                    
                    if len(df) > 0:
                        # æ·»åŠ è‚¡ç¥¨æ ‡è¯†
                        df['symbol'] = symbol
                        dfs.append(df)
                        successful_symbols.append(symbol)
                    else:
                        self.logger.warning(f"{symbol}: æ—¶é—´èŒƒå›´å†…æ— æ•°æ®")
                        
                except Exception as e:
                    self.logger.warning(f"{symbol}: åŠ è½½å¤±è´¥ - {e}")
        
        if not dfs:
            raise ValueError(f"æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•{timeframe}æ•°æ®")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®ä¸ºå¤šé‡ç´¢å¼•DataFrame
        combined_df = pd.concat(dfs, ignore_index=False)
        combined_df.reset_index(inplace=True)
        
        # åˆ›å»ºMultiIndex: (symbol, timestamp)
        combined_df.set_index(['symbol', 'timestamp'], inplace=True)
        combined_df.sort_index(inplace=True)
        
        # ç¼“å­˜æ•°æ®
        self.data_cache[cache_key] = combined_df
        
        self.logger.info(f"âœ… {timeframe}æ•°æ®åŠ è½½å®Œæˆ: {len(successful_symbols)}åªè‚¡ç¥¨, å½¢çŠ¶{combined_df.shape}")
        
        return combined_df
    
    def calculate_factors_vectorized(self, 
                                   data: pd.DataFrame, 
                                   factors: List[str] = None) -> Dict[str, pd.Series]:
        """å‘é‡åŒ–è®¡ç®—æ‰€æœ‰å› å­ - åŒæ—¶å¤„ç†æ‰€æœ‰è‚¡ç¥¨"""
        if factors is None:
            factors = self.all_factors
        
        self.logger.info(f"å¼€å§‹å‘é‡åŒ–è®¡ç®—å› å­: {factors}")
        
        factors_dict = {}
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„è¿›è¡Œå‘é‡åŒ–è®¡ç®—
        grouped = data.groupby('symbol')
        
        for factor_name in factors:
            self.logger.info(f"  è®¡ç®—å› å­: {factor_name}")
            
            factor_results = []
            
            # å¯¹æ¯åªè‚¡ç¥¨è®¡ç®—å› å­ï¼ˆè¿™é‡Œä»ç„¶éœ€è¦åˆ†è‚¡ç¥¨ï¼Œå› ä¸ºtalibä¸æ”¯æŒMultiIndexï¼‰
            for symbol, group_data in grouped:
                try:
                    factor_values = self._calculate_single_factor(group_data, factor_name)
                    
                    if factor_values is not None and len(factor_values) > 0:
                        # åˆ›å»ºå¸¦symbolæ ‡è¯†çš„Series
                        factor_series = pd.Series(
                            factor_values, 
                            index=group_data.index.get_level_values('timestamp'),
                            name=f"{symbol}_{factor_name}"
                        )
                        
                        # æ·»åŠ symbolåˆ—ç”¨äºåç»­é‡å»ºMultiIndex
                        factor_df = factor_series.to_frame(factor_name)
                        factor_df['symbol'] = symbol
                        factor_results.append(factor_df)
                        
                except Exception as e:
                    self.logger.warning(f"{symbol}è®¡ç®—{factor_name}å¤±è´¥: {e}")
            
            if factor_results:
                # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨çš„å› å­æ•°æ®
                factor_combined = pd.concat(factor_results)
                factor_combined.reset_index(inplace=True)
                factor_combined.set_index(['symbol', 'timestamp'], inplace=True)
                factor_combined.sort_index(inplace=True)
                
                factors_dict[factor_name] = factor_combined[factor_name]
                
                self.logger.info(f"  âœ… {factor_name}è®¡ç®—å®Œæˆ: {len(factor_results)}åªè‚¡ç¥¨")
            else:
                self.logger.warning(f"  âŒ {factor_name}è®¡ç®—å¤±è´¥ï¼šæ— æœ‰æ•ˆæ•°æ®")
        
        return factors_dict
    
    def _calculate_single_factor(self, data: pd.DataFrame, factor_name: str) -> np.ndarray:
        """è®¡ç®—å•ä¸ªå› å­"""
        try:
            close = data['close'].values
            high = data['high'].values
            low = data['low'].values
            volume = data['volume'].values
            
            if factor_name == "RSI":
                return talib.RSI(close, timeperiod=14)
            
            elif factor_name == "MACD":
                macd, macd_signal, macd_hist = talib.MACD(close)
                return macd
            
            elif factor_name == "Momentum_ROC":
                return talib.ROC(close, timeperiod=10)
            
            elif factor_name == "Price_Position":
                # ä»·æ ¼ä½ç½®ï¼šå½“å‰ä»·æ ¼åœ¨è¿‡å»Nå¤©é«˜ä½ç‚¹ä¸­çš„ç›¸å¯¹ä½ç½®
                period = 20
                rolling_high = pd.Series(high).rolling(period).max()
                rolling_low = pd.Series(low).rolling(period).min()
                price_position = (close - rolling_low) / (rolling_high - rolling_low)
                return price_position.values
            
            elif factor_name == "Volume_Ratio":
                # æˆäº¤é‡æ¯”ç‡ï¼šå½“å‰æˆäº¤é‡ä¸è¿‡å»Nå¤©å¹³å‡æˆäº¤é‡çš„æ¯”å€¼
                period = 20
                avg_volume = pd.Series(volume).rolling(period).mean()
                volume_ratio = volume / avg_volume
                return volume_ratio.values
            
            else:
                self.logger.warning(f"æœªçŸ¥å› å­: {factor_name}")
                return None
                
        except Exception as e:
            self.logger.error(f"è®¡ç®—å› å­{factor_name}å¤±è´¥: {e}")
            return None
    
    def calculate_ic_vectorized(self, 
                              data: pd.DataFrame, 
                              factors_dict: Dict[str, pd.Series]) -> Dict[str, Dict[str, float]]:
        """å‘é‡åŒ–è®¡ç®—æ‰€æœ‰å› å­çš„ICå€¼"""
        self.logger.info("å¼€å§‹å‘é‡åŒ–è®¡ç®—IC")
        
        ic_results = {}
        
        # è®¡ç®—æœªæ¥æ”¶ç›Šç‡ï¼ˆå‘é‡åŒ–ï¼‰
        grouped = data.groupby('symbol')
        future_returns_list = []
        
        for symbol, group_data in grouped:
            returns = group_data['close'].pct_change(periods=1).shift(-1)
            returns_df = returns.to_frame('future_returns')
            returns_df['symbol'] = symbol
            future_returns_list.append(returns_df)
        
        if future_returns_list:
            future_returns_combined = pd.concat(future_returns_list)
            future_returns = future_returns_combined['future_returns']
        else:
            self.logger.error("æ— æ³•è®¡ç®—æœªæ¥æ”¶ç›Šç‡")
            return ic_results
        
        # å¯¹æ¯ä¸ªå› å­è®¡ç®—IC
        for factor_name, factor_values in factors_dict.items():
            try:
                # ç¡®ä¿ç´¢å¼•ä¸€è‡´
                common_index = factor_values.index.intersection(future_returns.index)
                
                if len(common_index) > 10:  # è‡³å°‘éœ€è¦10ä¸ªæ•°æ®ç‚¹
                    aligned_factor = factor_values.loc[common_index]
                    aligned_returns = future_returns.loc[common_index]
                    
                    # å»é™¤NaNå€¼
                    valid_mask = aligned_factor.notna() & aligned_returns.notna()
                    clean_factor = aligned_factor[valid_mask]
                    clean_returns = aligned_returns[valid_mask]
                    
                    if len(clean_factor) > 10:
                        # è®¡ç®—IC (Information Coefficient)
                        ic = clean_factor.corr(clean_returns)
                        
                        ic_results[factor_name] = {
                            'ic': ic if not np.isnan(ic) else 0.0,
                            'sample_size': len(clean_factor),
                            'factor_mean': float(clean_factor.mean()),
                            'factor_std': float(clean_factor.std()),
                            'returns_mean': float(clean_returns.mean()),
                            'returns_std': float(clean_returns.std())
                        }
                        
                        self.logger.info(f"  {factor_name}: IC={ic:.4f}, æ ·æœ¬æ•°={len(clean_factor)}")
                    else:
                        self.logger.warning(f"  {factor_name}: æœ‰æ•ˆæ•°æ®ä¸è¶³({len(clean_factor)})")
                        ic_results[factor_name] = {'ic': 0.0, 'sample_size': 0}
                else:
                    self.logger.warning(f"  {factor_name}: ç´¢å¼•é‡å ä¸è¶³({len(common_index)})")
                    ic_results[factor_name] = {'ic': 0.0, 'sample_size': 0}
                    
            except Exception as e:
                self.logger.error(f"è®¡ç®—{factor_name}çš„ICå¤±è´¥: {e}")
                ic_results[factor_name] = {'ic': 0.0, 'sample_size': 0}
        
        return ic_results
    
    def run_vectorized_analysis(self, 
                              symbols: List[str] = None, 
                              timeframes: List[str] = None,
                              factors: List[str] = None) -> Dict:
        """è¿è¡Œå‘é‡åŒ–åˆ†æ - æ ¸å¿ƒæ–¹æ³•"""
        
        if symbols is None:
            symbols = self.all_symbols[:1]  # å…ˆæµ‹è¯•1åªè‚¡ç¥¨
        if timeframes is None:
            timeframes = ['1h']  # å…ˆæµ‹è¯•1ä¸ªæ—¶é—´æ¡†æ¶
        if factors is None:
            factors = ['RSI', 'MACD']  # å…ˆæµ‹è¯•2ä¸ªå› å­
        
        self.logger.info(f"å¼€å§‹å‘é‡åŒ–åˆ†æ:")
        self.logger.info(f"  è‚¡ç¥¨: {len(symbols)}åª {symbols}")
        self.logger.info(f"  æ—¶é—´æ¡†æ¶: {len(timeframes)}ä¸ª {timeframes}")
        self.logger.info(f"  å› å­: {len(factors)}ä¸ª {factors}")
        
        results = {
            'metadata': {
                'symbols': symbols,
                'timeframes': timeframes, 
                'factors': factors,
                'start_date': str(self.start_date),
                'end_date': str(self.end_date),
                'analysis_time': datetime.now().isoformat()
            },
            'timeframe_results': {}
        }
        
        # å¯¹æ¯ä¸ªæ—¶é—´æ¡†æ¶è¿›è¡Œåˆ†æ
        for timeframe in timeframes:
            self.logger.info(f"\nğŸ”„ åˆ†ææ—¶é—´æ¡†æ¶: {timeframe}")
            
            try:
                # 1. åŠ è½½æ•°æ®
                data = self.load_timeframe_data_vectorized(timeframe, symbols)
                
                # 2. è®¡ç®—å› å­
                factors_dict = self.calculate_factors_vectorized(data, factors)
                
                # 3. è®¡ç®—IC
                ic_results = self.calculate_ic_vectorized(data, factors_dict)
                
                # 4. å­˜å‚¨ç»“æœ
                results['timeframe_results'][timeframe] = {
                    'data_shape': data.shape,
                    'data_symbols': len(data.index.get_level_values('symbol').unique()),
                    'factors_calculated': list(factors_dict.keys()),
                    'ic_results': ic_results,
                    'analysis_status': 'success'
                }
                
                self.logger.info(f"âœ… {timeframe}åˆ†æå®Œæˆ")
                
            except Exception as e:
                error_msg = f"{timeframe}åˆ†æå¤±è´¥: {str(e)}\n{traceback.format_exc()}"
                self.logger.error(error_msg)
                
                results['timeframe_results'][timeframe] = {
                    'analysis_status': 'failed',
                    'error': str(e)
                }
        
        return results
    
    def run_smoke_test(self, symbol: str = "0700.HK") -> Dict:
        """è¿è¡Œå†’çƒŸæµ‹è¯• - å•è‚¡ç¥¨å¿«é€ŸéªŒè¯"""
        self.logger.info(f"ğŸ§ª å¼€å§‹å†’çƒŸæµ‹è¯•: {symbol}")
        
        # æµ‹è¯•å‚æ•°
        test_symbols = [symbol]
        test_timeframes = ['1h']  # å…ˆæµ‹è¯•1å°æ—¶æ•°æ®
        test_factors = ['RSI', 'MACD']  # å…ˆæµ‹è¯•2ä¸ªå› å­
        
        try:
            results = self.run_vectorized_analysis(
                symbols=test_symbols,
                timeframes=test_timeframes, 
                factors=test_factors
            )
            
            # éªŒè¯ç»“æœ
            success = True
            issues = []
            
            for timeframe, result in results['timeframe_results'].items():
                if result['analysis_status'] != 'success':
                    success = False
                    issues.append(f"{timeframe}: {result.get('error', 'Unknown error')}")
                else:
                    # æ£€æŸ¥ICç»“æœ
                    ic_results = result.get('ic_results', {})
                    for factor, ic_data in ic_results.items():
                        if ic_data.get('sample_size', 0) == 0:
                            issues.append(f"{timeframe}.{factor}: æ— æœ‰æ•ˆæ ·æœ¬")
            
            results['smoke_test'] = {
                'status': 'PASS' if success and not issues else 'FAIL',
                'issues': issues,
                'summary': {
                    'total_combinations': len(test_timeframes) * len(test_factors),
                    'successful_combinations': sum(
                        1 for tf_result in results['timeframe_results'].values()
                        for factor in tf_result.get('ic_results', {}).keys()
                        if tf_result.get('ic_results', {}).get(factor, {}).get('sample_size', 0) > 0
                    )
                }
            }
            
            # æ‰“å°ç»“æœ
            print(f"\nğŸ§ª å†’çƒŸæµ‹è¯•ç»“æœ: {results['smoke_test']['status']}")
            print(f"   æµ‹è¯•ç»„åˆ: {results['smoke_test']['summary']['total_combinations']}")
            print(f"   æˆåŠŸç»„åˆ: {results['smoke_test']['summary']['successful_combinations']}")
            
            if issues:
                print(f"   é—®é¢˜åˆ—è¡¨:")
                for issue in issues:
                    print(f"     - {issue}")
            
            return results
            
        except Exception as e:
            error_msg = f"å†’çƒŸæµ‹è¯•å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
            self.logger.error(error_msg)
            
            return {
                'smoke_test': {
                    'status': 'FAIL',
                    'error': str(e),
                    'traceback': traceback.format_exc()
                }
            }
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self.data_cache.clear()
        self.factors_cache.clear()
        gc.collect()
        self.logger.info("ç¼“å­˜å·²æ¸…ç†")
    
    def _clear_cache(self):
        """å†…éƒ¨ç¼“å­˜æ¸…ç†æ–¹æ³•ï¼ˆå…¼å®¹æ€§ï¼‰"""
        self.clear_cache()


def main():
    """ä¸»å‡½æ•° - è¿è¡Œå†’çƒŸæµ‹è¯•"""
    print("="*80)
    print("ğŸš€ å‘é‡åŒ–å¤šè‚¡ç¥¨åˆ†æå™¨ - å†’çƒŸæµ‹è¯•")
    print("="*80)
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = VectorizedMultiStockAnalyzer()
        
        # è¿è¡Œå†’çƒŸæµ‹è¯•
        results = analyzer.run_smoke_test("0700.HK")
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"vectorized_smoke_test_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜: {output_file}")
        
        # æ˜¾ç¤ºçŠ¶æ€
        if results.get('smoke_test', {}).get('status') == 'PASS':
            print("âœ… å†’çƒŸæµ‹è¯•é€šè¿‡!")
            return 0
        else:
            print("âŒ å†’çƒŸæµ‹è¯•å¤±è´¥!")
            return 1
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
