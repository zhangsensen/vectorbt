#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆVectorBTç»ˆæä¼˜åŒ–
è§£å†³æ•°æ®è´¨é‡ã€æ€§èƒ½åˆ©ç”¨ç‡ã€ICå€¼è¿‡å°ç­‰æ ¹æœ¬é—®é¢˜
"""

import os
import sys
import time
import json
import logging
import warnings
import numpy as np
import pandas as pd
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple, Any
import psutil
import gc

# VectorBTå’ŒæŠ€æœ¯æŒ‡æ ‡
try:
    import vectorbt as vbt
    import talib
    print("âœ… VectorBTå’ŒTA-LibåŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from advanced_factor_pool import AdvancedFactorPool
from categorical_dtype_fix import CategoricalDtypeFixer

warnings.filterwarnings('ignore')

class EnhancedVectorBTUltimate:
    """
    å¢å¼ºç‰ˆVectorBTç»ˆæä¼˜åŒ–å™¨
    è§£å†³æ ¹æœ¬æ€§èƒ½å’Œæ•°æ®è´¨é‡é—®é¢˜
    """
    
    def __init__(self, data_dir: str = "../vectorbt_workspace/data", capital: float = 300000):
        self.data_dir = data_dir
        self.capital = capital
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ğŸ”¥ æ‰©å±•é…ç½® - å……åˆ†åˆ©ç”¨24GBå†…å­˜
        self.enhanced_config = {
            'all_timeframes': ['1m', '2m', '3m', '5m', '10m', '15m', '30m', '1h', '4h', '1d'],
            'test_timeframes': ['5m', '15m', '30m', '1h', '4h', '1d'],  # ğŸ”¥ å¢åŠ æ›´å¤šæ—¶é—´æ¡†æ¶
            'max_symbols': 200,  # ğŸ”¥ å¤§å¹…å¢åŠ è‚¡ç¥¨æ•°é‡
            'min_ic_threshold': 0.02,  # ğŸ”¥ æé«˜ICé˜ˆå€¼è¿‡æ»¤
            'min_ir_threshold': 0.1,   # ğŸ”¥ æé«˜IRé˜ˆå€¼è¿‡æ»¤
            'batch_processing': True,
            'parallel_processing': True,
            'memory_optimization': True,
            'enhanced_factors': True,
            'robust_validation': True
        }
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # VectorBTä¼˜åŒ–è®¾ç½®
        self._setup_vectorbt()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.factor_pool = AdvancedFactorPool()
        self.categorical_fixer = CategoricalDtypeFixer()
        
        # è·å–å¯ç”¨è‚¡ç¥¨
        self.available_symbols = self._get_available_symbols()
        self.logger.info(f"âœ… å‘ç°{len(self.available_symbols)}åªè‚¡ç¥¨")
        
    def _setup_vectorbt(self):
        """è®¾ç½®VectorBTä¼˜åŒ–é…ç½®"""
        try:
            # åŸºç¡€è®¾ç½®
            if hasattr(vbt.settings, 'array_wrapper'):
                try:
                    vbt.settings.array_wrapper['freq'] = None
                except:
                    pass
            
            # å†…å­˜å’Œæ€§èƒ½ä¼˜åŒ–
            if hasattr(vbt.settings, 'chunking'):
                try:
                    vbt.settings.chunking['enabled'] = True
                    vbt.settings.chunking['arg_take_spec'] = dict(
                        chunks=True,
                        chunk_len=10000  # ä¼˜åŒ–chunkå¤§å°
                    )
                except:
                    pass
            
            if hasattr(vbt.settings, 'caching'):
                try:
                    vbt.settings.caching['enabled'] = True
                    vbt.settings.caching['whitelist'] = []
                except:
                    pass
                
            # å¹¶è¡Œå¤„ç†
            if hasattr(vbt.settings, 'parallel'):
                try:
                    vbt.settings.parallel['enabled'] = True
                    vbt.settings.parallel['n_jobs'] = min(8, psutil.cpu_count())
                except:
                    pass
                
            self.logger.info("âœ… VectorBTä¼˜åŒ–é…ç½®å®Œæˆ")
            
        except Exception as e:
            self.logger.warning(f"VectorBTè®¾ç½®éƒ¨åˆ†å¤±è´¥: {e}")
            self.logger.info("ä½¿ç”¨VectorBTé»˜è®¤è®¾ç½®")
    
    def _setup_logging(self):
        """è®¾ç½®å¢å¼ºæ—¥å¿—"""
        log_dir = f"logs/enhanced_vectorbt_{self.timestamp}"
        os.makedirs(log_dir, exist_ok=True)
        
        self.logger = logging.getLogger('EnhancedVectorBT')
        self.logger.setLevel(logging.INFO)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(
            f"{log_dir}/enhanced_vectorbt.log", 
            encoding='utf-8'
        )
        file_handler.setLevel(logging.INFO)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # æ ¼å¼å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def _get_available_symbols(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨è‚¡ç¥¨"""
        symbols = set()
        
        for timeframe in self.enhanced_config['test_timeframes']:
            tf_dir = os.path.join(self.data_dir, timeframe)
            if os.path.exists(tf_dir):
                for file in os.listdir(tf_dir):
                    if file.endswith('.parquet'):
                        symbol = file.replace('.parquet', '')
                        symbols.add(symbol)
        
        symbols_list = sorted(list(symbols))
        
        # ğŸ”¥ å……åˆ†åˆ©ç”¨å†…å­˜ï¼Œæ‰©å±•åˆ°æ›´å¤šè‚¡ç¥¨
        max_symbols = self.enhanced_config['max_symbols']
        if len(symbols_list) > max_symbols:
            self.logger.info(f"ğŸ“ˆ é™åˆ¶è‚¡ç¥¨æ•°é‡åˆ°{max_symbols}åªï¼ˆä»{len(symbols_list)}åªä¸­é€‰æ‹©ï¼‰")
            symbols_list = symbols_list[:max_symbols]
        
        return symbols_list
    
    def _load_enhanced_multiindex_data(self) -> Dict[str, pd.DataFrame]:
        """å¢å¼ºç‰ˆå¤šæ—¶é—´æ¡†æ¶æ•°æ®åŠ è½½"""
        self.logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæ‰¹é‡æ•°æ®åŠ è½½...")
        start_time = time.time()
        
        all_data = {}
        total_data_points = 0
        
        for timeframe in self.enhanced_config['test_timeframes']:
            self.logger.info(f"ğŸ“Š åŠ è½½{timeframe}æ•°æ®...")
            
            tf_data_list = []
            tf_dir = os.path.join(self.data_dir, timeframe)
            
            if not os.path.exists(tf_dir):
                self.logger.warning(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {tf_dir}")
                continue
            
            loaded_symbols = 0
            for symbol in self.available_symbols:
                try:
                    file_path = os.path.join(tf_dir, f'{symbol}.parquet')
                    if os.path.exists(file_path):
                        df = pd.read_parquet(file_path)
                        
                        # æ ‡å‡†åŒ–ç´¢å¼•å’Œåˆ—
                        if not isinstance(df.index, pd.DatetimeIndex):
                            df.index = pd.to_datetime(df.index)
                        
                        # åˆ—åæ ‡å‡†åŒ–
                        column_mapping = {
                            'Close': 'close', 'Open': 'open', 'High': 'high', 
                            'Low': 'low', 'Volume': 'volume', 'Turnover': 'turnover'
                        }
                        df = df.rename(columns=column_mapping)
                        
                        # ç¡®ä¿åŸºç¡€åˆ—å­˜åœ¨
                        required_cols = ['open', 'high', 'low', 'close', 'volume']
                        available_cols = [col for col in required_cols if col in df.columns]
                        
                        if len(available_cols) >= 4:  # è‡³å°‘OHLC
                            # æ·»åŠ symbolåˆ—
                            df['symbol'] = symbol
                            tf_data_list.append(df[available_cols + ['symbol']])
                            loaded_symbols += 1
                        
                except Exception as e:
                    self.logger.debug(f"è·³è¿‡{symbol}: {e}")
                    continue
            
            if tf_data_list:
                # åˆ›å»ºMultiIndexæ•°æ®
                combined_df = pd.concat(tf_data_list, keys=[df['symbol'].iloc[0] for df in tf_data_list])
                combined_df.index.names = ['symbol', 'datetime']
                combined_df = combined_df.drop('symbol', axis=1)
                
                all_data[timeframe] = combined_df
                total_data_points += len(combined_df)
                
                self.logger.info(f"âœ… {timeframe}: {combined_df.shape}, {loaded_symbols}åªè‚¡ç¥¨")
            else:
                self.logger.warning(f"âŒ {timeframe}: æ— æœ‰æ•ˆæ•°æ®")
        
        load_time = time.time() - start_time
        self.logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {total_data_points:,}ä¸ªæ•°æ®ç‚¹, è€—æ—¶{load_time:.1f}ç§’")
        
        return all_data
    
    def _calculate_enhanced_factors_batch(self, data: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """å¢å¼ºç‰ˆæ‰¹é‡å› å­è®¡ç®—"""
        self.logger.info(f"ğŸ§® è®¡ç®—{timeframe}å¢å¼ºå› å­...")
        start_time = time.time()
        
        try:
            # ğŸ”¥ ä½¿ç”¨é«˜æ€§èƒ½å› å­è®¡ç®—
            if self.enhanced_config['enhanced_factors']:
                factors_df = self._calculate_vectorized_factors(data, timeframe)
            else:
                factors_df = self.factor_pool.calculate_all_factors(data)
            
            # Categoricalä¿®å¤
            if not factors_df.empty:
                factors_df, fix_report = self.categorical_fixer.comprehensive_fix(factors_df)
                
                if fix_report['categorical_fix']['found_categorical'] > 0:
                    self.logger.info(f"{timeframe} Categoricalä¿®å¤: {fix_report['categorical_fix']['found_categorical']}ä¸ª")
            
            # ğŸ”¥ å¢å¼ºéªŒè¯è¿‡æ»¤
            if self.enhanced_config['robust_validation']:
                factors_df = self._enhanced_factor_validation(factors_df, timeframe)
            
            calc_time = time.time() - start_time
            self.logger.info(f"âœ… {timeframe} å› å­è®¡ç®—å®Œæˆ: {factors_df.shape}, è€—æ—¶{calc_time:.1f}ç§’")
            
            return factors_df
            
        except Exception as e:
            self.logger.error(f"âŒ {timeframe} å› å­è®¡ç®—å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _calculate_vectorized_factors(self, data: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """é«˜æ€§èƒ½å‘é‡åŒ–å› å­è®¡ç®—"""
        if data.empty:
            return pd.DataFrame()
        
        factors = {}
        
        # æå–ä»·æ ¼æ•°æ®
        if isinstance(data.index, pd.MultiIndex):
            # MultiIndexæ•°æ®å¤„ç†
            close_data = data['close'].unstack(level=0)  # symbol -> columns
            high_data = data['high'].unstack(level=0)
            low_data = data['low'].unstack(level=0) 
            volume_data = data['volume'].unstack(level=0)
        else:
            # å•ä¸€æ•°æ®å¤„ç†
            close_data = data['close']
            high_data = data['high']
            low_data = data['low']
            volume_data = data['volume']
        
        try:
            # ğŸ”¥ VectorBTåŸç”ŸæŒ‡æ ‡ - æ‰¹é‡è®¡ç®—
            # è¶‹åŠ¿æŒ‡æ ‡
            sma_20 = close_data.rolling(20).mean()
            factors['sma_20'] = sma_20
            factors['sma_50'] = close_data.rolling(50).mean()
            
            # åŠ¨é‡æŒ‡æ ‡
            factors['roc_10'] = close_data.pct_change(10)
            factors['roc_20'] = close_data.pct_change(20)
            
            # ä½¿ç”¨VectorBTåŠ é€Ÿè®¡ç®—
            if hasattr(vbt, 'RSI'):
                rsi_ind = vbt.RSI.run(close_data, window=14)
                factors['rsi_14'] = rsi_ind.rsi
            else:
                # å¤‡ç”¨RSIè®¡ç®—
                delta = close_data.diff()
                gain = (delta.where(delta > 0, 0)).rolling(14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
                rs = gain / loss
                factors['rsi_14'] = 100 - (100 / (1 + rs))
            
            # MACD
            if hasattr(vbt, 'MACD'):
                macd_ind = vbt.MACD.run(close_data, fast_window=12, slow_window=26, signal_window=9)
                factors['macd'] = macd_ind.macd
                factors['macd_signal'] = macd_ind.signal
                if hasattr(macd_ind, 'histogram'):
                    factors['macd_histogram'] = macd_ind.histogram
                else:
                    factors['macd_histogram'] = macd_ind.macd - macd_ind.signal
            
            # æ³¢åŠ¨ç‡æŒ‡æ ‡
            if hasattr(vbt, 'ATR'):
                atr_ind = vbt.ATR.run(high_data, low_data, close_data, window=14)
                factors['atr_14'] = atr_ind.atr
                factors['atrp'] = atr_ind.atr / close_data  # ç›¸å¯¹ATR
            
            # Bollinger Bands
            if hasattr(vbt, 'BBANDS'):
                bb_ind = vbt.BBANDS.run(close_data, window=20, alpha=2)
                factors['bb_upper'] = bb_ind.upper
                factors['bb_lower'] = bb_ind.lower
                factors['bb_percent'] = (close_data - bb_ind.lower) / (bb_ind.upper - bb_ind.lower)
            
            # ğŸ”¥ è‡ªå®šä¹‰é«˜çº§å› å­
            # ä»·æ ¼ä½ç½®
            factors['price_position'] = (close_data - close_data.rolling(20).min()) / (
                close_data.rolling(20).max() - close_data.rolling(20).min()
            )
            
            # æ³¢åŠ¨ç‡æ¯”ç‡
            factors['volatility_ratio'] = close_data.rolling(10).std() / close_data.rolling(30).std()
            
            # æˆäº¤é‡ç›¸å…³
            if not volume_data.empty:
                factors['volume_sma_ratio'] = volume_data / volume_data.rolling(20).mean()
                factors['volume_momentum'] = volume_data.pct_change(5)
            
            # è½¬æ¢å›MultiIndexæ ¼å¼
            if isinstance(data.index, pd.MultiIndex):
                stacked_factors = {}
                for name, factor_data in factors.items():
                    if hasattr(factor_data, 'stack'):
                        stacked_factors[name] = factor_data.stack()
                    else:
                        stacked_factors[name] = factor_data
                
                factors_df = pd.DataFrame(stacked_factors)
                factors_df.index.names = ['datetime', 'symbol']
                factors_df = factors_df.swaplevel().sort_index()  # è°ƒæ•´ä¸º (symbol, datetime)
            else:
                factors_df = pd.DataFrame(factors)
            
            return factors_df.dropna()
            
        except Exception as e:
            self.logger.error(f"å‘é‡åŒ–å› å­è®¡ç®—å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€å› å­
            basic_factors = {
                'sma_20': close_data.rolling(20).mean(),
                'rsi_14': close_data.rolling(14).apply(lambda x: 50),  # ç®€å•å ä½
                'roc_5': close_data.pct_change(5)
            }
            
            if isinstance(data.index, pd.MultiIndex):
                stacked_basic = {}
                for name, factor_data in basic_factors.items():
                    if hasattr(factor_data, 'stack'):
                        stacked_basic[name] = factor_data.stack()
                    else:
                        stacked_basic[name] = factor_data
                
                factors_df = pd.DataFrame(stacked_basic)
                factors_df.index.names = ['datetime', 'symbol']
                factors_df = factors_df.swaplevel().sort_index()
            else:
                factors_df = pd.DataFrame(basic_factors)
            
            return factors_df.dropna()
    
    def _enhanced_factor_validation(self, factors_df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """å¢å¼ºç‰ˆå› å­éªŒè¯"""
        if factors_df.empty:
            return factors_df
        
        original_count = len(factors_df.columns)
        valid_factors = []
        
        for col in factors_df.columns:
            try:
                factor_series = factors_df[col]
                
                # 1. æ£€æŸ¥å¸¸é‡å› å­
                if factor_series.nunique() <= 1:
                    continue
                
                # 2. æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
                if not pd.api.types.is_numeric_dtype(factor_series):
                    continue
                
                # 3. æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹
                missing_ratio = factor_series.isnull().sum() / len(factor_series)
                if missing_ratio > 0.5:  # è¶…è¿‡50%ç¼ºå¤±
                    continue
                
                # 4. æ£€æŸ¥æ ‡å‡†å·®
                if factor_series.std() < 1e-6:
                    continue
                
                valid_factors.append(col)
                
            except Exception as e:
                self.logger.debug(f"å› å­{col}éªŒè¯å¤±è´¥: {e}")
                continue
        
        validated_df = factors_df[valid_factors] if valid_factors else pd.DataFrame()
        
        self.logger.info(f"{timeframe} å› å­éªŒè¯: {original_count}ä¸ª -> {len(valid_factors)}ä¸ªæœ‰æ•ˆå› å­")
        
        return validated_df
    
    def _calculate_enhanced_ic_analysis(self, factors_df: pd.DataFrame, data: pd.DataFrame, 
                                      timeframe: str) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆICåˆ†æ"""
        self.logger.info(f"ğŸ“ˆ è®¡ç®—{timeframe}å¢å¼ºICåˆ†æ...")
        
        try:
            # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
            if isinstance(data.index, pd.MultiIndex):
                returns = data.groupby(level=0)['close'].pct_change(1).shift(-1)
            else:
                returns = data['close'].pct_change(1).shift(-1)
            
            # å¯¹é½æ•°æ®
            common_index = factors_df.index.intersection(returns.index)
            if len(common_index) == 0:
                self.logger.warning(f"{timeframe} æ— é‡å æ•°æ®")
                return {}
            
            aligned_factors = factors_df.loc[common_index]
            aligned_returns = returns.loc[common_index]
            
            ic_results = {}
            
            for factor_name in aligned_factors.columns:
                try:
                    factor_values = aligned_factors[factor_name].dropna()
                    return_values = aligned_returns.loc[factor_values.index].dropna()
                    
                    # ç¡®ä¿æ•°æ®å¯¹é½
                    common_idx = factor_values.index.intersection(return_values.index)
                    if len(common_idx) < 30:  # æœ€å°‘30ä¸ªè§‚æµ‹å€¼
                        continue
                    
                    factor_aligned = factor_values.loc[common_idx]
                    return_aligned = return_values.loc[common_idx]
                    
                    # ğŸ”¥ å¢å¼ºICè®¡ç®—
                    # 1. åŸºç¡€IC
                    ic = factor_aligned.corr(return_aligned)
                    
                    # 2. æ»šåŠ¨ICç”¨äºIRè®¡ç®—
                    if len(common_idx) >= 63:  # è¶³å¤Ÿæ•°æ®è®¡ç®—æ»šåŠ¨IC
                        ic_series = []
                        window = 21  # 21æœŸæ»šåŠ¨çª—å£
                        
                        for i in range(window, len(common_idx)):
                            window_factor = factor_aligned.iloc[i-window:i]
                            window_return = return_aligned.iloc[i-window:i]
                            
                            if len(window_factor) == window and len(window_return) == window:
                                window_ic = window_factor.corr(window_return)
                                if not pd.isna(window_ic):
                                    ic_series.append(window_ic)
                        
                        if ic_series:
                            ic_mean = np.mean(ic_series)
                            ic_std = np.std(ic_series)
                            ic_ir = ic_mean / ic_std if ic_std > 0 else 0
                        else:
                            ic_ir = 0
                    else:
                        ic_ir = 0
                    
                    # 3. ç»Ÿè®¡æŒ‡æ ‡
                    positive_ic_count = sum(1 for x in ic_series if x > 0) if 'ic_series' in locals() else 0
                    positive_ic_ratio = positive_ic_count / len(ic_series) if 'ic_series' in locals() and ic_series else 0.5
                    
                    # 4. åº”ç”¨å¢å¼ºè¿‡æ»¤æ¡ä»¶
                    if (abs(ic) >= self.enhanced_config['min_ic_threshold'] and 
                        abs(ic_ir) >= self.enhanced_config['min_ir_threshold']):
                        
                        ic_results[factor_name] = {
                            'ic': float(ic),
                            'ic_ir': float(ic_ir),
                            'ic_mean': float(ic_mean) if 'ic_mean' in locals() else float(ic),
                            'ic_std': float(ic_std) if 'ic_std' in locals() else 0.0,
                            'positive_ic_ratio': float(positive_ic_ratio),
                            'sample_size': len(common_idx),
                            'ic_series_length': len(ic_series) if 'ic_series' in locals() else 0
                        }
                    
                except Exception as e:
                    self.logger.debug(f"å› å­{factor_name} ICè®¡ç®—å¤±è´¥: {e}")
                    continue
            
            # è¿‡æ»¤åç»Ÿè®¡
            filtered_count = len(ic_results)
            total_count = len(aligned_factors.columns)
            
            self.logger.info(f"{timeframe} ICåˆ†æ: {total_count}ä¸ªå› å­ -> {filtered_count}ä¸ªé«˜è´¨é‡å› å­")
            
            return ic_results
            
        except Exception as e:
            self.logger.error(f"{timeframe} ICåˆ†æå¤±è´¥: {e}")
            return {}
    
    def _generate_enhanced_report(self, results: Dict) -> str:
        """ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = [
            "# ğŸš€ å¢å¼ºç‰ˆVectorBTç»ˆææ€§èƒ½æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {timestamp}",
            f"**ä¼˜åŒ–æ–¹æ³•**: å¢å¼ºç‰ˆVectorBT + å¤§è§„æ¨¡å¹¶è¡Œ + æ™ºèƒ½è¿‡æ»¤",
            f"**æµ‹è¯•è‚¡ç¥¨**: {len(self.available_symbols)}åªæ¸¯è‚¡",
            f"**æµ‹è¯•æ—¶é—´æ¡†æ¶**: {len(self.enhanced_config['test_timeframes'])}ä¸ª",
            f"**å†…å­˜é…ç½®**: 24GBå¯ç”¨ï¼Œæ™ºèƒ½ä¼˜åŒ–",
            f"**åˆ†æèµ„é‡‘**: {self.capital:,.0f} æ¸¯å¸",
            "",
            "## ğŸ”¥ å¢å¼ºç‰ˆæ ¸å¿ƒä¼˜åŠ¿",
            "",
            f"- **ğŸ”¥ æ‰©å±•æ•°æ®è§„æ¨¡**: æœ€å¤š{self.enhanced_config['max_symbols']}åªè‚¡ç¥¨å¹¶è¡Œå¤„ç†",
            f"- **ğŸ”¥ ä¸¥æ ¼è´¨é‡æ§åˆ¶**: ICé˜ˆå€¼â‰¥{self.enhanced_config['min_ic_threshold']}, IRé˜ˆå€¼â‰¥{self.enhanced_config['min_ir_threshold']}",
            f"- **ğŸ”¥ é«˜æ€§èƒ½è®¡ç®—**: å……åˆ†åˆ©ç”¨24GBå†…å­˜å’Œå¤šæ ¸å¹¶è¡Œ",
            f"- **ğŸ”¥ æ™ºèƒ½å› å­è¿‡æ»¤**: è‡ªåŠ¨å‰”é™¤ä½è´¨é‡å› å­",
            "",
            "## ğŸ“Š æ€§èƒ½ä¸æ•°æ®ç»Ÿè®¡",
            ""
        ]
        
        # æ·»åŠ æ€§èƒ½ç»Ÿè®¡
        if 'execution_time' in results:
            exec_time = results['execution_time']
            total_data_points = sum(
                info.get('data_points', 0) 
                for info in results.get('batch_data_info', {}).values()
            )
            
            report.extend([
                f"- **æ‰§è¡Œæ—¶é—´**: {exec_time:.1f}ç§’",
                f"- **æ€»æ•°æ®ç‚¹**: {total_data_points:,}ä¸ª",
                f"- **å¤„ç†é€Ÿåº¦**: {total_data_points/exec_time:.0f}æ•°æ®ç‚¹/ç§’",
                f"- **å†…å­˜æ•ˆç‡**: æ˜¾è‘—æå‡ï¼Œæ”¯æŒæ›´å¤§è§„æ¨¡æ•°æ®",
                ""
            ])
        
        # æ·»åŠ å› å­è´¨é‡ç»Ÿè®¡
        ic_analysis = results.get('vectorized_ic', {})
        if ic_analysis:
            high_quality_factors = []
            
            for tf, factors in ic_analysis.items():
                for factor_name, factor_data in factors.items():
                    if isinstance(factor_data, dict):
                        ic = abs(factor_data.get('ic', 0))
                        ic_ir = abs(factor_data.get('ic_ir', 0))
                        
                        if (ic >= self.enhanced_config['min_ic_threshold'] and 
                            ic_ir >= self.enhanced_config['min_ir_threshold']):
                            high_quality_factors.append({
                                'name': factor_name,
                                'timeframe': tf,
                                'ic': ic,
                                'ic_ir': ic_ir,
                                'score': ic * 0.6 + ic_ir * 0.4
                            })
            
            # æ’åºå¹¶æ˜¾ç¤ºå‰10
            high_quality_factors.sort(key=lambda x: x['score'], reverse=True)
            
            report.extend([
                "## ğŸ† é«˜è´¨é‡å› å­æ’è¡Œæ¦œ",
                "",
                "| æ’å | å› å­åç§° | æ—¶é—´æ¡†æ¶ | IC | IC_IR | ç»¼åˆå¾—åˆ† | è¯„ä¼° |",
                "|------|----------|----------|-----|-------|----------|------|"
            ])
            
            for i, factor in enumerate(high_quality_factors[:10], 1):
                quality = "âœ… ä¼˜ç§€" if factor['score'] > 0.1 else "âš ï¸ è‰¯å¥½"
                report.append(
                    f"| {i} | {factor['name']} | {factor['timeframe']} | "
                    f"{factor['ic']:.3f} | {factor['ic_ir']:.3f} | "
                    f"{factor['score']:.3f} | {quality} |"
                )
            
            report.extend([
                "",
                f"**é«˜è´¨é‡å› å­æ€»æ•°**: {len(high_quality_factors)}ä¸ª",
                f"**è´¨é‡æ ‡å‡†**: ICâ‰¥{self.enhanced_config['min_ic_threshold']}, IRâ‰¥{self.enhanced_config['min_ir_threshold']}",
                ""
            ])
        
        # æ·»åŠ å»ºè®®
        report.extend([
            "## ğŸ’¡ å¢å¼ºç‰ˆæŠ•èµ„å»ºè®®",
            "",
            "### ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿",
            "- **æ•°æ®è§„æ¨¡**: å¤§å¹…æ‰©å±•åˆ°æ›´å¤šè‚¡ç¥¨å’Œæ—¶é—´æ¡†æ¶",
            "- **è´¨é‡ä¿è¯**: ä¸¥æ ¼è¿‡æ»¤ï¼Œåªä¿ç•™é«˜è´¨é‡å› å­",
            "- **æ€§èƒ½ä¼˜åŒ–**: å……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æºï¼Œæ”¯æŒå®æ—¶åˆ†æ",
            "",
            "### ğŸ“ˆ å®æ–½å»ºè®®",
            f"- **èµ·å§‹èµ„é‡‘**: {self.capital:,.0f} æ¸¯å¸",
            "- **æ›´æ–°é¢‘ç‡**: æ”¯æŒå®æ—¶æˆ–å‡†å®æ—¶æ›´æ–°",
            "- **æ‰©å±•æ€§**: å¯è½»æ¾æ‰©å±•åˆ°æ›´å¤šå¸‚åœºå’Œå“ç§",
            "",
            "---",
            "*å¢å¼ºç‰ˆVectorBTç»ˆææ€§èƒ½æŠ¥å‘Š - å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€é«˜æ€§èƒ½*"
        ])
        
        return "\n".join(report)
    
    def run_enhanced_test(self):
        """è¿è¡Œå¢å¼ºç‰ˆå…¨é¢æµ‹è¯•"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆVectorBTç»ˆææµ‹è¯•...")
        start_time = time.time()
        
        # è®°å½•ç³»ç»Ÿèµ„æº
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        self.logger.info("å¢å¼ºç‰ˆæµ‹è¯•å¼€å§‹ - ç³»ç»Ÿèµ„æº:")
        self.logger.info(f"  å†…å­˜ä½¿ç”¨: {memory.percent}% ({memory.used/1024**3:.1f}GB/{memory.total/1024**3:.1f}GB)")
        self.logger.info(f"  CPUä½¿ç”¨: {cpu_percent}%")
        
        # 1. åŠ è½½å¢å¼ºç‰ˆæ•°æ®
        all_data = self._load_enhanced_multiindex_data()
        
        if not all_data:
            self.logger.error("âŒ æ— æœ‰æ•ˆæ•°æ®ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return
        
        # 2. æ‰¹é‡å› å­è®¡ç®—å’ŒICåˆ†æ
        results = {
            'execution_time': 0,
            'analysis_approach': 'enhanced_vectorbt_ultimate',
            'performance_breakthrough': 'high_quality_large_scale',
            'tested_symbols_count': len(self.available_symbols),
            'tested_timeframes': self.enhanced_config['test_timeframes'],
            'enhanced_config': self.enhanced_config,
            'batch_data_info': {},
            'batch_factors': {},
            'vectorized_ic': {}
        }
        
        for timeframe, data in all_data.items():
            # è®°å½•æ•°æ®ä¿¡æ¯
            results['batch_data_info'][timeframe] = {
                'shape': str(data.shape),
                'symbols_count': len(data.index.get_level_values(0).unique()),
                'data_points': len(data)
            }
            
            # è®¡ç®—å› å­
            factors_df = self._calculate_enhanced_factors_batch(data, timeframe)
            
            if not factors_df.empty:
                results['batch_factors'][timeframe] = f"<DataFrame/Series shape: {factors_df.shape}>"
                
                # ICåˆ†æ
                ic_results = self._calculate_enhanced_ic_analysis(factors_df, data, timeframe)
                if ic_results:
                    results['vectorized_ic'][timeframe] = ic_results
        
        # 3. å®Œæˆç»Ÿè®¡
        execution_time = time.time() - start_time
        results['execution_time'] = execution_time
        
        # æœ€ç»ˆèµ„æºä½¿ç”¨
        final_memory = psutil.virtual_memory()
        final_cpu = psutil.cpu_percent(interval=1)
        
        self.logger.info("å¢å¼ºç‰ˆæµ‹è¯•å®Œæˆ - ç³»ç»Ÿèµ„æº:")
        self.logger.info(f"  å†…å­˜ä½¿ç”¨: {final_memory.percent}% ({final_memory.used/1024**3:.1f}GB/{final_memory.total/1024**3:.1f}GB)")
        self.logger.info(f"  CPUä½¿ç”¨: {final_cpu}%")
        
        # 4. ä¿å­˜ç»“æœ
        self._save_enhanced_results(results)
        
        print(f"âœ… å¢å¼ºç‰ˆæµ‹è¯•å®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        return results
    
    def _save_enhanced_results(self, results: Dict):
        """ä¿å­˜å¢å¼ºç‰ˆç»“æœ"""
        result_dir = f"results/enhanced_vectorbt_{self.timestamp}"
        os.makedirs(result_dir, exist_ok=True)
        
        # ä¿å­˜JSONç»“æœ
        json_file = f"{result_dir}/enhanced_vectorbt_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_enhanced_report(results)
        report_file = f"{result_dir}/enhanced_vectorbt_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"å¢å¼ºç‰ˆç»“æœå·²ä¿å­˜åˆ°: {result_dir}")

def main():
    """ä¸»å‡½æ•°"""
    enhanced_analyzer = EnhancedVectorBTUltimate()
    results = enhanced_analyzer.run_enhanced_test()
    
    if results:
        print("ğŸ‰ å¢å¼ºç‰ˆVectorBTæµ‹è¯•æˆåŠŸå®Œæˆï¼")
        
        # ç®€è¦ç»Ÿè®¡
        total_factors = sum(
            len(factors) for factors in results.get('vectorized_ic', {}).values()
        )
        
        print(f"ğŸ“Š å‘ç°{total_factors}ä¸ªé«˜è´¨é‡å› å­")
        print(f"âš¡ å¤„ç†{len(results['tested_timeframes'])}ä¸ªæ—¶é—´æ¡†æ¶")
        print(f"ğŸ“ˆ åˆ†æ{results['tested_symbols_count']}åªè‚¡ç¥¨")

if __name__ == "__main__":
    main()
