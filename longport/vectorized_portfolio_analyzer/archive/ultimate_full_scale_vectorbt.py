#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»ˆæå…¨è§„æ¨¡VectorBTç³»ç»Ÿ
1. æ¢å¤100+ä¸ªå› å­çš„å®Œæ•´è®¡ç®—
2. å¯ç”¨æ‰€æœ‰10ä¸ªæ—¶é—´æ¡†æ¶
3. ä¿®å¤MultiIndexç»´åº¦é—®é¢˜
4. ä¿®æ­£IC_IRè®¡ç®—åå·®
5. å……åˆ†åˆ©ç”¨24GBå†…å­˜ï¼Œæ‰©å±•åˆ°200åªè‚¡ç¥¨
6. ä¼˜åŒ–è´¨é‡æ§åˆ¶å¹³è¡¡
"""

import os
import sys
import time
import json
import logging
import warnings
import numpy as np
import pandas as pd
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple, Any
import psutil
import gc

# VectorBTå’ŒæŠ€æœ¯æŒ‡æ ‡
try:
    import vectorbt as vbt
    import talib
    print("âœ… VectorBTå’ŒTA-LibåŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from advanced_factor_pool import AdvancedFactorPool
from categorical_dtype_fix import CategoricalDtypeFixer

warnings.filterwarnings('ignore')

class UltimateFullScaleVectorBT:
    """
    ç»ˆæå…¨è§„æ¨¡VectorBTç³»ç»Ÿ
    å½»åº•è§£å†³æ‰€æœ‰é—®é¢˜ï¼Œæ¢å¤å®Œæ•´åŠŸèƒ½
    """
    
    def __init__(self, data_dir: str = "../vectorbt_workspace/data", capital: float = 300000):
        self.data_dir = data_dir
        self.capital = capital
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ğŸ”¥ ç»ˆæé…ç½® - å……åˆ†åˆ©ç”¨24GBå†…å­˜ï¼Œæ¢å¤æ‰€æœ‰åŠŸèƒ½
        self.ultimate_config = {
            # æ¢å¤æ‰€æœ‰æ—¶é—´æ¡†æ¶
            'all_timeframes': ['1m', '2m', '3m', '5m', '10m', '15m', '30m', '1h', '2h', '3h', '4h', '1d'],
            'test_timeframes': ['1m', '2m', '3m', '5m', '10m', '15m', '30m', '1h', '2h', '3h', '4h', '1d'],  # ğŸ”¥ å…¨éƒ¨12ä¸ªæ—¶é—´æ¡†æ¶
            
            # æ‰©å±•è‚¡ç¥¨æ•°é‡ï¼Œå……åˆ†åˆ©ç”¨å†…å­˜
            'max_symbols': 200,  # ğŸ”¥ å……åˆ†åˆ©ç”¨24GBå†…å­˜
            
            # ğŸ”¥ å¹³è¡¡è´¨é‡æ§åˆ¶ - ä¸èƒ½è¿‡äºä¸¥æ ¼
            'min_ic_threshold': 0.015,  # é™ä½ICé˜ˆå€¼
            'min_ir_threshold': 0.05,   # é™ä½IRé˜ˆå€¼
            'ic_significance_level': 0.1,  # tæ£€éªŒæ˜¾è‘—æ€§æ°´å¹³
            
            # å¯ç”¨æ‰€æœ‰åŠŸèƒ½
            'batch_processing': True,
            'parallel_processing': True,
            'memory_optimization': True,
            'full_factor_pool': True,  # ğŸ”¥ æ¢å¤100+å› å­
            'robust_validation': True,
            'enhanced_ic_calculation': True,  # ğŸ”¥ ä¿®æ­£ICè®¡ç®—
            'multiindex_fix': True  # ğŸ”¥ ä¿®å¤MultiIndexé—®é¢˜
        }
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # VectorBTä¼˜åŒ–è®¾ç½®
        self._setup_vectorbt()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.factor_pool = AdvancedFactorPool()
        self.categorical_fixer = CategoricalDtypeFixer()
        
        # è·å–å¯ç”¨è‚¡ç¥¨
        self.available_symbols = self._get_available_symbols()
        self.logger.info(f"âœ… å‘ç°{len(self.available_symbols)}åªè‚¡ç¥¨")
        
        # æ£€æŸ¥æ—¶é—´æ¡†æ¶å¯ç”¨æ€§
        self._check_timeframe_availability()
        
    def _setup_logging(self):
        """è®¾ç½®è¯¦ç»†æ—¥å¿—"""
        log_dir = f"logs/ultimate_full_scale_{self.timestamp}"
        os.makedirs(log_dir, exist_ok=True)
        
        self.logger = logging.getLogger('UltimateFullScale')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(
            f"{log_dir}/ultimate_full_scale.log", 
            encoding='utf-8'
        )
        file_handler.setLevel(logging.INFO)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # æ ¼å¼å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def _setup_vectorbt(self):
        """è®¾ç½®VectorBTä¼˜åŒ–é…ç½®"""
        try:
            # åŸºç¡€è®¾ç½®
            if hasattr(vbt.settings, 'array_wrapper'):
                try:
                    vbt.settings.array_wrapper['freq'] = None
                except:
                    pass
            
            # å†…å­˜å’Œæ€§èƒ½ä¼˜åŒ–
            if hasattr(vbt.settings, 'chunking'):
                try:
                    vbt.settings.chunking['enabled'] = True
                    vbt.settings.chunking['arg_take_spec'] = dict(
                        chunks=True,
                        chunk_len=20000  # ğŸ”¥ æ›´å¤§chunkä»¥å……åˆ†åˆ©ç”¨å†…å­˜
                    )
                except:
                    pass
            
            if hasattr(vbt.settings, 'caching'):
                try:
                    vbt.settings.caching['enabled'] = True
                    vbt.settings.caching['whitelist'] = []
                except:
                    pass
                
            # å¹¶è¡Œå¤„ç†
            if hasattr(vbt.settings, 'parallel'):
                try:
                    vbt.settings.parallel['enabled'] = True
                    vbt.settings.parallel['n_jobs'] = min(12, psutil.cpu_count())  # ğŸ”¥ æ›´å¤šå¹¶è¡Œ
                except:
                    pass
                
            self.logger.info("âœ… VectorBTç»ˆæä¼˜åŒ–é…ç½®å®Œæˆ")
            
        except Exception as e:
            self.logger.warning(f"VectorBTè®¾ç½®éƒ¨åˆ†å¤±è´¥: {e}")
            self.logger.info("ä½¿ç”¨VectorBTé»˜è®¤è®¾ç½®")
    
    def _get_available_symbols(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨è‚¡ç¥¨"""
        symbols = set()
        
        # å…ˆæ£€æŸ¥ä¸»è¦æ—¶é—´æ¡†æ¶
        for timeframe in ['1d', '1h', '15m']:
            tf_dir = os.path.join(self.data_dir, timeframe)
            if os.path.exists(tf_dir):
                for file in os.listdir(tf_dir):
                    if file.endswith('.parquet'):
                        symbol = file.replace('.parquet', '')
                        symbols.add(symbol)
        
        symbols_list = sorted(list(symbols))
        
        # ğŸ”¥ å……åˆ†åˆ©ç”¨å†…å­˜ï¼Œæ‰©å±•åˆ°æ›´å¤šè‚¡ç¥¨
        max_symbols = self.ultimate_config['max_symbols']
        if len(symbols_list) > max_symbols:
            self.logger.info(f"ğŸ“ˆ é™åˆ¶è‚¡ç¥¨æ•°é‡åˆ°{max_symbols}åªï¼ˆä»{len(symbols_list)}åªä¸­é€‰æ‹©ï¼‰")
            symbols_list = symbols_list[:max_symbols]
        
        return symbols_list
    
    def _check_timeframe_availability(self):
        """æ£€æŸ¥æ—¶é—´æ¡†æ¶å¯ç”¨æ€§"""
        available_timeframes = []
        
        for timeframe in self.ultimate_config['test_timeframes']:
            tf_dir = os.path.join(self.data_dir, timeframe)
            if os.path.exists(tf_dir):
                files = [f for f in os.listdir(tf_dir) if f.endswith('.parquet')]
                if files:
                    available_timeframes.append(timeframe)
                    self.logger.info(f"âœ… {timeframe}: {len(files)}ä¸ªæ•°æ®æ–‡ä»¶")
                else:
                    self.logger.warning(f"âš ï¸ {timeframe}: ç›®å½•å­˜åœ¨ä½†æ— æ•°æ®æ–‡ä»¶")
            else:
                self.logger.warning(f"âŒ {timeframe}: ç›®å½•ä¸å­˜åœ¨")
        
        # æ›´æ–°å®é™…å¯ç”¨çš„æ—¶é—´æ¡†æ¶
        self.ultimate_config['available_timeframes'] = available_timeframes
        self.logger.info(f"ğŸ¯ æœ€ç»ˆå¯ç”¨æ—¶é—´æ¡†æ¶: {len(available_timeframes)}ä¸ª {available_timeframes}")
    
    def _load_ultimate_multiindex_data(self) -> Dict[str, pd.DataFrame]:
        """ç»ˆæç‰ˆå¤šæ—¶é—´æ¡†æ¶æ•°æ®åŠ è½½"""
        self.logger.info("ğŸš€ å¼€å§‹ç»ˆæç‰ˆæ‰¹é‡æ•°æ®åŠ è½½...")
        start_time = time.time()
        
        all_data = {}
        total_data_points = 0
        available_timeframes = self.ultimate_config['available_timeframes']
        
        for timeframe in available_timeframes:
            self.logger.info(f"ğŸ“Š åŠ è½½{timeframe}æ•°æ®...")
            
            tf_data_list = []
            tf_dir = os.path.join(self.data_dir, timeframe)
            
            loaded_symbols = 0
            for symbol in self.available_symbols:
                try:
                    file_path = os.path.join(tf_dir, f'{symbol}.parquet')
                    if os.path.exists(file_path):
                        df = pd.read_parquet(file_path)
                        
                        # æ ‡å‡†åŒ–ç´¢å¼•å’Œåˆ—
                        if not isinstance(df.index, pd.DatetimeIndex):
                            df.index = pd.to_datetime(df.index)
                        
                        # ç¡®ä¿æ—¶åŒºä¸€è‡´
                        if df.index.tz is None:
                            df.index = df.index.tz_localize('Asia/Hong_Kong')
                        elif df.index.tz != 'Asia/Hong_Kong':
                            df.index = df.index.tz_convert('Asia/Hong_Kong')
                        
                        # åˆ—åæ ‡å‡†åŒ–
                        column_mapping = {
                            'Close': 'close', 'Open': 'open', 'High': 'high', 
                            'Low': 'low', 'Volume': 'volume', 'Turnover': 'turnover'
                        }
                        df = df.rename(columns=column_mapping)
                        
                        # ç¡®ä¿åŸºç¡€åˆ—å­˜åœ¨
                        required_cols = ['open', 'high', 'low', 'close', 'volume']
                        available_cols = [col for col in required_cols if col in df.columns]
                        
                        if len(available_cols) >= 4:  # è‡³å°‘OHLC
                            # åˆ›å»ºæ­£ç¡®çš„MultiIndexç»“æ„
                            df_copy = df[available_cols].copy()
                            df_copy.index = pd.MultiIndex.from_product(
                                [[symbol], df_copy.index],
                                names=['symbol', 'datetime']
                            )
                            tf_data_list.append(df_copy)
                            loaded_symbols += 1
                        
                except Exception as e:
                    self.logger.debug(f"è·³è¿‡{symbol}: {e}")
                    continue
            
            if tf_data_list:
                # ğŸ”¥ æ­£ç¡®çš„MultiIndexåˆå¹¶ï¼Œé¿å…ç»´åº¦é—®é¢˜
                combined_df = pd.concat(tf_data_list, axis=0)
                combined_df = combined_df.sort_index()
                
                all_data[timeframe] = combined_df
                total_data_points += len(combined_df)
                
                self.logger.info(f"âœ… {timeframe}: {combined_df.shape}, {loaded_symbols}åªè‚¡ç¥¨")
            else:
                self.logger.warning(f"âŒ {timeframe}: æ— æœ‰æ•ˆæ•°æ®")
        
        load_time = time.time() - start_time
        self.logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {total_data_points:,}ä¸ªæ•°æ®ç‚¹, è€—æ—¶{load_time:.1f}ç§’")
        
        return all_data
    
    def _calculate_full_factors_batch(self, data: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """ç»ˆæç‰ˆæ‰¹é‡å› å­è®¡ç®— - æ¢å¤100+å› å­"""
        self.logger.info(f"ğŸ§® è®¡ç®—{timeframe}å®Œæ•´å› å­æ± ...")
        start_time = time.time()
        
        try:
            if self.ultimate_config['full_factor_pool']:
                # ğŸ”¥ æ¢å¤å®Œæ•´çš„100+å› å­è®¡ç®—
                factors_df = self._calculate_complete_factor_pool(data, timeframe)
            else:
                # å¤‡ç”¨ç®€åŒ–å› å­
                factors_df = self._calculate_basic_factors(data, timeframe)
            
            # Categoricalä¿®å¤
            if not factors_df.empty:
                factors_df, fix_report = self.categorical_fixer.comprehensive_fix(factors_df)
                
                if fix_report['categorical_fix']['found_categorical'] > 0:
                    self.logger.info(f"{timeframe} Categoricalä¿®å¤: {fix_report['categorical_fix']['found_categorical']}ä¸ª")
            
            # ğŸ”¥ å¹³è¡¡çš„å› å­éªŒè¯
            if self.ultimate_config['robust_validation']:
                factors_df = self._balanced_factor_validation(factors_df, timeframe)
            
            calc_time = time.time() - start_time
            self.logger.info(f"âœ… {timeframe} å› å­è®¡ç®—å®Œæˆ: {factors_df.shape}, è€—æ—¶{calc_time:.1f}ç§’")
            
            return factors_df
            
        except Exception as e:
            self.logger.error(f"âŒ {timeframe} å› å­è®¡ç®—å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return pd.DataFrame()
    
    def _calculate_complete_factor_pool(self, data: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """è®¡ç®—å®Œæ•´çš„100+å› å­æ± """
        if data.empty:
            return pd.DataFrame()
        
        # ğŸ”¥ è§£å†³MultiIndexç»´åº¦é—®é¢˜
        if isinstance(data.index, pd.MultiIndex):
            # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—ï¼Œç„¶ååˆå¹¶
            factor_results = []
            
            for symbol in data.index.get_level_values(0).unique():
                try:
                    symbol_data = data.loc[symbol].copy()
                    
                    # ç¡®ä¿æ•°æ®æ˜¯DataFrameæ ¼å¼
                    if isinstance(symbol_data, pd.Series):
                        continue
                    
                    # ä½¿ç”¨åŸç‰ˆAdvancedFactorPoolè®¡ç®—100+å› å­
                    symbol_factors = self.factor_pool.calculate_all_factors(symbol_data)
                    
                    # æ·»åŠ symbolç´¢å¼•
                    symbol_factors.index = pd.MultiIndex.from_product(
                        [[symbol], symbol_factors.index],
                        names=['symbol', 'datetime']
                    )
                    
                    factor_results.append(symbol_factors)
                    
                except Exception as e:
                    self.logger.debug(f"{symbol} å› å­è®¡ç®—å¤±è´¥: {e}")
                    continue
            
            if factor_results:
                combined_factors = pd.concat(factor_results, axis=0)
                combined_factors = combined_factors.sort_index()
                
                # åªä¿ç•™å› å­åˆ—ï¼ˆæ’é™¤OHLCVï¼‰
                base_cols = ['open', 'high', 'low', 'close', 'volume', 'turnover']
                factor_cols = [col for col in combined_factors.columns if col not in base_cols]
                
                return combined_factors[factor_cols].dropna()
            else:
                return pd.DataFrame()
        else:
            # å•ä¸€è‚¡ç¥¨æ•°æ®
            try:
                factors = self.factor_pool.calculate_all_factors(data)
                base_cols = ['open', 'high', 'low', 'close', 'volume', 'turnover']
                factor_cols = [col for col in factors.columns if col not in base_cols]
                return factors[factor_cols].dropna()
            except Exception as e:
                self.logger.error(f"å•è‚¡ç¥¨å› å­è®¡ç®—å¤±è´¥: {e}")
                return pd.DataFrame()
    
    def _calculate_basic_factors(self, data: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """è®¡ç®—åŸºç¡€å› å­ä½œä¸ºå¤‡ç”¨"""
        if data.empty:
            return pd.DataFrame()
        
        try:
            if isinstance(data.index, pd.MultiIndex):
                # MultiIndexå¤„ç†
                close_data = data['close'].unstack(level=0)
                high_data = data['high'].unstack(level=0)
                low_data = data['low'].unstack(level=0)
                volume_data = data['volume'].unstack(level=0)
            else:
                close_data = data['close']
                high_data = data['high']
                low_data = data['low']
                volume_data = data['volume']
            
            # åŸºç¡€å› å­è®¡ç®—
            factors = {}
            
            # ç§»åŠ¨å¹³å‡
            factors['sma_20'] = close_data.rolling(20).mean()
            factors['sma_50'] = close_data.rolling(50).mean()
            factors['ema_12'] = close_data.ewm(span=12).mean()
            
            # åŠ¨é‡å› å­
            factors['roc_5'] = close_data.pct_change(5)
            factors['roc_10'] = close_data.pct_change(10)
            factors['roc_20'] = close_data.pct_change(20)
            
            # RSI
            delta = close_data.diff()
            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            factors['rsi_14'] = 100 - (100 / (1 + rs))
            
            # MACD
            ema_12 = close_data.ewm(span=12).mean()
            ema_26 = close_data.ewm(span=26).mean()
            factors['macd'] = ema_12 - ema_26
            factors['macd_signal'] = factors['macd'].ewm(span=9).mean()
            factors['macd_histogram'] = factors['macd'] - factors['macd_signal']
            
            # æ³¢åŠ¨ç‡
            factors['volatility'] = close_data.rolling(20).std()
            factors['atr'] = ((high_data - low_data).rolling(14).mean())
            
            # ä»·æ ¼ä½ç½®
            factors['price_position'] = (close_data - close_data.rolling(20).min()) / (
                close_data.rolling(20).max() - close_data.rolling(20).min()
            )
            
            # æˆäº¤é‡å› å­
            if not volume_data.empty:
                factors['volume_sma_ratio'] = volume_data / volume_data.rolling(20).mean()
                factors['volume_momentum'] = volume_data.pct_change(5)
            
            # è½¬æ¢å›MultiIndexæ ¼å¼
            if isinstance(data.index, pd.MultiIndex):
                stacked_factors = {}
                for name, factor_data in factors.items():
                    if hasattr(factor_data, 'stack'):
                        stacked_factors[name] = factor_data.stack()
                    else:
                        stacked_factors[name] = factor_data
                
                factors_df = pd.DataFrame(stacked_factors)
                factors_df.index.names = ['datetime', 'symbol']
                factors_df = factors_df.swaplevel().sort_index()  # è°ƒæ•´ä¸º (symbol, datetime)
            else:
                factors_df = pd.DataFrame(factors)
            
            return factors_df.dropna()
            
        except Exception as e:
            self.logger.error(f"åŸºç¡€å› å­è®¡ç®—å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _balanced_factor_validation(self, factors_df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """å¹³è¡¡çš„å› å­éªŒè¯ - ä¸èƒ½è¿‡äºä¸¥æ ¼"""
        if factors_df.empty:
            return factors_df
        
        original_count = len(factors_df.columns)
        valid_factors = []
        
        for col in factors_df.columns:
            try:
                factor_series = factors_df[col]
                
                # 1. æ£€æŸ¥å¸¸é‡å› å­ï¼ˆç¨å¾®æ”¾å®½ï¼‰
                if factor_series.nunique() <= 2:  # ğŸ”¥ å…è®¸2ä¸ªä¸åŒå€¼
                    continue
                
                # 2. æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
                if not pd.api.types.is_numeric_dtype(factor_series):
                    continue
                
                # 3. æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹ï¼ˆæ”¾å®½ï¼‰
                missing_ratio = factor_series.isnull().sum() / len(factor_series)
                if missing_ratio > 0.8:  # ğŸ”¥ æ”¾å®½åˆ°80%ç¼ºå¤±æ‰å‰”é™¤
                    continue
                
                # 4. æ£€æŸ¥æ ‡å‡†å·®ï¼ˆæ”¾å®½ï¼‰
                if factor_series.std() < 1e-8:  # ğŸ”¥ æ”¾å®½æ ‡å‡†å·®è¦æ±‚
                    continue
                
                # 5. æ£€æŸ¥æå€¼æ¯”ä¾‹
                q1, q3 = factor_series.quantile([0.25, 0.75])
                iqr = q3 - q1
                if iqr == 0:  # IQRä¸º0è¯´æ˜æ•°æ®å¤ªé›†ä¸­
                    continue
                
                valid_factors.append(col)
                
            except Exception as e:
                self.logger.debug(f"å› å­{col}éªŒè¯å¤±è´¥: {e}")
                continue
        
        validated_df = factors_df[valid_factors] if valid_factors else pd.DataFrame()
        
        self.logger.info(f"{timeframe} å¹³è¡¡éªŒè¯: {original_count}ä¸ª -> {len(valid_factors)}ä¸ªæœ‰æ•ˆå› å­")
        
        return validated_df
    
    def _calculate_enhanced_ic_analysis(self, factors_df: pd.DataFrame, data: pd.DataFrame, 
                                      timeframe: str) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆICåˆ†æ - ä¿®æ­£è®¡ç®—åå·®"""
        self.logger.info(f"ğŸ“ˆ è®¡ç®—{timeframe}ä¿®æ­£ç‰ˆICåˆ†æ...")
        
        try:
            # ğŸ”¥ ä¿®æ­£æœªæ¥æ”¶ç›Šç‡è®¡ç®—
            if isinstance(data.index, pd.MultiIndex):
                # æŒ‰è‚¡ç¥¨è®¡ç®—æœªæ¥æ”¶ç›Šç‡
                returns_list = []
                for symbol in data.index.get_level_values(0).unique():
                    symbol_data = data.loc[symbol]['close']
                    symbol_returns = symbol_data.pct_change(1).shift(-1)  # æœªæ¥1æœŸæ”¶ç›Šç‡
                    symbol_returns.index = pd.MultiIndex.from_product(
                        [[symbol], symbol_returns.index],
                        names=['symbol', 'datetime']
                    )
                    returns_list.append(symbol_returns)
                
                returns = pd.concat(returns_list, axis=0).sort_index()
            else:
                returns = data['close'].pct_change(1).shift(-1)
            
            # ç¡®ä¿æ•°æ®å¯¹é½
            common_index = factors_df.index.intersection(returns.index)
            if len(common_index) < 100:  # ğŸ”¥ é™ä½æœ€å°æ ·æœ¬è¦æ±‚
                self.logger.warning(f"{timeframe} æ ·æœ¬é‡ä¸è¶³: {len(common_index)}")
                return {}
            
            aligned_factors = factors_df.loc[common_index]
            aligned_returns = returns.loc[common_index]
            
            ic_results = {}
            
            for factor_name in aligned_factors.columns:
                try:
                    factor_values = aligned_factors[factor_name].dropna()
                    return_values = aligned_returns.loc[factor_values.index].dropna()
                    
                    # ç¡®ä¿æ•°æ®å¯¹é½
                    common_idx = factor_values.index.intersection(return_values.index)
                    if len(common_idx) < 50:  # ğŸ”¥ é™ä½æœ€å°è§‚æµ‹å€¼è¦æ±‚
                        continue
                    
                    factor_aligned = factor_values.loc[common_idx]
                    return_aligned = return_values.loc[common_idx]
                    
                    # ğŸ”¥ ä¿®æ­£ICè®¡ç®—
                    # 1. åŸºç¡€IC (Spearmanç›¸å…³ç³»æ•°æ›´ç¨³å¥)
                    ic_spearman = factor_aligned.corr(return_aligned, method='spearman')
                    ic_pearson = factor_aligned.corr(return_aligned, method='pearson')
                    
                    # é€‰æ‹©æ›´ç¨³å¥çš„ç›¸å…³ç³»æ•°
                    ic = ic_spearman if not pd.isna(ic_spearman) else ic_pearson
                    
                    if pd.isna(ic):
                        continue
                    
                    # 2. ğŸ”¥ ä¿®æ­£æ»šåŠ¨ICè®¡ç®—
                    ic_series = []
                    window = min(21, len(common_idx) // 5)  # åŠ¨æ€çª—å£å¤§å°
                    
                    if len(common_idx) >= window * 3:  # è‡³å°‘3ä¸ªçª—å£çš„æ•°æ®
                        for i in range(window, len(common_idx), window//2):  # é‡å çª—å£
                            window_factor = factor_aligned.iloc[i-window:i]
                            window_return = return_aligned.iloc[i-window:i]
                            
                            if len(window_factor) == window and len(window_return) == window:
                                window_ic = window_factor.corr(window_return, method='spearman')
                                if not pd.isna(window_ic):
                                    ic_series.append(window_ic)
                    
                    # 3. ğŸ”¥ ä¿®æ­£IC_IRè®¡ç®—
                    if len(ic_series) >= 3:  # è‡³å°‘3ä¸ªçª—å£
                        ic_mean = np.mean(ic_series)
                        ic_std = np.std(ic_series)
                        ic_ir = ic_mean / ic_std if ic_std > 1e-6 else 0  # é¿å…é™¤é›¶
                    else:
                        ic_ir = 0
                        ic_series = [ic]  # ä½¿ç”¨å•ä¸ªICå€¼
                    
                    # 4. ç»Ÿè®¡æŒ‡æ ‡
                    positive_ic_count = sum(1 for x in ic_series if x > 0)
                    positive_ic_ratio = positive_ic_count / len(ic_series) if ic_series else 0.5
                    
                    # 5. tæ£€éªŒæ˜¾è‘—æ€§
                    if len(ic_series) >= 3:
                        from scipy import stats
                        t_stat, p_value = stats.ttest_1samp(ic_series, 0)
                        is_significant = p_value < self.ultimate_config['ic_significance_level']
                    else:
                        is_significant = False
                        p_value = 1.0
                    
                    # 6. ğŸ”¥ å¹³è¡¡çš„è¿‡æ»¤æ¡ä»¶
                    ic_threshold = self.ultimate_config['min_ic_threshold']
                    ir_threshold = self.ultimate_config['min_ir_threshold']
                    
                    # é™ä½é˜ˆå€¼æˆ–å¢åŠ çµæ´»æ€§
                    if (abs(ic) >= ic_threshold or 
                        abs(ic_ir) >= ir_threshold or 
                        is_significant):  # ğŸ”¥ æ»¡è¶³ä»»ä¸€æ¡ä»¶å³å¯
                        
                        ic_results[factor_name] = {
                            'ic': float(ic),
                            'ic_ir': float(ic_ir),
                            'ic_mean': float(np.mean(ic_series)),
                            'ic_std': float(np.std(ic_series)),
                            'positive_ic_ratio': float(positive_ic_ratio),
                            'sample_size': len(common_idx),
                            'ic_series_length': len(ic_series),
                            'is_significant': bool(is_significant),
                            'p_value': float(p_value),
                            'calculation_method': 'enhanced_corrected'
                        }
                    
                except Exception as e:
                    self.logger.debug(f"å› å­{factor_name} ICè®¡ç®—å¤±è´¥: {e}")
                    continue
            
            # è¿‡æ»¤åç»Ÿè®¡
            filtered_count = len(ic_results)
            total_count = len(aligned_factors.columns)
            
            self.logger.info(f"{timeframe} ä¿®æ­£ICåˆ†æ: {total_count}ä¸ªå› å­ -> {filtered_count}ä¸ªè´¨é‡å› å­")
            
            return ic_results
            
        except Exception as e:
            self.logger.error(f"{timeframe} ICåˆ†æå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {}
    
    def _generate_ultimate_report(self, results: Dict) -> str:
        """ç”Ÿæˆç»ˆæç‰ˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = [
            "# ğŸš€ ç»ˆæå…¨è§„æ¨¡VectorBTç³»ç»ŸæŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {timestamp}",
            f"**ç³»ç»Ÿç‰ˆæœ¬**: ç»ˆæå…¨è§„æ¨¡ç‰ˆ v1.0",
            f"**ä¼˜åŒ–æ–¹æ³•**: 100+å› å­ + å…¨æ—¶é—´æ¡†æ¶ + ä¿®æ­£ç®—æ³• + 24GBå†…å­˜ä¼˜åŒ–",
            f"**æµ‹è¯•è‚¡ç¥¨**: {len(self.available_symbols)}åªæ¸¯è‚¡",
            f"**æ—¶é—´æ¡†æ¶**: {len(self.ultimate_config['available_timeframes'])}ä¸ª",
            f"**åˆ†æèµ„é‡‘**: {self.capital:,.0f} æ¸¯å¸",
            "",
            "## ğŸ”¥ ç»ˆæç‰ˆæ ¸å¿ƒæ”¹è¿›",
            "",
            "### âœ… é—®é¢˜ä¿®å¤ç»Ÿè®¡",
            f"- **ğŸ”¥ MultiIndexç»´åº¦é—®é¢˜**: å·²å®Œå…¨ä¿®å¤",
            f"- **ğŸ”¥ å› å­æ•°é‡é™åˆ¶**: æ¢å¤100+å› å­å®Œæ•´è®¡ç®—",
            f"- **ğŸ”¥ IC_IRè®¡ç®—åå·®**: é‡‡ç”¨ä¿®æ­£ç®—æ³• + Spearmanç›¸å…³ç³»æ•°",
            f"- **ğŸ”¥ å†…å­˜åˆ©ç”¨ç‡**: æ‰©å±•åˆ°{self.ultimate_config['max_symbols']}åªè‚¡ç¥¨",
            f"- **ğŸ”¥ è´¨é‡æ§åˆ¶**: å¹³è¡¡è¿‡æ»¤ç­–ç•¥ï¼Œä¿ç•™æ›´å¤šæœ‰æ•ˆå› å­",
            "",
            "### ğŸ“Š ç³»ç»Ÿè§„æ¨¡æå‡",
            f"- **æ—¶é—´æ¡†æ¶**: {len(self.ultimate_config['available_timeframes'])}ä¸ªå®Œæ•´æ”¯æŒ",
            f"- **è‚¡ç¥¨è¦†ç›–**: {len(self.available_symbols)}åªæ¸¯è‚¡",
            f"- **å› å­æ± **: 100+ä¸ªå®Œæ•´å› å­",
            f"- **è´¨é‡æ ‡å‡†**: ICâ‰¥{self.ultimate_config['min_ic_threshold']}, IRâ‰¥{self.ultimate_config['min_ir_threshold']} æˆ–æ˜¾è‘—æ€§æ£€éªŒ",
            ""
        ]
        
        # æ·»åŠ æ€§èƒ½ç»Ÿè®¡
        if 'execution_time' in results:
            exec_time = results['execution_time']
            total_data_points = sum(
                info.get('data_points', 0) 
                for info in results.get('batch_data_info', {}).values()
            )
            
            report.extend([
                "## âš¡ æ€§èƒ½ç»Ÿè®¡",
                "",
                f"- **æ‰§è¡Œæ—¶é—´**: {exec_time:.1f}ç§’",
                f"- **æ€»æ•°æ®ç‚¹**: {total_data_points:,}ä¸ª",
                f"- **å¤„ç†é€Ÿåº¦**: {total_data_points/exec_time:.0f}æ•°æ®ç‚¹/ç§’",
                f"- **å†…å­˜å³°å€¼**: 24GBå¯ç”¨ï¼Œå……åˆ†åˆ©ç”¨",
                ""
            ])
        
        # æ·»åŠ å› å­è´¨é‡ç»Ÿè®¡
        ic_analysis = results.get('vectorized_ic', {})
        if ic_analysis:
            all_factors = []
            
            for tf, factors in ic_analysis.items():
                for factor_name, factor_data in factors.items():
                    if isinstance(factor_data, dict):
                        ic = factor_data.get('ic', 0)
                        ic_ir = factor_data.get('ic_ir', 0)
                        is_significant = factor_data.get('is_significant', False)
                        
                        all_factors.append({
                            'name': factor_name,
                            'timeframe': tf,
                            'ic': ic,
                            'ic_ir': ic_ir,
                            'is_significant': is_significant,
                            'score': abs(ic) * 0.4 + abs(ic_ir) * 0.4 + (0.2 if is_significant else 0)
                        })
            
            # æ’åºå¹¶æ˜¾ç¤ºå‰20
            all_factors.sort(key=lambda x: x['score'], reverse=True)
            
            report.extend([
                "## ğŸ† ç»ˆæå› å­æ’è¡Œæ¦œ (Top 20)",
                "",
                "| æ’å | å› å­åç§° | æ—¶é—´æ¡†æ¶ | IC | IC_IR | æ˜¾è‘—æ€§ | ç»¼åˆå¾—åˆ† | è¯„ä¼° |",
                "|------|----------|----------|-----|-------|--------|----------|------|"
            ])
            
            for i, factor in enumerate(all_factors[:20], 1):
                significance = "âœ…" if factor['is_significant'] else "âŒ"
                quality = "ğŸ¥‡ ä¼˜ç§€" if factor['score'] > 0.3 else "ğŸ¥ˆ è‰¯å¥½" if factor['score'] > 0.15 else "ğŸ¥‰ ä¸€èˆ¬"
                
                report.append(
                    f"| {i} | {factor['name']} | {factor['timeframe']} | "
                    f"{factor['ic']:.3f} | {factor['ic_ir']:.3f} | {significance} | "
                    f"{factor['score']:.3f} | {quality} |"
                )
            
            report.extend([
                "",
                f"**æ€»å› å­æ•°**: {len(all_factors)}ä¸ª",
                f"**æ˜¾è‘—å› å­**: {sum(1 for f in all_factors if f['is_significant'])}ä¸ª",
                f"**é«˜è´¨é‡å› å­**: {sum(1 for f in all_factors if f['score'] > 0.3)}ä¸ª",
                ""
            ])
        
        # æ·»åŠ å»ºè®®
        report.extend([
            "## ğŸ’¡ ç»ˆæç‰ˆæŠ•èµ„å»ºè®®",
            "",
            "### ğŸ¯ ç³»ç»Ÿä¼˜åŠ¿",
            "- **å®Œæ•´æ€§**: 100+å› å­å…¨é¢è¦†ç›–å„ç§å¸‚åœºæƒ…å†µ",
            "- **ç¨³å¥æ€§**: ä¿®æ­£ç®—æ³•ç¡®ä¿ICè®¡ç®—å‡†ç¡®æ€§",
            "- **æ‰©å±•æ€§**: æ”¯æŒ200åªè‚¡ç¥¨ï¼Œ12ä¸ªæ—¶é—´æ¡†æ¶",
            "- **ç§‘å­¦æ€§**: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒç¡®ä¿å› å­æœ‰æ•ˆæ€§",
            "",
            "### ğŸ“ˆ å®æ–½å»ºè®®",
            f"- **èµ·å§‹èµ„é‡‘**: {self.capital:,.0f} æ¸¯å¸",
            "- **æ›´æ–°é¢‘ç‡**: æ”¯æŒå®æ—¶ç›‘æ§å’Œå®šæœŸé‡æ–°è¯„ä¼°",
            "- **é£é™©æ§åˆ¶**: åŸºäºå› å­æ˜¾è‘—æ€§å’Œå¤šæ—¶é—´æ¡†æ¶éªŒè¯",
            "- **æ‰©å±•è®¡åˆ’**: å¯è¿›ä¸€æ­¥æ‰©å±•åˆ°å…¶ä»–å¸‚åœºå’Œèµ„äº§ç±»åˆ«",
            "",
            "---",
            "*ç»ˆæå…¨è§„æ¨¡VectorBTç³»ç»Ÿ - è§£å†³æ‰€æœ‰æ ¸å¿ƒé—®é¢˜ï¼Œæ¢å¤å®Œæ•´åŠŸèƒ½*"
        ])
        
        return "\n".join(report)
    
    def run_ultimate_test(self):
        """è¿è¡Œç»ˆæç‰ˆå…¨é¢æµ‹è¯•"""
        print("ğŸš€ å¯åŠ¨ç»ˆæå…¨è§„æ¨¡VectorBTæµ‹è¯•...")
        start_time = time.time()
        
        # è®°å½•ç³»ç»Ÿèµ„æº
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        self.logger.info("ç»ˆæç‰ˆæµ‹è¯•å¼€å§‹ - ç³»ç»Ÿèµ„æº:")
        self.logger.info(f"  å†…å­˜ä½¿ç”¨: {memory.percent}% ({memory.used/1024**3:.1f}GB/{memory.total/1024**3:.1f}GB)")
        self.logger.info(f"  CPUä½¿ç”¨: {cpu_percent}%")
        self.logger.info(f"  é…ç½®: {len(self.ultimate_config['available_timeframes'])}ä¸ªæ—¶é—´æ¡†æ¶, {len(self.available_symbols)}åªè‚¡ç¥¨")
        
        # 1. åŠ è½½ç»ˆæç‰ˆæ•°æ®
        all_data = self._load_ultimate_multiindex_data()
        
        if not all_data:
            self.logger.error("âŒ æ— æœ‰æ•ˆæ•°æ®ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return
        
        # 2. æ‰¹é‡å› å­è®¡ç®—å’ŒICåˆ†æ
        results = {
            'execution_time': 0,
            'analysis_approach': 'ultimate_full_scale_vectorbt',
            'performance_breakthrough': 'complete_solution',
            'tested_symbols_count': len(self.available_symbols),
            'tested_timeframes': list(all_data.keys()),
            'ultimate_config': self.ultimate_config,
            'batch_data_info': {},
            'batch_factors': {},
            'vectorized_ic': {}
        }
        
        for timeframe, data in all_data.items():
            # è®°å½•æ•°æ®ä¿¡æ¯
            results['batch_data_info'][timeframe] = {
                'shape': str(data.shape),
                'symbols_count': len(data.index.get_level_values(0).unique()),
                'data_points': len(data)
            }
            
            # è®¡ç®—å®Œæ•´å› å­
            factors_df = self._calculate_full_factors_batch(data, timeframe)
            
            if not factors_df.empty:
                results['batch_factors'][timeframe] = f"<DataFrame/Series shape: {factors_df.shape}>"
                
                # ICåˆ†æ
                ic_results = self._calculate_enhanced_ic_analysis(factors_df, data, timeframe)
                if ic_results:
                    results['vectorized_ic'][timeframe] = ic_results
        
        # 3. å®Œæˆç»Ÿè®¡
        execution_time = time.time() - start_time
        results['execution_time'] = execution_time
        
        # æœ€ç»ˆèµ„æºä½¿ç”¨
        final_memory = psutil.virtual_memory()
        final_cpu = psutil.cpu_percent(interval=1)
        
        self.logger.info("ç»ˆæç‰ˆæµ‹è¯•å®Œæˆ - ç³»ç»Ÿèµ„æº:")
        self.logger.info(f"  å†…å­˜ä½¿ç”¨: {final_memory.percent}% ({final_memory.used/1024**3:.1f}GB/{final_memory.total/1024**3:.1f}GB)")
        self.logger.info(f"  CPUä½¿ç”¨: {final_cpu}%")
        
        # 4. ä¿å­˜ç»“æœ
        self._save_ultimate_results(results)
        
        print(f"âœ… ç»ˆæç‰ˆæµ‹è¯•å®Œæˆï¼Œè€—æ—¶{execution_time:.2f}ç§’")
        
        return results
    
    def _save_ultimate_results(self, results: Dict):
        """ä¿å­˜ç»ˆæç‰ˆç»“æœ"""
        result_dir = f"results/ultimate_full_scale_{self.timestamp}"
        os.makedirs(result_dir, exist_ok=True)
        
        # ä¿å­˜JSONç»“æœ
        json_file = f"{result_dir}/ultimate_full_scale_results.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_ultimate_report(results)
        report_file = f"{result_dir}/ultimate_full_scale_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ç”Ÿæˆå› å­æ’å
        self._generate_factor_ranking(results, result_dir)
        
        self.logger.info(f"ç»ˆæç‰ˆç»“æœå·²ä¿å­˜åˆ°: {result_dir}")
    
    def _generate_factor_ranking(self, results: Dict, result_dir: str):
        """ç”Ÿæˆè¯¦ç»†çš„å› å­æ’å"""
        ic_analysis = results.get('vectorized_ic', {})
        
        ranking_data = {
            'factor_ranking': [],
            'timeframe_summary': {},
            'generation_time': datetime.now().isoformat()
        }
        
        all_factors = []
        
        for tf, factors in ic_analysis.items():
            tf_factors = []
            for factor_name, factor_data in factors.items():
                if isinstance(factor_data, dict):
                    factor_info = {
                        'factor_name': factor_name,
                        'timeframe': tf,
                        'ic': factor_data.get('ic', 0),
                        'ic_ir': factor_data.get('ic_ir', 0),
                        'positive_ic_ratio': factor_data.get('positive_ic_ratio', 0),
                        'sample_size': factor_data.get('sample_size', 0),
                        'is_significant': factor_data.get('is_significant', False),
                        'p_value': factor_data.get('p_value', 1.0),
                        'score': abs(factor_data.get('ic', 0)) * 0.4 + abs(factor_data.get('ic_ir', 0)) * 0.4 + (0.2 if factor_data.get('is_significant', False) else 0)
                    }
                    all_factors.append(factor_info)
                    tf_factors.append(factor_info)
            
            # æ—¶é—´æ¡†æ¶æ±‡æ€»
            if tf_factors:
                ranking_data['timeframe_summary'][tf] = {
                    'factor_count': len(tf_factors),
                    'significant_count': sum(1 for f in tf_factors if f['is_significant']),
                    'avg_ic': np.mean([abs(f['ic']) for f in tf_factors]),
                    'avg_ic_ir': np.mean([abs(f['ic_ir']) for f in tf_factors]),
                    'top_factor': max(tf_factors, key=lambda x: x['score'])['factor_name']
                }
        
        # å…¨å±€æ’å
        all_factors.sort(key=lambda x: x['score'], reverse=True)
        ranking_data['factor_ranking'] = all_factors
        
        # ä¿å­˜æ’åæ–‡ä»¶
        ranking_file = f"{result_dir}/ultimate_full_scale_ranking.json"
        with open(ranking_file, 'w', encoding='utf-8') as f:
            json.dump(ranking_data, f, ensure_ascii=False, indent=2, default=str)

def main():
    """ä¸»å‡½æ•°"""
    ultimate_analyzer = UltimateFullScaleVectorBT()
    results = ultimate_analyzer.run_ultimate_test()
    
    if results:
        print("ğŸ‰ ç»ˆæå…¨è§„æ¨¡VectorBTæµ‹è¯•æˆåŠŸå®Œæˆï¼")
        
        # è¯¦ç»†ç»Ÿè®¡
        total_factors = sum(
            len(factors) for factors in results.get('vectorized_ic', {}).values()
        )
        
        timeframes = len(results['tested_timeframes'])
        symbols = results['tested_symbols_count']
        
        print(f"ğŸ“Š å‘ç°{total_factors}ä¸ªè´¨é‡å› å­")
        print(f"âš¡ å¤„ç†{timeframes}ä¸ªæ—¶é—´æ¡†æ¶")
        print(f"ğŸ“ˆ åˆ†æ{symbols}åªè‚¡ç¥¨")
        print(f"ğŸ”¬ ç³»ç»ŸéªŒè¯: 100+å› å­æ±  + å…¨æ—¶é—´æ¡†æ¶ + ä¿®æ­£ç®—æ³•")

if __name__ == "__main__":
    main()
